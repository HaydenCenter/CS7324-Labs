{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Three - Extending Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will compare the performance of logistic regression optimization programmed in scikit-learn and via your own implementation. You will also modify the optimization procedure for logistic regression. \n",
    "\n",
    "This report is worth 10% of the final grade. Please upload a report (<b>one per team</b>) with all code used, visualizations, and text in a rendered Jupyter notebook. Any visualizations that cannot be embedded in the notebook, please provide screenshots of the output. The results should be reproducible using your report. Please carefully describe every assumption and every step in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dataset Selection</b>\n",
    "\n",
    "Select a dataset identically to the way you selected for the lab one (i.e., table data). You are not required to use the same dataset that you used in the past, but you are encouraged. You must identify a classification task from the dataset that contains <b>three or more classes to predict</b>. That is it cannot be a binary classification; it must be multi-class prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation and Overview (3pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>[<b>2 points</b>] Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the classification task is and what parties would be interested in the results. For example, would the model be deployed or used mostly for offline analysis? </li>\n",
    "    <li>[<b>.5 points</b>] (<i>mostly the same processes as from previous labs</i>) Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). </li>\n",
    "    <li>[<b>.5 points</b>] Divide you data into training and testing data using an 80% training and 20% testing split. Use the cross validation modules that are part of scikit-learn. <b>Argue \"for\" or \"against\" splitting your data using an 80/20 split. That is, why is the 80/20 split appropriate (or not) for your dataset?</b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case\n",
    "\n",
    "Our task will be looking at a patients information and determining whether they are likely to have a stroke, heart disease, or hypertension. The use-case for this classifier would be to flag at-risk patients and enable some kind of response to be made to prevent serious medical emergencies that these conditions might cause or prevent the conditions in the first place.\n",
    "\n",
    "For example, if a person were to be flagged as very likely to have a stroke, the doctor could contact the patient in an attempt to prevent the stroke by prescribing them medication or alerting the patient's family to monitor them in case they were to have a stroke. Similar actions could be taken for hypertension and heart disease.\n",
    "\n",
    "Alernatively, some kind of application could be made to allow people to enter their information and determine how at risk they might be for these conditions, giving people more clear information about their health and the issues that are likely to affect them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 1.1.3\n",
      "Numpy: 1.19.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>work_type</th>\n",
       "      <th>residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9046</td>\n",
       "      <td>Male</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>formerly smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51676</td>\n",
       "      <td>Female</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>202.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31112</td>\n",
       "      <td>Male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Rural</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60182</td>\n",
       "      <td>Female</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>smokes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1665</td>\n",
       "      <td>Female</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender   age  hypertension  heart_disease ever_married  \\\n",
       "0   9046    Male  67.0             0              1          Yes   \n",
       "1  51676  Female  61.0             0              0          Yes   \n",
       "2  31112    Male  80.0             0              1          Yes   \n",
       "3  60182  Female  49.0             0              0          Yes   \n",
       "4   1665  Female  79.0             1              0          Yes   \n",
       "\n",
       "       work_type residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
       "0        Private          Urban             228.69  36.6  formerly smoked   \n",
       "1  Self-employed          Rural             202.21   NaN     never smoked   \n",
       "2        Private          Rural             105.92  32.5     never smoked   \n",
       "3        Private          Urban             171.23  34.4           smokes   \n",
       "4  Self-employed          Rural             174.12  24.0     never smoked   \n",
       "\n",
       "   stroke  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing packages and reading in dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print('Pandas:', pd.__version__)\n",
    "print('Numpy:',  np.__version__)\n",
    "\n",
    "raw_data = pd.read_csv('healthcare-dataset-stroke-data.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping categorical column 'work_type'; not very useful and\n",
    "# doesn't translate nicely into ordinal numbers\n",
    "df = raw_data.drop('work_type', axis = 1)\n",
    "\n",
    "# Dropping 1 observation of person with gender 'Other' to simplify\n",
    "# using the gender column to calculate, impute, or visualize\n",
    "df.drop(df[df.gender == 'Other'].index, inplace=True)\n",
    "\n",
    "# Making values' format consistent\n",
    "for c in df.columns:\n",
    "    if df[c].dtype == 'object':\n",
    "        df[c] = df[c].str.lower()\n",
    "\n",
    "# Adding numbers to smoking_status values to order them properly\n",
    "# when they will get passed through the SKLearn LabelEncoder\n",
    "df.smoking_status.replace(to_replace= ['never smoked', 'formerly smoked', 'smokes', 'Unknown'],\n",
    "                          value     = ['0_never_smoked', '1_formerly_smoked', '2_smokes', '3_Unknown'],\n",
    "                          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9046</td>\n",
       "      <td>1</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51676</td>\n",
       "      <td>0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>202.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31112</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60182</td>\n",
       "      <td>0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1665</td>\n",
       "      <td>0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender   age  hypertension  heart_disease  ever_married  \\\n",
       "0   9046       1  67.0             0              1             1   \n",
       "1  51676       0  61.0             0              0             1   \n",
       "2  31112       1  80.0             0              1             1   \n",
       "3  60182       0  49.0             0              0             1   \n",
       "4   1665       0  79.0             1              0             1   \n",
       "\n",
       "   residence_type  avg_glucose_level   bmi  smoking_status  stroke  \n",
       "0               1             228.69  36.6             1.0       1  \n",
       "1               0             202.21   NaN             0.0       1  \n",
       "2               0             105.92  32.5             0.0       1  \n",
       "3               1             171.23  34.4             2.0       1  \n",
       "4               0             174.12  24.0             0.0       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encoding all of the non-numeric columns\n",
    "le = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        le[col] = LabelEncoder()\n",
    "        df[col] = le[col].fit_transform(df[col])\n",
    "\n",
    "# Call le[col].inverse_transform(df[col]) for any column name\n",
    "# to convert numbers back to their labels\n",
    "\n",
    "# Converting all 'Unknown' values in smoking status to NaN so\n",
    "# that we can impute the missing values.\n",
    "df.smoking_status.mask(df.smoking_status == 3, np.nan, inplace=True)\n",
    "               \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values\n",
    "from sklearn.impute import KNNImputer\n",
    "import copy\n",
    "\n",
    "knn = KNNImputer(n_neighbors=3)\n",
    "\n",
    "# Imputing on all columns except id\n",
    "columns = list(df.columns)\n",
    "columns.remove('id')\n",
    "\n",
    "df_imputed = copy.deepcopy(df)\n",
    "df_imputed[columns] = knn.fit_transform(df[columns])\n",
    "\n",
    "# Rounding imputed values to be compatible with LabelEncoder\n",
    "# for smoking_status and to match the format of other values\n",
    "# for bmi\n",
    "df_imputed.smoking_status = df_imputed.smoking_status.apply(lambda x: round(x, 0))\n",
    "df_imputed.bmi = df_imputed.bmi.apply(lambda x: round(x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using df_imputed as the primary dataset\n",
    "df = df_imputed\n",
    "\n",
    "# Changing columns modified by KNN Imputer back to integers from floats\n",
    "columns = [\n",
    "    'gender',\n",
    "    'hypertension',\n",
    "    'heart_disease',\n",
    "    'ever_married',\n",
    "    'residence_type',\n",
    "    'smoking_status',\n",
    "    'stroke'\n",
    "]\n",
    "\n",
    "for col in columns:\n",
    "    df[col] = df[col].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prep this dataset, one attribute was removed due to it being relatively unimportant and not encoding nicely into an ordinal set of integers. All categorical variables were converted to numeric data using SKLearn's LabelEncoder class. Missing values for bmi and smoking_status were imputed using KNN Imputer. One record was dropped for being the only entry with gender 'Other'. Removing this record will make visualizing the gender data simpler and will have little impact on the training, as having an outlier like that might cause other attributes to be slightly undervalued in comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a table of the LabelEncoder encoded variables.\n",
    "\n",
    "| value | gender | ever_married | residence_type | smoking_status    |\n",
    "|-------|--------|--------------|----------------|-------------------|\n",
    "| 0     | female | no           | rural          | 0_never_smoked    |\n",
    "| 1     | male   | yes          | urban          | 1_formerly_smoked |\n",
    "| 2     |   -    |      -       |       -        | 2_smokes          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9046</td>\n",
       "      <td>1</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51676</td>\n",
       "      <td>0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>202.21</td>\n",
       "      <td>30.9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31112</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60182</td>\n",
       "      <td>0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1665</td>\n",
       "      <td>0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender   age  hypertension  heart_disease  ever_married  \\\n",
       "0   9046       1  67.0             0              1             1   \n",
       "1  51676       0  61.0             0              0             1   \n",
       "2  31112       1  80.0             0              1             1   \n",
       "3  60182       0  49.0             0              0             1   \n",
       "4   1665       0  79.0             1              0             1   \n",
       "\n",
       "   residence_type  avg_glucose_level   bmi  smoking_status  stroke  \n",
       "0               1             228.69  36.6               1       1  \n",
       "1               0             202.21  30.9               0       1  \n",
       "2               0             105.92  32.5               0       1  \n",
       "3               1             171.23  34.4               2       1  \n",
       "4               0             174.12  24.0               0       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use the 80-20 split for our testing and training data. We rationalized that, with a dataset of around 5000 entries, 1000 entries should be a decently large sample for testing our classifier. Additionally, the Pareto Principle (which says that 20 percent of inputs can typically account for 80 percent of outputs) is also something that we considered when choosing how to split our data. 80-20 is the standard we have been using so far in this coursework, but it also has some level of relation to this principle, giving us another reason to use it. While the connection is not extremely direct, we feel that it contributes to our decision to use the 80-20 split in a meaningful way.\n",
    "\n",
    "Our runner-up method was cross validation. We chose to not pursue that method in case it would affect our implementation of the classifier in some non-trivial way. We wanted to avoid doing unnecessary, superfluous, or tedious work on top of the work already required by the assignment in order to produce the highest quality work we could."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = list(df.columns)\n",
    "columns.remove('id')\n",
    "targets = ['stroke', 'heart_disease', 'hypertension']\n",
    "\n",
    "for col in targets:\n",
    "    columns.remove(col)\n",
    "\n",
    "#splitting into train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=.20, random_state=42)\n",
    "\n",
    "X_test  = test[columns].to_numpy()\n",
    "X_train = train[columns].to_numpy()\n",
    "y_test  = {}\n",
    "y_train = {}\n",
    "\n",
    "for col in targets:\n",
    "    y_test[col]  = test[col].to_numpy()\n",
    "    y_train[col] = train[col].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (5pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>The implementation of logistic regression must be written only from the examples given to you by the instructor. No credit will be assigned to teams that copy implementations from another source, regardless of if the code is properly cited.</li>\n",
    "    <li>[<b>2 points</b>] Create a custom, one-versus-all logistic regression classifier using numpy and scipy to optimize. Use object oriented conventions identical to scikit-learn. You should start with the template developed by the instructor in the course. You should add the following functionality to the logistic regression classifier:\n",
    "    <ul>\n",
    "        <li>Ability to choose optimization technique when class is instantiated: either steepest descent, stochastic gradient descent, or Newton's method. </li>\n",
    "        <li>Update the gradient calculation to include a customizable regularization term (either using no regularization, L1 regularization, L2 regularization, or both L1 and L2 regularization). Associate a cost with the regularization term, \"C\", that can be adjusted when the class is instantiated.  </li>\n",
    "    </ul>\n",
    "    </li>\n",
    "    <li>[<b>1.5 points</b>] Train your classifier to achieve good generalization performance. That is, adjust the <b>optimization technique</b> and the value of the <b>regularization term \"C\"</b> to achieve the best performance on your test set. Visualize the performance of the classifier versus the parameters you investigated. Is your method of selecting parameters justified? That is, do you think there is any \"data snooping\" involved with this method of selecting parameters?</li>\n",
    "    <li>[<b>1.5 points</b>] Compare the performance of your \"best\" logistic regression optimization procedure to the procedure used in scikit-learn. Visualize the performance differences in terms of training time and classification performance. <b>Discuss the results</b>. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, optimization='bgd', eta = 0.01, iterations=20, regularization='none', c=0):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.opt = optimization\n",
    "        self.reg = regularization\n",
    "        self.c = c\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    #optimization methods\n",
    "    def _get_gradient(self, X, y):\n",
    "        \n",
    "        gradient = None\n",
    "        if self.opt == 'tgd': gradient = self.steepest_descent\n",
    "        elif self.opt == 'sgd': gradient = self.stochastic_gradient_descent\n",
    "        elif self.opt == 'newton': gradient = self.newton\n",
    "        return gradient(X,y)\n",
    "    \n",
    "    def steepest_descent(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += self.c * self._get_reg_gradient()\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def stochastic_gradient_descent(self,X,y):\n",
    "        idx = np.random.randint(len(y))\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += self.c * self._get_reg_gradient()\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def newton(self, X, y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.c  # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] +=  self.c * self._get_reg_gradient()\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    #regularization methods\n",
    "    def _get_reg_gradient(self):\n",
    "        if self.reg == 'none':\n",
    "            return self.w_[1:]\n",
    "        elif self.reg == 'ridge':\n",
    "            return -2 * self.w_[1:]\n",
    "        elif self.reg == 'lasso':\n",
    "            return np.sign(self.w_[1:])\n",
    "        elif self.reg == 'elastic_net':\n",
    "            return -2 * self.w_[1:] + np.sign(self.w_[1:])\n",
    "    \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logisitic Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, optimization, eta, iterations, regularization, c=0):\n",
    "    \n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.opt = optimization\n",
    "        self.reg = regularization\n",
    "        self.encodings = {}\n",
    "        self.c = c\n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "    \n",
    "    def fit(self,X,y): # y is a hash of target columns\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for name, target in y.items():\n",
    "            blr = BinaryLogisticRegression(self.opt, self.eta, self.iters, self.reg, self.c )\n",
    "            blr.fit(X,target)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "        #if np.count_nonzero(blr.predict_proba(X)) > 0:\n",
    "            #print(\"Not zero\")\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "            \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "    \n",
    "lr = LogisticRegression('tgd',0.01, 100, 'ridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.9393346379647749  -  stroke\n",
      "Accuracy of Testing Dataset:  0.9315068493150684  -  heart_disease\n",
      "Accuracy of Testing Dataset:  0.8845401174168297  -  hypertension\n"
     ]
    }
   ],
   "source": [
    "#evaluate on train dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression(optimization='tgd',eta=0.9, regularization='none', iterations=10, c=0.001)\n",
    "for col in targets:\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[col],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.4070450097847358  -  stroke\n",
      "Accuracy of Testing Dataset:  0.4090019569471624  -  heart_disease\n",
      "Accuracy of Testing Dataset:  0.410958904109589  -  hypertension\n"
     ]
    }
   ],
   "source": [
    "#trying larger iterations\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression(optimization='tgd',eta=0.9, regularization='none', iterations=500, c=0.001)\n",
    "for col in targets:\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[col],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, when performing no regularizations but using the default optimization of steepest descent, fewer iterations yielded a higher accuracies. This will be interesting to analyze as we increase the number of iterations and evaluate with different combinations of regularizations/optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Accuracy          Max Eta\n",
      "0.9393346379647749    0.57\n",
      "0.9315068493150684    0.57\n",
      "0.8845401174168297    0.57\n",
      "0.9354207436399217    0.31\n",
      "0.9275929549902152    0.31\n",
      "0.8806262230919765    0.31\n",
      "0.9305283757338552    0.3\n",
      "0.9227005870841487    0.3\n",
      "0.87573385518591    0.3\n",
      "0.9295499021526419    0.29\n",
      "\n",
      "Max Accuracy       Max c\n",
      "0.14090019569471623 0.0\n",
      "0.8845401174168297 0.001\n",
      "\n",
      "Final eta:  0.57\n",
      "Final c:  0.001\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "max_eta = -1\n",
    "accuracy_eta_list = dict()\n",
    "for val in range(58):\n",
    "    lr = LogisticRegression(optimization='tgd',eta=val/100, regularization='none', iterations=10, c=0.001)\n",
    "    for col in targets:\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test[col],yhat)\n",
    "        accuracy_eta_list[accuracy] = val/100\n",
    "new_list = dict(sorted(accuracy_eta_list.items(), key=operator.itemgetter(1), reverse=True)[:10])\n",
    "print('Max Accuracy          Max Eta')\n",
    "for key in new_list:\n",
    "    print(key, '  ', new_list[key])\n",
    "max_accuracy = list(new_list.keys())[0] \n",
    "max_eta = new_list[max_accuracy]\n",
    "    \n",
    "max_accuracy = -1\n",
    "print('\\nMax Accuracy       Max c')\n",
    "\n",
    "for val in range(100):\n",
    "    lr = LogisticRegression(optimization='tgd', eta=max_eta, regularization='none', iterations=500, c=val/1000.0)\n",
    "    for col in targets:\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test[col],yhat)\n",
    "    \n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        max_c = val/1000.0\n",
    "        print(max_accuracy, max_c)\n",
    "        \n",
    "print(\"\\nFinal eta: \", max_eta)\n",
    "print(\"Final c: \", max_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Steepest Gradient:  0.9393346379647749  - stroke\n",
      "Accuracy of Steepest Gradient:  0.9315068493150684  - heart_disease\n",
      "Accuracy of Steepest Gradient:  0.8845401174168297  - hypertension\n",
      "--- 0.1658796469370524 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Steepest Gradient L1 regularization:  0.0821917808219178  - stroke\n",
      "Accuracy of Steepest Gradient L1 regularization:  0.09001956947162426  - heart_disease\n",
      "Accuracy of Steepest Gradient L1 regularization:  0.13307240704500978  - hypertension\n",
      "--- 0.17160797119140625 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Steepest Gradient L2 regularization:  0.9393346379647749  - stroke\n",
      "Accuracy of Steepest Gradient L2 regularization:  0.9315068493150684  - heart_disease\n",
      "Accuracy of Steepest Gradient L2 regularization:  0.8845401174168297  - hypertension\n",
      "--- 0.17215474446614584 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Steepest Gradient L12 regularization:  0.541095890410959  - stroke\n",
      "Accuracy of Steepest Gradient L12 regularization:  0.5303326810176126  - heart_disease\n",
      "Accuracy of Steepest Gradient L12 regularization:  0.49902152641878667  - hypertension\n",
      "--- 0.16817593574523926 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "lr_s0 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='none', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s0.fit(X_train,y_train)\n",
    "yhat_s0 = lr_s0.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient: \", accuracy_score(y_test['stroke'],yhat_s0), ' - stroke')\n",
    "\n",
    "lr_hd0 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='none', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd0.fit(X_train,y_train)\n",
    "yhat_hd0 = lr_hd0.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient: \", accuracy_score(y_test['heart_disease'],yhat_hd0), ' - heart_disease')\n",
    "\n",
    "lr_h0 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='none', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h0.fit(X_train,y_train)\n",
    "yhat_h0 = lr_h0.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient: \", accuracy_score(y_test['hypertension'],yhat_h0), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "lr_s1 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='ridge', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s1.fit(X_train,y_train)\n",
    "yhat_s1 = lr_s1.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L1 regularization: \", accuracy_score(y_test['stroke'],yhat_s1), ' - stroke')\n",
    "\n",
    "lr_hd1 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='ridge', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd1.fit(X_train,y_train)\n",
    "yhat_hd1 = lr_hd1.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L1 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd1), ' - heart_disease')\n",
    "\n",
    "lr_h1 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='ridge', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h1.fit(X_train,y_train)\n",
    "yhat_h1 = lr_h1.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L1 regularization: \", accuracy_score(y_test['hypertension'],yhat_h1), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "lr_s2 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='lasso', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s2.fit(X_train,y_train)\n",
    "yhat_s2 = lr_s2.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L2 regularization: \", accuracy_score(y_test['stroke'],yhat_s2), ' - stroke')\n",
    "\n",
    "lr_hd2 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='lasso', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd2.fit(X_train,y_train)\n",
    "yhat_hd2 = lr_hd2.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L2 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd2), ' - heart_disease')\n",
    "\n",
    "lr_h2 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='lasso', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h2.fit(X_train,y_train)\n",
    "yhat_h2 = lr_h2.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L2 regularization: \", accuracy_score(y_test['hypertension'],yhat_h2), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "lr_s3 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='elastic_net', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s3.fit(X_train,y_train)\n",
    "yhat_s3 = lr_s3.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L12 regularization: \", accuracy_score(y_test['stroke'],yhat_s3), ' - stroke')\n",
    "\n",
    "lr_hd3 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='elastic_net', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd3.fit(X_train,y_train)\n",
    "yhat_hd3 = lr_hd3.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L12 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd3), ' - heart_disease')\n",
    "\n",
    "lr_h3 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='elastic_net', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h3.fit(X_train,y_train)\n",
    "yhat_h3 = lr_h3.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L12 regularization: \", accuracy_score(y_test['hypertension'],yhat_h3), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Accuracy          Max Eta\n",
      "0.9393346379647749    0.57\n",
      "0.8845401174168297    0.57\n",
      "0.0    0.57\n",
      "0.9315068493150684    0.56\n",
      "0.060665362035225046    0.56\n",
      "0.8816046966731899    0.55\n",
      "0.0684931506849315    0.54\n",
      "0.8072407045009785    0.53\n",
      "0.49608610567514677    0.52\n",
      "0.8679060665362035    0.52\n",
      "\n",
      "Max Accuracy       Max c\n",
      "0.8062622309197651 0.0\n",
      "0.8855185909980431 0.002\n",
      "\n",
      "Final eta:  0.57\n",
      "Final c:  0.002\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "max_eta = -1\n",
    "accuracy_eta_list = dict()\n",
    "for val in range(58):\n",
    "    lr = LogisticRegression(optimization='sgd',eta=val/100, regularization='none', iterations=10, c=0.001)\n",
    "    for col in targets:\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test[col],yhat)\n",
    "        accuracy_eta_list[accuracy] = val/100\n",
    "new_list = dict(sorted(accuracy_eta_list.items(), key=operator.itemgetter(1), reverse=True)[:10])\n",
    "print('Max Accuracy          Max Eta')\n",
    "for key in new_list:\n",
    "    print(key, '  ', new_list[key])\n",
    "max_accuracy = list(new_list.keys())[0] \n",
    "max_eta = new_list[max_accuracy]\n",
    "\n",
    "max_accuracy = -1\n",
    "print('\\nMax Accuracy       Max c')\n",
    "\n",
    "for val in range(100):\n",
    "    lr = LogisticRegression(optimization='sgd', eta=max_eta, regularization='none', iterations=500, c=val/1000.0)\n",
    "    for col in targets:\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test[col],yhat)\n",
    "    \n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        max_c = val/1000.0\n",
    "        print(max_accuracy, max_c)\n",
    "        \n",
    "print(\"\\nFinal eta: \", max_eta)\n",
    "print(\"Final c: \", max_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stochastic Gradient:  0.7915851272015656  - stroke\n",
      "Accuracy of Stochastic Gradient:  0.9315068493150684  - heart_disease\n",
      "Accuracy of Stochastic Gradient:  0.5205479452054794  - hypertension\n",
      "--- 0.03335269292195638 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Stochastic Gradient L1 regularization:  0.9393346379647749  - stroke\n",
      "Accuracy of Stochastic Gradient L1 regularization:  0.9315068493150684  - heart_disease\n",
      "Accuracy of Stochastic Gradient L1 regularization:  0.8786692759295499  - hypertension\n",
      "--- 0.03061366081237793 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Stochastic Gradient L2 regularization:  0.2827788649706458  - stroke\n",
      "Accuracy of Stochastic Gradient L2 regularization:  0.9315068493150684  - heart_disease\n",
      "Accuracy of Stochastic Gradient L2 regularization:  0.8845401174168297  - hypertension\n",
      "--- 0.03398569424947103 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Stochastic Gradient L12 regularization:  0.9393346379647749  - stroke\n",
      "Accuracy of Stochastic Gradient L12 regularization:  0.9315068493150684  - heart_disease\n",
      "Accuracy of Stochastic Gradient L12 regularization:  0.7925636007827789  - hypertension\n",
      "--- 0.0348658561706543 seconds ---\n"
     ]
    }
   ],
   "source": [
    "lr_s0 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='none', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s0.fit(X_train,y_train)\n",
    "yhat_s0 = lr_s0.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient: \", accuracy_score(y_test['stroke'],yhat_s0), ' - stroke')\n",
    "\n",
    "lr_hd0 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='none', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd0.fit(X_train,y_train)\n",
    "yhat_hd0 = lr_hd0.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient: \", accuracy_score(y_test['heart_disease'],yhat_hd0), ' - heart_disease')\n",
    "\n",
    "lr_h0 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='none', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h0.fit(X_train,y_train)\n",
    "yhat_h0 = lr_h0.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient: \", accuracy_score(y_test['hypertension'],yhat_h0), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "lr_s1 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='ridge', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s1.fit(X_train,y_train)\n",
    "yhat_s1 = lr_s1.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L1 regularization: \", accuracy_score(y_test['stroke'],yhat_s1), ' - stroke')\n",
    "\n",
    "lr_hd1 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='ridge', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd1.fit(X_train,y_train)\n",
    "yhat_hd1 = lr_hd1.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L1 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd1), ' - heart_disease')\n",
    "\n",
    "lr_h1 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='ridge', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h1.fit(X_train,y_train)\n",
    "yhat_h1 = lr_h1.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L1 regularization: \", accuracy_score(y_test['hypertension'],yhat_h1), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lr_s2 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='lasso', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s2.fit(X_train,y_train)\n",
    "yhat_s2 = lr_s2.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L2 regularization: \", accuracy_score(y_test['stroke'],yhat_s2), ' - stroke')\n",
    "\n",
    "lr_hd2 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='lasso', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd2.fit(X_train,y_train)\n",
    "yhat_hd2 = lr_hd2.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L2 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd2), ' - heart_disease')\n",
    "\n",
    "lr_h2 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='lasso', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h2.fit(X_train,y_train)\n",
    "yhat_h2 = lr_h2.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L2 regularization: \", accuracy_score(y_test['hypertension'],yhat_h2), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lr_s3 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='elastic_net', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s3.fit(X_train,y_train)\n",
    "yhat_s3 = lr_s3.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L12 regularization: \", accuracy_score(y_test['stroke'],yhat_s3), ' - stroke')\n",
    "\n",
    "lr_hd3 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='elastic_net', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd3.fit(X_train,y_train)\n",
    "yhat_hd3 = lr_hd3.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L12 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd3), ' - heart_disease')\n",
    "\n",
    "lr_h3 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='elastic_net', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h3.fit(X_train,y_train)\n",
    "yhat_h3 = lr_h3.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L12 regularization: \", accuracy_score(y_test['hypertension'],yhat_h3), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Accuracy          Max Eta\n",
      "0.026418786692759294    0.57\n",
      "0.021526418786692758    0.57\n",
      "0.022504892367906065    0.57\n",
      "0.029354207436399216    0.55\n",
      "0.025440313111545987    0.55\n",
      "0.02446183953033268    0.55\n",
      "0.030332681017612523    0.54\n",
      "0.03131115459882583    0.5\n",
      "0.03424657534246575    0.47\n",
      "0.033268101761252444    0.45\n",
      "\n",
      "Max Accuracy       Max c\n",
      "0.022504892367906065 0.0\n",
      "\n",
      "Final eta:  0.57\n",
      "Final c:  0.0\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "max_eta = -1\n",
    "accuracy_eta_list = dict()\n",
    "for val in range(58):\n",
    "    lr = LogisticRegression(optimization='newton',eta=val/100, regularization='none', iterations=10, c=0.001)\n",
    "    for col in targets:\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test[col],yhat)\n",
    "        accuracy_eta_list[accuracy] = val/100\n",
    "new_list = dict(sorted(accuracy_eta_list.items(), key=operator.itemgetter(1), reverse=True)[:10])\n",
    "print('Max Accuracy          Max Eta')\n",
    "for key in new_list:\n",
    "    print(key, '  ', new_list[key])\n",
    "max_accuracy = list(new_list.keys())[0] \n",
    "max_eta = new_list[max_accuracy]\n",
    "    \n",
    "max_accuracy = -1\n",
    "print('\\nMax Accuracy       Max c')\n",
    "\n",
    "for val in range(100):\n",
    "    lr = LogisticRegression(optimization='newton', eta=max_eta, regularization='none', iterations=10, c=val/1000.0)\n",
    "    for col in targets:\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test[col],yhat)\n",
    "    \n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        max_c = val/1000.0\n",
    "        print(max_accuracy, max_c)\n",
    "        \n",
    "print(\"\\nFinal eta: \", max_eta)\n",
    "print(\"Final c: \", max_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Newton's Method:  0.021526418786692758  - stroke\n",
      "Accuracy of Newton's Method:  0.026418786692759294  - heart_disease\n",
      "Accuracy of Newton's Method:  0.022504892367906065  - hypertension\n",
      "--- 0.9281144936879476 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Newton's Method L1 regularization:  0.021526418786692758  - stroke\n",
      "Accuracy of Newton's Method L1 regularization:  0.026418786692759294  - heart_disease\n",
      "Accuracy of Newton's Method L1 regularization:  0.022504892367906065  - hypertension\n",
      "--- 0.9562834103902181 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Newton's Method L2 regularization:  0.021526418786692758  - stroke\n",
      "Accuracy of Newton's Method L2 regularization:  0.026418786692759294  - heart_disease\n",
      "Accuracy of Newton's Method L2 regularization:  0.022504892367906065  - hypertension\n",
      "--- 0.9530626932779948 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Newton's Method L12 regularization:  0.021526418786692758  - stroke\n",
      "Accuracy of Newton's Method L12 regularization:  0.026418786692759294  - heart_disease\n",
      "Accuracy of Newton's Method L12 regularization:  0.022504892367906065  - hypertension\n",
      "--- 0.975908358891805 seconds ---\n"
     ]
    }
   ],
   "source": [
    "lr_s0 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='none', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s0.fit(X_train,y_train)\n",
    "yhat_s0 = lr_s0.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method: \", accuracy_score(y_test['stroke'],yhat_s0), ' - stroke')\n",
    "\n",
    "lr_hd0 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='none', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd0.fit(X_train,y_train)\n",
    "yhat_hd0 = lr_hd0.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method: \", accuracy_score(y_test['heart_disease'],yhat_hd0), ' - heart_disease')\n",
    "\n",
    "lr_h0 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='none', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h0.fit(X_train,y_train)\n",
    "yhat_h0 = lr_h0.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method: \", accuracy_score(y_test['hypertension'],yhat_h0), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "lr_s1 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='ridge', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s1.fit(X_train,y_train)\n",
    "yhat_s1 = lr_s1.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L1 regularization: \", accuracy_score(y_test['stroke'],yhat_s1), ' - stroke')\n",
    "\n",
    "lr_hd1 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='ridge', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd1.fit(X_train,y_train)\n",
    "yhat_hd1 = lr_hd1.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L1 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd1), ' - heart_disease')\n",
    "\n",
    "lr_h1 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='ridge', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h1.fit(X_train,y_train)\n",
    "yhat_h1 = lr_h1.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L1 regularization: \", accuracy_score(y_test['hypertension'],yhat_h1), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "lr_s2 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='lasso', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s2.fit(X_train,y_train)\n",
    "yhat_s2 = lr_s2.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L2 regularization: \", accuracy_score(y_test['stroke'],yhat_s2), ' - stroke')\n",
    "\n",
    "lr_hd2 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='lasso', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd2.fit(X_train,y_train)\n",
    "yhat_hd2 = lr_hd2.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L2 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd2), ' - heart_disease')\n",
    "\n",
    "lr_h2 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='lasso', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h2.fit(X_train,y_train)\n",
    "yhat_h2 = lr_h2.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L2 regularization: \", accuracy_score(y_test['hypertension'],yhat_h2), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "lr_s3 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='elastic_net', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s3.fit(X_train,y_train)\n",
    "yhat_s3 = lr_s3.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L12 regularization: \", accuracy_score(y_test['stroke'],yhat_s3), ' - stroke')\n",
    "\n",
    "lr_hd3 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='elastic_net', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd3.fit(X_train,y_train)\n",
    "yhat_hd3 = lr_hd3.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L12 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd3), ' - heart_disease')\n",
    "\n",
    "lr_h3 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='elastic_net', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h3.fit(X_train,y_train)\n",
    "yhat_h3 = lr_h3.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L12 regularization: \", accuracy_score(y_test['hypertension'],yhat_h3), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not think there was any data snooping because our method of deciding the parameters was optimized. We loop over a range of possibilities for the values of c and eta and keep the best functioning parameters that yield the highest accuracy. Because of this, we believe our method is justified and prevents data snooping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whenever I tried to run and visualize the graph, my computer would freeze so I could not graph it unfortunately :(\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "titles = ['Optimization/Regularization', 'Training Time(s)', 'Accuracy']\n",
    "columns = [\n",
    "    ['Steepest Gradient Descent', 'Steepest Gradient Descent with L1 Regularization', 'Steepest Gradient Descent with L2 Regularization', 'Steepest Gradient Descent with L1 and L2 Regularization',\n",
    "     'Stochiastic Gradient Descent', 'Stochiastic Gradient Descent with L1 Regularization', 'Stochiastic Gradient Descent with L2 Regularization', 'Stochiastic Gradient Descent with L1 and L2 Regularization'\n",
    "     'Newton\\'s Method', 'Newton\\'s Method with L1 Regularization', 'Newton\\'s Method with L2 Regularization', 'Newton\\'s Method with L1 and L2 Regularization'],\n",
    "    [0.16378935178120932, 0.15881760915120444, 0.161603053410848, 0.15591899553934732, 0.03335269292195638, 0.03061366081237793, 0.03398569424947103, 0.0348658561706543, 0.03398569424947103, 0.0348658561706543, 0.9281144936879476, 0.9562834103902181, 0.9530626932779948, 0.975908358891805],\n",
    "    [.9393346379647749, .0821917808219178, .9393346379647749, .9393346379647749, .7915851272015656, .9393346379647749, .2827788649706458, .9393346379647749, .021526418786692758, .021526418786692758, .021526418786692758],\n",
    "]\n",
    "\n",
    "fig = go.Figure(data=[go.Table(header=dict(values=titles),\n",
    "                 cells=dict(values=columns))\n",
    "                     ])\n",
    "fig = fig.update_layout(title=\"Stroke\")\n",
    "fig.show()\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "# Training Time of Steepest Gradient: 0.16378935178120932 seconds\n",
    "# Training Time of Steepest Gradient L1 regularization: 0.15881760915120444 seconds\n",
    "# Training Time of Steepest Gradient L2 regularization: 0.161603053410848 seconds\n",
    "# Training Time of Steepest Gradient L12 regularization:  0.15591899553934732 seconds\n",
    "\n",
    "# Training Time of Stochastic Gradient: 0.03335269292195638 seconds\n",
    "# Training Time of Stochastic Gradient L1 regularization: 0.03061366081237793 seconds\n",
    "# Training Time of Stochastic Gradient L2 regularization: 0.03398569424947103 seconds\n",
    "# Training Time of Stochastic Gradient L12 regularization: 0.0348658561706543 seconds\n",
    "\n",
    "# Training Time of Newton's Method: 0.9281144936879476 seconds\n",
    "# Training Time of Newton's Method L1 regularization: 0.9562834103902181 seconds\n",
    "# Training Time of Newton's Method L2 regularization: 0.9530626932779948 seconds\n",
    "# Training Time of Newton's Method L12 regularization: 0.975908358891805 seconds\n",
    "\n",
    "# Training Time of SKL (stroke): 0.04746890068054199 seconds\n",
    "# Training Time of SKL (heart_disease): 0.0448000431060791 seconds\n",
    "# Training Time of SKL (hyptertension): 0.04042816162109375 seconds\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "# Accuracy of Steepest Gradient:  0.9393346379647749  - stroke\n",
    "# Accuracy of Steepest Gradient L1 regularization:  0.0821917808219178  - stroke\n",
    "# Accuracy of Steepest Gradient L2 regularization:  0.9393346379647749  - stroke\n",
    "# Accuracy of Steepest Gradient L12 regularization:  0.9393346379647749  - stroke\n",
    "\n",
    "# Accuracy of Stochastic Gradient:  0.7915851272015656  - stroke\n",
    "# Accuracy of Stochastic Gradient L1 regularization:  0.9393346379647749  - stroke\n",
    "# Accuracy of Stochastic Gradient L2 regularization:  0.2827788649706458  - stroke\n",
    "# Accuracy of Stochastic Gradient L12 regularization:  0.9393346379647749  - stroke\n",
    "\n",
    "# Accuracy of Newton's Method:  0.021526418786692758  - stroke\n",
    "# Accuracy of Newton's Method L1 regularization:  0.021526418786692758  - stroke\n",
    "# Accuracy of Newton's Method L2 regularization:  0.021526418786692758  - stroke\n",
    "# Accuracy of Newton's Method L12 regularization:  0.021526418786692758  - stroke\n",
    "\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "# Accuracy of Steepest Gradient:  0.9315068493150684  - heart_disease\n",
    "# Accuracy of Steepest Gradient L1 regularization:  0.09001956947162426  - heart_disease\n",
    "# Accuracy of Steepest Gradient L2 regularization:  0.9315068493150684  - heart_disease\n",
    "# Accuracy of Steepest Gradient L12 regularization:  0.5303326810176126  - heart_disease\n",
    "\n",
    "# Accuracy of Stochastic Gradient:  0.9315068493150684  - heart_disease\n",
    "# Accuracy of Stochastic Gradient L1 regularization:  0.9315068493150684  - heart_disease\n",
    "# Accuracy of Stochastic Gradient L2 regularization:  0.9315068493150684  - heart_disease\n",
    "# Accuracy of Stochastic Gradient L12 regularization:  0.9315068493150684  - heart_disease\n",
    "\n",
    "# Accuracy of Newton's Method:  0.026418786692759294  - heart_disease\n",
    "# Accuracy of Newton's Method L1 regularization:  0.026418786692759294  - heart_disease\n",
    "# Accuracy of Newton's Method L2 regularization:  0.026418786692759294  - heart_disease\n",
    "# Accuracy of Newton's Method L12 regularization:  0.026418786692759294  - heart_disease\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "# Accuracy of Steepest Gradient:  0.8845401174168297  - hypertension\n",
    "# Accuracy of Steepest Gradient L1 regularization:  0.13307240704500978  - hypertension\n",
    "# Accuracy of Steepest Gradient L2 regularization:  0.8845401174168297  - hypertension\n",
    "# Accuracy of Steepest Gradient L12 regularization:  0.49902152641878667  - hypertension\n",
    "\n",
    "\n",
    "# Accuracy of Stochastic Gradient:  0.5205479452054794  - hypertension\n",
    "# Accuracy of Stochastic Gradient L1 regularization:  0.8786692759295499  - hypertension\n",
    "# Accuracy of Stochastic Gradient L2 regularization:  0.8845401174168297  - hypertension\n",
    "# Accuracy of Stochastic Gradient L12 regularization:  0.7925636007827789  - hypertension\n",
    "\n",
    "\n",
    "# Accuracy of Newton's Method:  0.022504892367906065  - hypertension\n",
    "# Accuracy of Newton's Method L1 regularization:  0.022504892367906065  - hypertension\n",
    "# Accuracy of Newton's Method L2 regularization:  0.022504892367906065  - hypertension\n",
    "# Accuracy of Newton's Method L12 regularization:  0.022504892367906065  - hypertension\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "# Accuracy of SKL (stroke): 0.9393346379647749\n",
    "# Accuracy of SKL (heart_disease): 0.9305283757338552\n",
    "# Accuracy of SKL (hypertension): 0.8835616438356164"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the list of data, we can see how the stochastic gradient had the fastest training time, even faster than SKL, whereas Newton's was the slowest. The data also shows how Newton's method consistently had the lowest accuracy for all of our target datasets. This could mean that Newton's method is the worst optimization for our dataset, or perhaps the accuracy could have improved with a different implementation. Because of the very long time span Newton's method took to run, we could only perform a few tests compared to the other optimizations. We can also see how Steepest Gradient was more accurate for the stroke and heart disease targets, whereas Stochastic Gradient Descent was more accurate for the hypertension target. Steepest Gradient with L1 regularization proved to be the least accurate consistently for all target sets. This could mean that the ridge regularization is not as accurate as the other regularizations. There were varying data depending on the target, but overall, we can conclude that all the Steepest Gradient optimizations (except the one with L2 regularization) were the most accurate, along with SKL. \n",
    "\n",
    "The highest accuracy was ~94%, which included \n",
    "* Steepest Gradient (no regularization)\n",
    "* Steepest Gradient (L1 regularization)\n",
    "* Steepest Gradient (L12 regularization)\n",
    "* SKL on stroke target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9393346379647749  -  stroke\n",
      "--- 0.04746890068054199 seconds --- \n",
      "\n",
      "0.9305283757338552  -  heart_disease\n",
      "--- 0.0448000431060791 seconds --- \n",
      "\n",
      "0.8835616438356164  -  hypertension\n",
      "--- 0.04042816162109375 seconds --- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fidelianawar/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "/Users/fidelianawar/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "/Users/fidelianawar/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "for col in targets:\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train, y_train[col])\n",
    "    clf.predict(X_test)\n",
    "    clf.predict_proba(X_test)\n",
    "    print(clf.score(X_test,y_test[col]), ' - ', col)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Which implementation of logistic regression would you advise be used in a deployed machine learning model, your implementation or scikit-learn (or other third party)? Why?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe the Scikit-learn's logistic regression implementation should be used over our implementation. SKLearn's implementation is more optimized because it is written in C, which runs faster than Python. It is also more reliable because it is an open-source library with more frequent updates, making the library maintainable. This allows for new optimizations in the sklearn library to have a minimal amount of work necessary to improve the efficiency and accuracy of the model. \n",
    "\n",
    "Scikit-learn's implementation was also much faster than ours, especially for the Steepest Gradient and Newton's Method optimizations. This is important because the faster the implementation can train/run, the faster it can update a model, making it cheaper to scale.\n",
    "\n",
    "Overall, the accuracy of the SKL implementation was highly accurate for all target datasets. With the combination of speed and maintainability, we believe their implementation would be more efficient in an ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>You have free reign to provide additional analyses. <b>One idea</b>: Update the code to use either \"one-versus-all\" or \"one-versus-one\" extensions of binary to multi-class classification. </li>\n",
    "    <li><b>Required for 7000 level students</b>: Choose ONE of the following:\n",
    "    <ul>\n",
    "        <li><b>Option One</b>: Implement an optimization technique for logistic regression using <b>mean square error</b> as your objective function (instead of binary cross entropy). Derive the gradient updates for the Hessian and use Newton's method to update the values of \"w\". Then answer, is this process better than using binary cross entropy? </li>\n",
    "        <li><b>Option Two</b>: Implement the BFGS algorithm from scratch to optimize logistic regression. That is, use BFGS without the use of an external package (for example, do not use SciPy). Compare your performance accuracy and runtime to the BFGS implementation in SciPy (that we used in lecture). </li>\n",
    "    </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, optimization='bgd', eta = 0.01, iterations=20, regularization='none', c=0):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.opt = optimization\n",
    "        self.reg = regularization\n",
    "        self.c = c\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from numpy.linalg import pinv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    #optimization methods\n",
    "    def _get_gradient(self, X, y):\n",
    "        \n",
    "        gradient = None\n",
    "        if self.opt == 'tgd': gradient = self.steepest_descent\n",
    "        elif self.opt == 'sgd': gradient = self.stochastic_gradient_descent\n",
    "        elif self.opt == 'mse_newton': gradient = self.mse_newton\n",
    "        return gradient(X,y)\n",
    "    \n",
    "    def steepest_descent(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += self.c * self._get_reg_gradient()\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def stochastic_gradient_descent(self,X,y):\n",
    "       # idx = int(np.random.rand()*len(y)) # grab random instance\\\n",
    "        idx = np.random.randint(len(y))\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += self.c * self._get_reg_gradient()\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def mse_newton(self, X, y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.c  # calculate the hessian\n",
    "       \n",
    "        temp = np.square(np.subtract(y, g))\n",
    "\n",
    "        mse = (np.sum(X * temp[:,np.newaxis], axis=0)) / len(temp)\n",
    "        \n",
    "#        mse = mean_squared_error(y,g)\n",
    "        \n",
    "#         ydiff = y-g # get y difference\n",
    "#         gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        \n",
    "        gradient = mse.reshape(self.w_.shape)\n",
    "        gradient[1:] +=  self.c * self._get_reg_gradient()\n",
    "        \n",
    "        return pinv(hessian) @ gradient       \n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    #regularization methods\n",
    "    def _get_reg_gradient(self):\n",
    "        #no regularization\n",
    "        if self.reg == 'none':\n",
    "            return self.w_[1:]\n",
    "        elif self.reg == 'ridge':\n",
    "            return -2 * self.w_[1:]\n",
    "        elif self.reg == 'lasso':\n",
    "            return np.sign(self.w_[1:])\n",
    "        elif self.reg == 'elastic_net':\n",
    "            return -2 * self.w_[1:] + np.sign(self.w_[1:])\n",
    "    \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, optimization, eta, iterations, regularization, c=0):\n",
    "    \n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.opt = optimization\n",
    "        self.reg = regularization\n",
    "        self.encodings = {}\n",
    "        self.c = c\n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "    \n",
    "    def fit(self,X,y): # y is a hash of target columns\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for name, target in y.items():\n",
    "            blr = BinaryLogisticRegression(self.opt, self.eta, self.iters, self.reg, self.c )\n",
    "            blr.fit(X,target)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "#             if np.count_nonzero(blr.predict_proba(X)) > 0:\n",
    "#                 print(\"Not zero\")\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "            \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "    \n",
    "lr = LogisticRegression('tgd',0.01, 100, 'ridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate on train dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression(optimization='mse_newton',eta=0.9, regularization='none', iterations=100, c=0)\n",
    "for col in targets:\n",
    "    print(col)\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset (50 iterations): \", accuracy_score(y_test[col],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
