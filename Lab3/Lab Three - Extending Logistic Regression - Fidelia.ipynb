{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Three - Extending Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will compare the performance of logistic regression optimization programmed in scikit-learn and via your own implementation. You will also modify the optimization procedure for logistic regression. \n",
    "\n",
    "This report is worth 10% of the final grade. Please upload a report (<b>one per team</b>) with all code used, visualizations, and text in a rendered Jupyter notebook. Any visualizations that cannot be embedded in the notebook, please provide screenshots of the output. The results should be reproducible using your report. Please carefully describe every assumption and every step in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dataset Selection</b>\n",
    "\n",
    "Select a dataset identically to the way you selected for the lab one (i.e., table data). You are not required to use the same dataset that you used in the past, but you are encouraged. You must identify a classification task from the dataset that contains <b>three or more classes to predict</b>. That is it cannot be a binary classification; it must be multi-class prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation and Overview (3pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>[<b>2 points</b>] Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the classification task is and what parties would be interested in the results. For example, would the model be deployed or used mostly for offline analysis? </li>\n",
    "    <li>[<b>.5 points</b>] (<i>mostly the same processes as from previous labs</i>) Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). </li>\n",
    "    <li>[<b>.5 points</b>] Divide you data into training and testing data using an 80% training and 20% testing split. Use the cross validation modules that are part of scikit-learn. <b>Argue \"for\" or \"against\" splitting your data using an 80/20 split. That is, why is the 80/20 split appropriate (or not) for your dataset?</b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case\n",
    "\n",
    "Our task will be looking at a patients information and determining whether they are likely to have a stroke, heart disease, or hypertension. The use-case for this classifier would be to flag at-risk patients and enable some kind of response to be made to prevent serious medical emergencies that these conditions might cause or prevent the conditions in the first place.\n",
    "\n",
    "For example, if a person were to be flagged as very likely to have a stroke, the doctor could contact the patient in an attempt to prevent the stroke by prescribing them medication or alerting the patient's family to monitor them in case they were to have a stroke. Similar actions could be taken for hypertension and heart disease.\n",
    "\n",
    "Alernatively, some kind of application could be made to allow people to enter their information and determine how at risk they might be for these conditions, giving people more clear information about their health and the issues that are likely to affect them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 1.1.3\n",
      "Numpy: 1.19.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>work_type</th>\n",
       "      <th>residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9046</td>\n",
       "      <td>Male</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>formerly smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51676</td>\n",
       "      <td>Female</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>202.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31112</td>\n",
       "      <td>Male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Rural</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60182</td>\n",
       "      <td>Female</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>smokes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1665</td>\n",
       "      <td>Female</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender   age  hypertension  heart_disease ever_married  \\\n",
       "0   9046    Male  67.0             0              1          Yes   \n",
       "1  51676  Female  61.0             0              0          Yes   \n",
       "2  31112    Male  80.0             0              1          Yes   \n",
       "3  60182  Female  49.0             0              0          Yes   \n",
       "4   1665  Female  79.0             1              0          Yes   \n",
       "\n",
       "       work_type residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
       "0        Private          Urban             228.69  36.6  formerly smoked   \n",
       "1  Self-employed          Rural             202.21   NaN     never smoked   \n",
       "2        Private          Rural             105.92  32.5     never smoked   \n",
       "3        Private          Urban             171.23  34.4           smokes   \n",
       "4  Self-employed          Rural             174.12  24.0     never smoked   \n",
       "\n",
       "   stroke  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing packages and reading in dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print('Pandas:', pd.__version__)\n",
    "print('Numpy:',  np.__version__)\n",
    "\n",
    "raw_data = pd.read_csv('healthcare-dataset-stroke-data.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping categorical column 'work_type'; not very useful and\n",
    "# doesn't translate nicely into ordinal numbers\n",
    "df = raw_data.drop('work_type', axis = 1)\n",
    "\n",
    "# Dropping 1 observation of person with gender 'Other' to simplify\n",
    "# using the gender column to calculate, impute, or visualize\n",
    "df.drop(df[df.gender == 'Other'].index, inplace=True)\n",
    "\n",
    "# Making values' format consistent\n",
    "for c in df.columns:\n",
    "    if df[c].dtype == 'object':\n",
    "        df[c] = df[c].str.lower()\n",
    "\n",
    "# Adding numbers to smoking_status values to order them properly\n",
    "# when they will get passed through the SKLearn LabelEncoder\n",
    "df.smoking_status.replace(to_replace= ['never smoked', 'formerly smoked', 'smokes', 'Unknown'],\n",
    "                          value     = ['0_never_smoked', '1_formerly_smoked', '2_smokes', '3_Unknown'],\n",
    "                          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9046</td>\n",
       "      <td>1</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51676</td>\n",
       "      <td>0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>202.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31112</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60182</td>\n",
       "      <td>0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1665</td>\n",
       "      <td>0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender   age  hypertension  heart_disease  ever_married  \\\n",
       "0   9046       1  67.0             0              1             1   \n",
       "1  51676       0  61.0             0              0             1   \n",
       "2  31112       1  80.0             0              1             1   \n",
       "3  60182       0  49.0             0              0             1   \n",
       "4   1665       0  79.0             1              0             1   \n",
       "\n",
       "   residence_type  avg_glucose_level   bmi  smoking_status  stroke  \n",
       "0               1             228.69  36.6             1.0       1  \n",
       "1               0             202.21   NaN             0.0       1  \n",
       "2               0             105.92  32.5             0.0       1  \n",
       "3               1             171.23  34.4             2.0       1  \n",
       "4               0             174.12  24.0             0.0       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encoding all of the non-numeric columns\n",
    "le = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        le[col] = LabelEncoder()\n",
    "        df[col] = le[col].fit_transform(df[col])\n",
    "\n",
    "# Call le[col].inverse_transform(df[col]) for any column name\n",
    "# to convert numbers back to their labels\n",
    "\n",
    "# Converting all 'Unknown' values in smoking status to NaN so\n",
    "# that we can impute the missing values.\n",
    "df.smoking_status.mask(df.smoking_status == 3, np.nan, inplace=True)\n",
    "               \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values\n",
    "from sklearn.impute import KNNImputer\n",
    "import copy\n",
    "\n",
    "knn = KNNImputer(n_neighbors=3)\n",
    "\n",
    "# Imputing on all columns except id\n",
    "columns = list(df.columns)\n",
    "columns.remove('id')\n",
    "\n",
    "df_imputed = copy.deepcopy(df)\n",
    "df_imputed[columns] = knn.fit_transform(df[columns])\n",
    "\n",
    "# Rounding imputed values to be compatible with LabelEncoder\n",
    "# for smoking_status and to match the format of other values\n",
    "# for bmi\n",
    "df_imputed.smoking_status = df_imputed.smoking_status.apply(lambda x: round(x, 0))\n",
    "df_imputed.bmi = df_imputed.bmi.apply(lambda x: round(x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using df_imputed as the primary dataset\n",
    "df = df_imputed\n",
    "\n",
    "# Changing columns modified by KNN Imputer back to integers from floats\n",
    "columns = [\n",
    "    'gender',\n",
    "    'hypertension',\n",
    "    'heart_disease',\n",
    "    'ever_married',\n",
    "    'residence_type',\n",
    "    'smoking_status',\n",
    "    'stroke'\n",
    "]\n",
    "\n",
    "for col in columns:\n",
    "    df[col] = df[col].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prep this dataset, one attribute was removed due to it being relatively unimportant and not encoding nicely into an ordinal set of integers. All categorical variables were converted to numeric data using SKLearn's LabelEncoder class. Missing values for bmi and smoking_status were imputed using KNN Imputer. One record was dropped for being the only entry with gender 'Other'. Removing this record will make visualizing the gender data simpler and will have little impact on the training, as having an outlier like that might cause other attributes to be slightly undervalued in comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a table of the LabelEncoder encoded variables.\n",
    "\n",
    "| value | gender | ever_married | residence_type | smoking_status    |\n",
    "|-------|--------|--------------|----------------|-------------------|\n",
    "| 0     | female | no           | rural          | 0_never_smoked    |\n",
    "| 1     | male   | yes          | urban          | 1_formerly_smoked |\n",
    "| 2     |   -    |      -       |       -        | 2_smokes          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9046</td>\n",
       "      <td>1</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51676</td>\n",
       "      <td>0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>202.21</td>\n",
       "      <td>30.9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31112</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60182</td>\n",
       "      <td>0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1665</td>\n",
       "      <td>0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender   age  hypertension  heart_disease  ever_married  \\\n",
       "0   9046       1  67.0             0              1             1   \n",
       "1  51676       0  61.0             0              0             1   \n",
       "2  31112       1  80.0             0              1             1   \n",
       "3  60182       0  49.0             0              0             1   \n",
       "4   1665       0  79.0             1              0             1   \n",
       "\n",
       "   residence_type  avg_glucose_level   bmi  smoking_status  stroke  \n",
       "0               1             228.69  36.6               1       1  \n",
       "1               0             202.21  30.9               0       1  \n",
       "2               0             105.92  32.5               0       1  \n",
       "3               1             171.23  34.4               2       1  \n",
       "4               0             174.12  24.0               0       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = list(df.columns)\n",
    "columns.remove('id')\n",
    "targets = ['stroke', 'heart_disease', 'hypertension']\n",
    "\n",
    "for col in targets:\n",
    "    columns.remove(col)\n",
    "\n",
    "#splitting into train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=.20, random_state=42)\n",
    "\n",
    "X_test  = test[columns].to_numpy()\n",
    "X_train = train[columns].to_numpy()\n",
    "y_test  = {}\n",
    "y_train = {}\n",
    "\n",
    "for col in targets:\n",
    "    y_test[col]  = test[col].to_numpy()\n",
    "    y_train[col] = train[col].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (5pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>The implementation of logistic regression must be written only from the examples given to you by the instructor. No credit will be assigned to teams that copy implementations from another source, regardless of if the code is properly cited.</li>\n",
    "    <li>[<b>2 points</b>] Create a custom, one-versus-all logistic regression classifier using numpy and scipy to optimize. Use object oriented conventions identical to scikit-learn. You should start with the template developed by the instructor in the course. You should add the following functionality to the logistic regression classifier:\n",
    "    <ul>\n",
    "        <li>Ability to choose optimization technique when class is instantiated: either steepest descent, stochastic gradient descent, or Newton's method. </li>\n",
    "        <li>Update the gradient calculation to include a customizable regularization term (either using no regularization, L1 regularization, L2 regularization, or both L1 and L2 regularization). Associate a cost with the regularization term, \"C\", that can be adjusted when the class is instantiated.  </li>\n",
    "    </ul>\n",
    "    </li>\n",
    "    <li>[<b>1.5 points</b>] Train your classifier to achieve good generalization performance. That is, adjust the <b>optimization technique</b> and the value of the <b>regularization term \"C\"</b> to achieve the best performance on your test set. Visualize the performance of the classifier versus the parameters you investigated. Is your method of selecting parameters justified? That is, do you think there is any \"data snooping\" involved with this method of selecting parameters?</li>\n",
    "    <li>[<b>1.5 points</b>] Compare the performance of your \"best\" logistic regression optimization procedure to the procedure used in scikit-learn. Visualize the performance differences in terms of training time and classification performance. <b>Discuss the results</b>. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, optimization='bgd', eta = 0.01, iterations=20, regularization='ridge', c=0):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.opt = optimization\n",
    "        self.reg = regularization\n",
    "        self.c = c\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    #optimization methods\n",
    "    def _get_gradient(self, X, y):\n",
    "        \n",
    "        gradient = None\n",
    "        if self.opt == 'tgd': gradient = self.steepest_descent\n",
    "        elif self.opt == 'sgd': gradient = self.stochastic_gradient_descent\n",
    "        elif self.opt == 'newton': gradient = self.newton\n",
    "        return gradient(X,y)\n",
    "    \n",
    "    def steepest_descent(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += self.c * self._get_reg_gradient()\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def stochastic_gradient_descent(self,X,y):\n",
    "       # idx = int(np.random.rand()*len(y)) # grab random instance\\\n",
    "        idx = np.random.randint(len(y))\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += self.c * self._get_reg_gradient()\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def newton(self, X, y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.c  # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] +=  self._get_reg_gradient()\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    #regularization methods\n",
    "    def _get_reg_gradient(self):\n",
    "        if self.reg == 'ridge':\n",
    "            return -2 * self.w_[1:]\n",
    "        elif self.reg == 'lasso':\n",
    "            return np.sign(self.w_[1:])\n",
    "        elif self.reg == 'elastic_net':\n",
    "            return -2 * self.w_[1:] + np.sign(self.w_[1:])\n",
    "    \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logisitic Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, optimization, eta, iterations, regularization, c=0):\n",
    "    \n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.opt = optimization\n",
    "        self.reg = regularization\n",
    "        self.encodings = {}\n",
    "        self.c = c\n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "    \n",
    "    def fit(self,X,y): # y is a hash of target columns\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for name, target in y.items():\n",
    "            blr = BinaryLogisticRegression(self.opt, self.eta, self.iters, self.reg, self.c )\n",
    "            blr.fit(X,target)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "#             if np.count_nonzero(blr.predict_proba(X)) > 0:\n",
    "#                 print(\"Not zero\")\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "            \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "    \n",
    "lr = LogisticRegression('tgd',0.01, 100, 'ridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset (10 iterations):  0.9393346379647749  -  stroke\n",
      "Accuracy of Testing Dataset (10 iterations):  0.9315068493150684  -  heart_disease\n",
      "Accuracy of Testing Dataset (10 iterations):  0.8845401174168297  -  hypertension\n"
     ]
    }
   ],
   "source": [
    "#evaluate on train dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression(optimization='tgd',eta=0.9, regularization='ridge', iterations=10, c=0.001)\n",
    "for col in targets:\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset (10 iterations): \", accuracy_score(y_test[col],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset (500 iterations):  0.9393346379647749  -  stroke\n",
      "Accuracy of Testing Dataset (500 iterations):  0.9315068493150684  -  heart_disease\n",
      "Accuracy of Testing Dataset (500 iterations):  0.8845401174168297  -  hypertension\n"
     ]
    }
   ],
   "source": [
    "#can we do better with more iterations?\n",
    "lr = LogisticRegression(optimization='tgd',eta=0.9, regularization='ridge', iterations=500, c=0.001)\n",
    "for col in targets:\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset (500 iterations): \", accuracy_score(y_test[col],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_accuracy max_eta\n",
      "0.9393346379647749 0.0\n",
      "0.9315068493150684 0.0\n",
      "0.8845401174168297 0.0\n",
      "0.0 0.01\n",
      "0.0 0.01\n",
      "0.0 0.01\n",
      "0.0 0.02\n",
      "0.0 0.02\n",
      "0.0 0.02\n",
      "0.0 0.03\n",
      "0.0 0.03\n",
      "0.0 0.03\n",
      "0.0 0.04\n",
      "0.0 0.04\n",
      "0.0 0.04\n",
      "0.0 0.05\n",
      "0.0 0.05\n",
      "0.0 0.05\n",
      "0.0 0.06\n",
      "0.0 0.06\n",
      "0.0 0.06\n",
      "0.0009784735812133072 0.07\n",
      "0.0029354207436399216 0.07\n",
      "0.0029354207436399216 0.07\n",
      "0.01761252446183953 0.08\n",
      "0.018590998043052837 0.08\n",
      "0.01761252446183953 0.08\n",
      "0.06653620352250489 0.09\n",
      "0.06653620352250489 0.09\n",
      "0.06164383561643835 0.09\n",
      "0.09197651663405088 0.1\n",
      "0.09001956947162426 0.1\n",
      "0.0812133072407045 0.1\n",
      "0.09980430528375733 0.11\n",
      "0.09784735812133072 0.11\n",
      "0.08708414872798434 0.11\n",
      "0.12035225048923678 0.12\n",
      "0.11741682974559686 0.12\n",
      "0.1086105675146771 0.12\n",
      "0.14481409001956946 0.13\n",
      "0.14090019569471623 0.13\n",
      "0.13209393346379647 0.13\n",
      "0.16634050880626222 0.14\n",
      "0.162426614481409 0.14\n",
      "0.15362035225048923 0.14\n",
      "0.2191780821917808 0.15\n",
      "0.2172211350293542 0.15\n",
      "0.2035225048923679 0.15\n",
      "0.29354207436399216 0.16\n",
      "0.2857142857142857 0.16\n",
      "0.2710371819960861 0.16\n",
      "0.37084148727984345 0.17\n",
      "0.36203522504892366 0.17\n",
      "0.34540117416829746 0.17\n",
      "0.46966731898238745 0.18\n",
      "0.45792563600782776 0.18\n",
      "0.43737769080234834 0.18\n",
      "0.5450097847358122 0.19\n",
      "0.5293542074363993 0.19\n",
      "0.5088062622309197 0.19\n",
      "0.6311154598825832 0.2\n",
      "0.6144814090019569 0.2\n",
      "0.589041095890411 0.2\n",
      "0.6947162426614482 0.21\n",
      "0.6859099804305284 0.21\n",
      "0.6516634050880626 0.21\n",
      "0.7495107632093934 0.22\n",
      "0.7416829745596869 0.22\n",
      "0.7054794520547946 0.22\n",
      "0.8091976516634051 0.23\n",
      "0.8023483365949119 0.23\n",
      "0.764187866927593 0.23\n",
      "0.8473581213307241 0.24\n",
      "0.8385518590998043 0.24\n",
      "0.799412915851272 0.24\n",
      "0.8708414872798435 0.25\n",
      "0.863013698630137 0.25\n",
      "0.821917808219178 0.25\n",
      "0.8972602739726028 0.26\n",
      "0.8904109589041096 0.26\n",
      "0.8434442270058709 0.26\n",
      "0.9080234833659491 0.27\n",
      "0.9001956947162426 0.27\n",
      "0.8542074363992173 0.27\n",
      "0.9197651663405088 0.28\n",
      "0.9119373776908023 0.28\n",
      "0.8649706457925636 0.28\n",
      "0.9275929549902152 0.29\n",
      "0.9197651663405088 0.29\n",
      "0.87279843444227 0.29\n",
      "0.9305283757338552 0.3\n",
      "0.9227005870841487 0.3\n",
      "0.87573385518591 0.3\n",
      "0.9334637964774951 0.31\n",
      "0.9256360078277887 0.31\n",
      "0.8786692759295499 0.31\n",
      "0.9383561643835616 0.32\n",
      "0.9305283757338552 0.32\n",
      "0.8835616438356164 0.32\n",
      "0.9393346379647749 0.33\n",
      "0.9315068493150684 0.33\n",
      "0.8845401174168297 0.33\n",
      "0.9393346379647749 0.34\n",
      "0.9315068493150684 0.34\n",
      "0.8845401174168297 0.34\n",
      "0.9393346379647749 0.35\n",
      "0.9315068493150684 0.35\n",
      "0.8845401174168297 0.35\n",
      "0.9393346379647749 0.36\n",
      "0.9315068493150684 0.36\n",
      "0.8845401174168297 0.36\n",
      "0.9393346379647749 0.37\n",
      "0.9315068493150684 0.37\n",
      "0.8845401174168297 0.37\n",
      "0.9393346379647749 0.38\n",
      "0.9315068493150684 0.38\n",
      "0.8845401174168297 0.38\n",
      "0.9393346379647749 0.39\n",
      "0.9315068493150684 0.39\n",
      "0.8845401174168297 0.39\n",
      "0.9393346379647749 0.4\n",
      "0.9315068493150684 0.4\n",
      "0.8845401174168297 0.4\n",
      "0.9393346379647749 0.41\n",
      "0.9315068493150684 0.41\n",
      "0.8845401174168297 0.41\n",
      "0.9393346379647749 0.42\n",
      "0.9315068493150684 0.42\n",
      "0.8845401174168297 0.42\n",
      "0.9393346379647749 0.43\n",
      "0.9315068493150684 0.43\n",
      "0.8845401174168297 0.43\n",
      "0.9393346379647749 0.44\n",
      "0.9315068493150684 0.44\n",
      "0.8845401174168297 0.44\n",
      "0.9393346379647749 0.45\n",
      "0.9315068493150684 0.45\n",
      "0.8845401174168297 0.45\n",
      "0.9393346379647749 0.46\n",
      "0.9315068493150684 0.46\n",
      "0.8845401174168297 0.46\n",
      "0.9393346379647749 0.47\n",
      "0.9315068493150684 0.47\n",
      "0.8845401174168297 0.47\n",
      "0.9393346379647749 0.48\n",
      "0.9315068493150684 0.48\n",
      "0.8845401174168297 0.48\n",
      "0.9393346379647749 0.49\n",
      "0.9315068493150684 0.49\n",
      "0.8845401174168297 0.49\n",
      "0.9393346379647749 0.5\n",
      "0.9315068493150684 0.5\n",
      "0.8845401174168297 0.5\n",
      "0.9393346379647749 0.51\n",
      "0.9315068493150684 0.51\n",
      "0.8845401174168297 0.51\n",
      "0.9393346379647749 0.52\n",
      "0.9315068493150684 0.52\n",
      "0.8845401174168297 0.52\n",
      "0.9393346379647749 0.53\n",
      "0.9315068493150684 0.53\n",
      "0.8845401174168297 0.53\n",
      "0.9393346379647749 0.54\n",
      "0.9315068493150684 0.54\n",
      "0.8845401174168297 0.54\n",
      "0.9393346379647749 0.55\n",
      "0.9315068493150684 0.55\n",
      "0.8845401174168297 0.55\n",
      "0.9393346379647749 0.56\n",
      "0.9315068493150684 0.56\n",
      "0.8845401174168297 0.56\n",
      "0.9393346379647749 0.57\n",
      "0.9315068493150684 0.57\n",
      "0.8845401174168297 0.57\n",
      "0.9393346379647749 0.58\n",
      "0.9315068493150684 0.58\n",
      "0.8845401174168297 0.58\n",
      "0.9393346379647749 0.59\n",
      "0.9315068493150684 0.59\n",
      "0.8845401174168297 0.59\n",
      "0.9393346379647749 0.6\n",
      "0.9315068493150684 0.6\n",
      "0.8845401174168297 0.6\n",
      "0.9393346379647749 0.61\n",
      "0.9315068493150684 0.61\n",
      "0.8845401174168297 0.61\n",
      "0.9393346379647749 0.62\n",
      "0.9315068493150684 0.62\n",
      "0.8845401174168297 0.62\n",
      "0.9393346379647749 0.63\n",
      "0.9315068493150684 0.63\n",
      "0.8845401174168297 0.63\n",
      "0.9393346379647749 0.64\n",
      "0.9315068493150684 0.64\n",
      "0.8845401174168297 0.64\n",
      "0.9393346379647749 0.65\n",
      "0.9315068493150684 0.65\n",
      "0.8845401174168297 0.65\n",
      "0.9393346379647749 0.66\n",
      "0.9315068493150684 0.66\n",
      "0.8845401174168297 0.66\n",
      "0.9393346379647749 0.67\n",
      "0.9315068493150684 0.67\n",
      "0.8845401174168297 0.67\n",
      "0.9393346379647749 0.68\n",
      "0.9315068493150684 0.68\n",
      "0.8845401174168297 0.68\n",
      "0.9393346379647749 0.69\n",
      "0.9315068493150684 0.69\n",
      "0.8845401174168297 0.69\n",
      "0.9393346379647749 0.7\n",
      "0.9315068493150684 0.7\n",
      "0.8845401174168297 0.7\n",
      "0.9393346379647749 0.71\n",
      "0.9315068493150684 0.71\n",
      "0.8845401174168297 0.71\n",
      "0.9393346379647749 0.72\n",
      "0.9315068493150684 0.72\n",
      "0.8845401174168297 0.72\n",
      "0.9393346379647749 0.73\n",
      "0.9315068493150684 0.73\n",
      "0.8845401174168297 0.73\n",
      "0.9393346379647749 0.74\n",
      "0.9315068493150684 0.74\n",
      "0.8845401174168297 0.74\n",
      "0.9393346379647749 0.75\n",
      "0.9315068493150684 0.75\n",
      "0.8845401174168297 0.75\n",
      "0.9393346379647749 0.76\n",
      "0.9315068493150684 0.76\n",
      "0.8845401174168297 0.76\n",
      "0.9393346379647749 0.77\n",
      "0.9315068493150684 0.77\n",
      "0.8845401174168297 0.77\n",
      "0.9393346379647749 0.78\n",
      "0.9315068493150684 0.78\n",
      "0.8845401174168297 0.78\n",
      "0.9393346379647749 0.79\n",
      "0.9315068493150684 0.79\n",
      "0.8845401174168297 0.79\n",
      "0.9393346379647749 0.8\n",
      "0.9315068493150684 0.8\n",
      "0.8845401174168297 0.8\n",
      "0.9393346379647749 0.81\n",
      "0.9315068493150684 0.81\n",
      "0.8845401174168297 0.81\n",
      "0.9393346379647749 0.82\n",
      "0.9315068493150684 0.82\n",
      "0.8845401174168297 0.82\n",
      "0.9393346379647749 0.83\n",
      "0.9315068493150684 0.83\n",
      "0.8845401174168297 0.83\n",
      "0.9393346379647749 0.84\n",
      "0.9315068493150684 0.84\n",
      "0.8845401174168297 0.84\n",
      "0.9393346379647749 0.85\n",
      "0.9315068493150684 0.85\n",
      "0.8845401174168297 0.85\n",
      "0.9393346379647749 0.86\n",
      "0.9315068493150684 0.86\n",
      "0.8845401174168297 0.86\n",
      "0.9393346379647749 0.87\n",
      "0.9315068493150684 0.87\n",
      "0.8845401174168297 0.87\n",
      "0.9393346379647749 0.88\n",
      "0.9315068493150684 0.88\n",
      "0.8845401174168297 0.88\n",
      "0.9393346379647749 0.89\n",
      "0.9315068493150684 0.89\n",
      "0.8845401174168297 0.89\n",
      "0.9393346379647749 0.9\n",
      "0.9315068493150684 0.9\n",
      "0.8845401174168297 0.9\n",
      "0.9393346379647749 0.91\n",
      "0.9315068493150684 0.91\n",
      "0.8845401174168297 0.91\n",
      "0.9393346379647749 0.92\n",
      "0.9315068493150684 0.92\n",
      "0.8845401174168297 0.92\n",
      "0.9393346379647749 0.93\n",
      "0.9315068493150684 0.93\n",
      "0.8845401174168297 0.93\n",
      "0.9393346379647749 0.94\n",
      "0.9315068493150684 0.94\n",
      "0.8845401174168297 0.94\n",
      "0.9393346379647749 0.95\n",
      "0.9315068493150684 0.95\n",
      "0.8845401174168297 0.95\n",
      "0.9393346379647749 0.96\n",
      "0.9315068493150684 0.96\n",
      "0.8845401174168297 0.96\n",
      "0.9393346379647749 0.97\n",
      "0.9315068493150684 0.97\n",
      "0.8845401174168297 0.97\n",
      "0.9393346379647749 0.98\n",
      "0.9315068493150684 0.98\n",
      "0.8845401174168297 0.98\n",
      "0.9393346379647749 0.99\n",
      "0.9315068493150684 0.99\n",
      "0.8845401174168297 0.99\n"
     ]
    }
   ],
   "source": [
    "max_eta = -1\n",
    "max_accuracy = -1\n",
    "print('max_accuracy', 'max_eta')\n",
    "\n",
    "for val in range(100):\n",
    "    lr = LogisticRegression(optimization='tgd',eta=val/100.0, regularization='ridge', iterations=10, c=0.001)\n",
    "    for col in targets:\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test[col],yhat)\n",
    "        print(accuracy, val/100)\n",
    "        \n",
    "#     if accuracy > max_accuracy:\n",
    "#         max_accuracy = accuracy\n",
    "#         max_eta = val/100.0\n",
    "#         print(max_accuracy, max_eta)\n",
    "\n",
    "# for e in eVals:\n",
    "#     print(e)\n",
    "    \n",
    "#     lr = LogisticRegression(optimization='hessian',eta=0.8, regularization='ridge', iterations=500, c=0.001)\n",
    "#     for col in targets:\n",
    "#         lr.fit(X_train, y_train)\n",
    "#         yhat = lr.predict(X_test)\n",
    "#         print(\"Accuracy of Testing Dataset (500 iterations): \", accuracy_score(y_test[col],yhat), \" - \", col)\n",
    "\n",
    "#     lr = LogisticRegression(optimization='bgd',eta=e, regularization='ridge', iterations=1)\n",
    "#     lr.fit(X_train, y_train)\n",
    "#     yhat = lr.predict(X_train)\n",
    "#     print(\"Training Accuracy: {}, Eta: {}, optimization: {}, regularization: {}\".format(accuracy_score(y_train,yhat),e,\"bgd\",\"ridge\"))\n",
    "\n",
    "#     lr = LogisticRegression(optimization='bgd',eta=e, regularization='lasso', iterations=1)\n",
    "#     lr.fit(X_train, y_train)\n",
    "#     yhat = lr.predict(X_train)\n",
    "#     print(\"Training Accuracy: {}, Eta: {}, optimization: {}, regularization: {}\".format(accuracy_score(y_train,yhat),e,\"bgd\",\"lasso\"))\n",
    "    \n",
    "#     lr = LogisticRegression(optimization='bgd',eta=e, regularization='elastic_net', iterations=1)\n",
    "#     lr.fit(X_train, y_train)\n",
    "#     yhat = lr.predict(X_train)\n",
    "#     print(\"Training Accuracy: {}, Eta: {}, optimization: {}, regularization: {}\".format(accuracy_score(y_train,yhat),e,\"bgd\",\"elastic_net\"))\n",
    "    \n",
    "    \n",
    "#     lr = LogisticRegression(optimization='bgd',eta=e, regularization='elastic_net', iterations=300)\n",
    "#     lr.fit(X_train, y_train)\n",
    "#     pred = lr.predict(X_train)\n",
    "#     encode = lambda x: lr.encodings[x]\n",
    "#     y_train_encode = np.array(list(map(encode, y_train)))\n",
    "#     train_mse = accuracy_score(y_train_encode, pred)\n",
    "#     print(\"Training MSE: {}, Eta: {}, optimization: {}, regularization: {}\".format(train_mse,e,\"bgd\",\"elastic_net\"))\n",
    "#     results.append([train_mse,e,\"bgd\",\"elastic_net\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsLists_sgd=[]\n",
    "for e in eVals:\n",
    "    print(e)\n",
    "    lr = LogisticRegression(optimization='sgd',eta=e, regularization='ridge', iterations=300)\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_train)\n",
    "    print(\"Training Accuracy: {}, Eta: {}, optimization: {}, regularization: {}\".format(accuracy_score(y_train,yhat),e,\"sgd\",\"ridge\"))\n",
    "\n",
    "    lr = LogisticRegression(optimization='sgd',eta=e, regularization='lasso', iterations=300)\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_train)\n",
    "    print(\"Training Accuracy: {}, Eta: {}, optimization: {}, regularization: {}\".format(accuracy_score(y_train,yhat),e,\"sgd\",\"lasso\"))\n",
    "    \n",
    "    lr = LogisticRegression(optimization='sgd',eta=e, regularization='elastic_net', iterations=300)\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_train)\n",
    "    print(\"Training Accuracy: {}, Eta: {}, optimization: {}, regularization: {}\".format(accuracy_score(y_train,yhat),e,\"sgd\",\"elastic_net\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsLists_newton=[]\n",
    "for e in eVals:\n",
    "    print(e)\n",
    "    lr = LogisticRegression(optimization='newton',eta=e, regularization='ridge', iterations=300)\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_train)\n",
    "    print(\"Training Accuracy: {}, Eta: {}, optimization: {}, regularization: {}\".format(accuracy_score(y_train,yhat),e,\"newton\",\"ridge\"))\n",
    "\n",
    "    lr = LogisticRegression(optimization='newton',eta=e, regularization='lasso', iterations=300)\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_train)\n",
    "    print(\"Training Accuracy: {}, Eta: {}, optimization: {}, regularization: {}\".format(accuracy_score(y_train,yhat),e,\"newton\",\"lasso\"))\n",
    "    \n",
    "    lr = LogisticRegression(optimization='newton',eta=e, regularization='elastic_net', iterations=300)\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_train)\n",
    "    print(\"Training Accuracy: {}, Eta: {}, optimization: {}, regularization: {}\".format(accuracy_score(y_train,yhat),e,\"newton\",\"elastic_net\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "eVals = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "optimizations = [\"tgd\", \"sgd\", \"newton\"]\n",
    "results = []\n",
    "\n",
    "for e in eVals:\n",
    "    print(\"Eta: \", e, \", Optimization: \", optimizations[0])\n",
    "    lr = LogisticRegression(optimization='tgd',eta=e, regularization='ridge', iterations=200)\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_train)\n",
    "    print(\"Training: {}, regularization: {}\".format(accuracy_score(y_train,yhat), \"ridge\"))\n",
    "    \n",
    "    lr = LogisticRegression(optimization='tgd',eta=e, regularization='lasso', iterations=200)\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_train)\n",
    "    print(\"Training: {}, regularization: {}\".format(accuracy_score(y_train,yhat), \"lasso\"))\n",
    "    \n",
    "    lr = LogisticRegression(optimization='tgd',eta=e, regularization='elastic_net', iterations=200)\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_train)\n",
    "    print(\"Training: {}, regularization: {}\".format(accuracy_score(y_train,yhat), \"elastic_net\"))\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eVals = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "optimizations = [\"tgd\", \"sgd\", \"newton\"]\n",
    "regularizations = [\"ridge\", \"lasso\", \"elastic_net\"]\n",
    "results = []\n",
    "\n",
    "for e in eVals:\n",
    "    print(\"Eta: \", e, \", Optimization: \", optimizations[0])\n",
    "    lr = LogisticRegression(optimization='tgd',eta=e, regularization='ridge', iterations=499)\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_train)\n",
    "    encode = lambda x: lr.encodings[x]\n",
    "    y_train_encode = np.array(list(map(encode, y_train)))\n",
    "    train = accuracy_score(y_train_encode, yhat)\n",
    "    print(\"Training: {}, regularization: {}\".format(train, \"ridge\"))\n",
    "    results.append([train,e,\"bgd\",\"ridge\"])\n",
    "    \n",
    "    lr = LogisticRegression(optimization='tgd',eta=e, regularization='lasso', iterations=499)\n",
    "    lr.fit(X_train, y_train)\n",
    "    pred = lr.predict(X_train)\n",
    "    encode = lambda x: lr.encodings[x]\n",
    "    y_train_encode = np.array(list(map(encode, y_train)))\n",
    "    train = accuracy_score(y_train_encode, pred)\n",
    "    print(\"Training: {}, regularization: {}\".format(train,\"lasso\"))\n",
    "    results.append([train,e,\"bgd\",\"lasso\"])\n",
    "    \n",
    "    lr = LogisticRegression(optimization='tgd',eta=e, regularization='elastic_net', iterations=499)\n",
    "    lr.fit(X_train, y_train)\n",
    "    pred = lr.predict(X_train)\n",
    "    encode = lambda x: lr.encodings[x]\n",
    "    y_train_encode = np.array(list(map(encode, y_train)))\n",
    "    train = accuracy_score(y_train_encode, pred)\n",
    "    print(\"Training: {}, regularization: {}\".format(train,\"elastic_net\"))\n",
    "    results.append([train,e,\"bgd\",\"elastic_net\"])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Eta: \", e, \", Optimization: \", optimizations[1])\n",
    "    lr = LogisticRegression(optimization='sgd',eta=e, regularization='ridge', iterations=499)\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_train)\n",
    "    encode = lambda x: lr.encodings[x]\n",
    "    y_train_encode = np.array(list(map(encode, y_train)))\n",
    "    train = accuracy_score(y_train_encode, yhat)\n",
    "    print(\"Training: {}, regularization: {}\".format(train, \"ridge\"))\n",
    "    results.append([train,e,\"bgd\",\"ridge\"])\n",
    "    \n",
    "    lr = LogisticRegression(optimization='sgd',eta=e, regularization='lasso', iterations=499)\n",
    "    lr.fit(X_train, y_train)\n",
    "    pred = lr.predict(X_train)\n",
    "    encode = lambda x: lr.encodings[x]\n",
    "    y_train_encode = np.array(list(map(encode, y_train)))\n",
    "    train = accuracy_score(y_train_encode, pred)\n",
    "    print(\"Training: {}, regularization: {}\".format(train,\"lasso\"))\n",
    "    results.append([train,e,\"bgd\",\"lasso\"])\n",
    "    \n",
    "    lr = LogisticRegression(optimization='sgd',eta=e, regularization='elastic_net', iterations=499)\n",
    "    lr.fit(X_train, y_train)\n",
    "    pred = lr.predict(X_train)\n",
    "    encode = lambda x: lr.encodings[x]\n",
    "    y_train_encode = np.array(list(map(encode, y_train)))\n",
    "    train = accuracy_score(y_train_encode, pred)\n",
    "    print(\"Training: {}, regularization: {}\".format(train,\"elastic_net\"))\n",
    "    results.append([train,e,\"bgd\",\"elastic_net\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Which implementation of logistic regression would you advise be used in a deployed machine learning model, your implementation or scikit-learn (or other third party)? Why?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>You have free reign to provide additional analyses. <b>One idea</b>: Update the code to use either \"one-versus-all\" or \"one-versus-one\" extensions of binary to multi-class classification. </li>\n",
    "    <li><b>Required for 7000 level students</b>: Choose ONE of the following:\n",
    "    <ul>\n",
    "        <li><b>Option One</b>: Implement an optimization technique for logistic regression using <b>mean square error</b> as your objective function (instead of binary cross entropy). Derive the gradient updates for the Hessian and use Newton's method to update the values of \"w\". Then answer, is this process better than using binary cross entropy? </li>\n",
    "        <li><b>Option Two</b>: Implement the BFGS algorithm from scratch to optimize logistic regression. That is, use BFGS without the use of an external package (for example, do not use SciPy). Compare your performance accuracy and runtime to the BFGS implementation in SciPy (that we used in lecture). </li>\n",
    "    </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eVals=[]\n",
    "start_e=0.000001\n",
    "for x in range(0,6):\n",
    "    eVals.append(start_e)\n",
    "    start_e*=10\n",
    "results=[]\n",
    "\n",
    "for e in eVals:\n",
    "    print(e)\n",
    "    lr = LogisticRegression(optimization='bgd',eta=e, regularization='ridge', iterations=300)\n",
    "    lr.fit(X_train, y_train)\n",
    "    pred = lr.predict(X_train)\n",
    "    encode = lambda x: lr.encodings[x]\n",
    "    y_train_encode = np.array(list(map(encode, y_train)))\n",
    "    train_mse = accuracy_score(y_train_encode, pred)\n",
    "    print(\"Training MSE: {}, Eta: {}, optimization: {}, regularization: {}\".format(train_mse,e,\"bgd\",\"ridge\"))\n",
    "    results.append([train_mse,e,\"bgd\",\"ridge\"])\n",
    "    \n",
    "    lr = LogisticRegression(optimization='bgd',eta=e, regularization='lasso', iterations=300)\n",
    "    lr.fit(X_train, y_train)\n",
    "    pred = lr.predict(X_train)\n",
    "    encode = lambda x: lr.encodings[x]\n",
    "    y_train_encode = np.array(list(map(encode, y_train)))\n",
    "    train_mse = accuracy_score(y_train_encode, pred)\n",
    "    print(\"Training MSE: {}, Eta: {}, optimization: {}, regularization: {}\".format(train_mse,e,\"bgd\",\"lasso\"))\n",
    "    results.append([train_mse,e,\"bgd\",\"lasso\"])\n",
    "    \n",
    "    lr = LogisticRegression(optimization='bgd',eta=e, regularization='elastic_net', iterations=300)\n",
    "    lr.fit(X_train, y_train)\n",
    "    pred = lr.predict(X_train)\n",
    "    encode = lambda x: lr.encodings[x]\n",
    "    y_train_encode = np.array(list(map(encode, y_train)))\n",
    "    train_mse = accuracy_score(y_train_encode, pred)\n",
    "    print(\"Training MSE: {}, Eta: {}, optimization: {}, regularization: {}\".format(train_mse,e,\"bgd\",\"elastic_net\"))\n",
    "    results.append([train_mse,e,\"bgd\",\"elastic_net\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
