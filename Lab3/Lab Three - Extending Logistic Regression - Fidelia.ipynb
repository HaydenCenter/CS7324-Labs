{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Three - Extending Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will compare the performance of logistic regression optimization programmed in scikit-learn and via your own implementation. You will also modify the optimization procedure for logistic regression. \n",
    "\n",
    "This report is worth 10% of the final grade. Please upload a report (<b>one per team</b>) with all code used, visualizations, and text in a rendered Jupyter notebook. Any visualizations that cannot be embedded in the notebook, please provide screenshots of the output. The results should be reproducible using your report. Please carefully describe every assumption and every step in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dataset Selection</b>\n",
    "\n",
    "Select a dataset identically to the way you selected for the lab one (i.e., table data). You are not required to use the same dataset that you used in the past, but you are encouraged. You must identify a classification task from the dataset that contains <b>three or more classes to predict</b>. That is it cannot be a binary classification; it must be multi-class prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation and Overview (3pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>[<b>2 points</b>] Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the classification task is and what parties would be interested in the results. For example, would the model be deployed or used mostly for offline analysis? </li>\n",
    "    <li>[<b>.5 points</b>] (<i>mostly the same processes as from previous labs</i>) Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). </li>\n",
    "    <li>[<b>.5 points</b>] Divide you data into training and testing data using an 80% training and 20% testing split. Use the cross validation modules that are part of scikit-learn. <b>Argue \"for\" or \"against\" splitting your data using an 80/20 split. That is, why is the 80/20 split appropriate (or not) for your dataset?</b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case\n",
    "\n",
    "Our task will be looking at a patients information and determining whether they are likely to have a stroke, heart disease, or hypertension. The use-case for this classifier would be to flag at-risk patients and enable some kind of response to be made to prevent serious medical emergencies that these conditions might cause or prevent the conditions in the first place.\n",
    "\n",
    "For example, if a person were to be flagged as very likely to have a stroke, the doctor could contact the patient in an attempt to prevent the stroke by prescribing them medication or alerting the patient's family to monitor them in case they were to have a stroke. Similar actions could be taken for hypertension and heart disease.\n",
    "\n",
    "Alernatively, some kind of application could be made to allow people to enter their information and determine how at risk they might be for these conditions, giving people more clear information about their health and the issues that are likely to affect them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 1.1.3\n",
      "Numpy: 1.19.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>work_type</th>\n",
       "      <th>residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9046</td>\n",
       "      <td>Male</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>formerly smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51676</td>\n",
       "      <td>Female</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>202.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31112</td>\n",
       "      <td>Male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Rural</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60182</td>\n",
       "      <td>Female</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>smokes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1665</td>\n",
       "      <td>Female</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender   age  hypertension  heart_disease ever_married  \\\n",
       "0   9046    Male  67.0             0              1          Yes   \n",
       "1  51676  Female  61.0             0              0          Yes   \n",
       "2  31112    Male  80.0             0              1          Yes   \n",
       "3  60182  Female  49.0             0              0          Yes   \n",
       "4   1665  Female  79.0             1              0          Yes   \n",
       "\n",
       "       work_type residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
       "0        Private          Urban             228.69  36.6  formerly smoked   \n",
       "1  Self-employed          Rural             202.21   NaN     never smoked   \n",
       "2        Private          Rural             105.92  32.5     never smoked   \n",
       "3        Private          Urban             171.23  34.4           smokes   \n",
       "4  Self-employed          Rural             174.12  24.0     never smoked   \n",
       "\n",
       "   stroke  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing packages and reading in dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print('Pandas:', pd.__version__)\n",
    "print('Numpy:',  np.__version__)\n",
    "\n",
    "raw_data = pd.read_csv('healthcare-dataset-stroke-data.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping categorical column 'work_type'; not very useful and\n",
    "# doesn't translate nicely into ordinal numbers\n",
    "df = raw_data.drop('work_type', axis = 1)\n",
    "\n",
    "# Dropping 1 observation of person with gender 'Other' to simplify\n",
    "# using the gender column to calculate, impute, or visualize\n",
    "df.drop(df[df.gender == 'Other'].index, inplace=True)\n",
    "\n",
    "# Making values' format consistent\n",
    "for c in df.columns:\n",
    "    if df[c].dtype == 'object':\n",
    "        df[c] = df[c].str.lower()\n",
    "\n",
    "# Adding numbers to smoking_status values to order them properly\n",
    "# when they will get passed through the SKLearn LabelEncoder\n",
    "df.smoking_status.replace(to_replace= ['never smoked', 'formerly smoked', 'smokes', 'Unknown'],\n",
    "                          value     = ['0_never_smoked', '1_formerly_smoked', '2_smokes', '3_Unknown'],\n",
    "                          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9046</td>\n",
       "      <td>1</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51676</td>\n",
       "      <td>0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>202.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31112</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60182</td>\n",
       "      <td>0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1665</td>\n",
       "      <td>0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender   age  hypertension  heart_disease  ever_married  \\\n",
       "0   9046       1  67.0             0              1             1   \n",
       "1  51676       0  61.0             0              0             1   \n",
       "2  31112       1  80.0             0              1             1   \n",
       "3  60182       0  49.0             0              0             1   \n",
       "4   1665       0  79.0             1              0             1   \n",
       "\n",
       "   residence_type  avg_glucose_level   bmi  smoking_status  stroke  \n",
       "0               1             228.69  36.6             1.0       1  \n",
       "1               0             202.21   NaN             0.0       1  \n",
       "2               0             105.92  32.5             0.0       1  \n",
       "3               1             171.23  34.4             2.0       1  \n",
       "4               0             174.12  24.0             0.0       1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encoding all of the non-numeric columns\n",
    "le = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        le[col] = LabelEncoder()\n",
    "        df[col] = le[col].fit_transform(df[col])\n",
    "\n",
    "# Call le[col].inverse_transform(df[col]) for any column name\n",
    "# to convert numbers back to their labels\n",
    "\n",
    "# Converting all 'Unknown' values in smoking status to NaN so\n",
    "# that we can impute the missing values.\n",
    "df.smoking_status.mask(df.smoking_status == 3, np.nan, inplace=True)\n",
    "               \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values\n",
    "from sklearn.impute import KNNImputer\n",
    "import copy\n",
    "\n",
    "knn = KNNImputer(n_neighbors=3)\n",
    "\n",
    "# Imputing on all columns except id\n",
    "columns = list(df.columns)\n",
    "columns.remove('id')\n",
    "\n",
    "df_imputed = copy.deepcopy(df)\n",
    "df_imputed[columns] = knn.fit_transform(df[columns])\n",
    "\n",
    "# Rounding imputed values to be compatible with LabelEncoder\n",
    "# for smoking_status and to match the format of other values\n",
    "# for bmi\n",
    "df_imputed.smoking_status = df_imputed.smoking_status.apply(lambda x: round(x, 0))\n",
    "df_imputed.bmi = df_imputed.bmi.apply(lambda x: round(x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using df_imputed as the primary dataset\n",
    "df = df_imputed\n",
    "\n",
    "# Changing columns modified by KNN Imputer back to integers from floats\n",
    "columns = [\n",
    "    'gender',\n",
    "    'hypertension',\n",
    "    'heart_disease',\n",
    "    'ever_married',\n",
    "    'residence_type',\n",
    "    'smoking_status',\n",
    "    'stroke'\n",
    "]\n",
    "\n",
    "for col in columns:\n",
    "    df[col] = df[col].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prep this dataset, one attribute was removed due to it being relatively unimportant and not encoding nicely into an ordinal set of integers. All categorical variables were converted to numeric data using SKLearn's LabelEncoder class. Missing values for bmi and smoking_status were imputed using KNN Imputer. One record was dropped for being the only entry with gender 'Other'. Removing this record will make visualizing the gender data simpler and will have little impact on the training, as having an outlier like that might cause other attributes to be slightly undervalued in comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a table of the LabelEncoder encoded variables.\n",
    "\n",
    "| value | gender | ever_married | residence_type | smoking_status    |\n",
    "|-------|--------|--------------|----------------|-------------------|\n",
    "| 0     | female | no           | rural          | 0_never_smoked    |\n",
    "| 1     | male   | yes          | urban          | 1_formerly_smoked |\n",
    "| 2     |   -    |      -       |       -        | 2_smokes          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9046</td>\n",
       "      <td>1</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51676</td>\n",
       "      <td>0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>202.21</td>\n",
       "      <td>30.9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31112</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60182</td>\n",
       "      <td>0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1665</td>\n",
       "      <td>0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender   age  hypertension  heart_disease  ever_married  \\\n",
       "0   9046       1  67.0             0              1             1   \n",
       "1  51676       0  61.0             0              0             1   \n",
       "2  31112       1  80.0             0              1             1   \n",
       "3  60182       0  49.0             0              0             1   \n",
       "4   1665       0  79.0             1              0             1   \n",
       "\n",
       "   residence_type  avg_glucose_level   bmi  smoking_status  stroke  \n",
       "0               1             228.69  36.6               1       1  \n",
       "1               0             202.21  30.9               0       1  \n",
       "2               0             105.92  32.5               0       1  \n",
       "3               1             171.23  34.4               2       1  \n",
       "4               0             174.12  24.0               0       1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = list(df.columns)\n",
    "columns.remove('id')\n",
    "targets = ['stroke', 'heart_disease', 'hypertension']\n",
    "\n",
    "for col in targets:\n",
    "    columns.remove(col)\n",
    "\n",
    "#splitting into train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=.20, random_state=42)\n",
    "\n",
    "X_test  = test[columns].to_numpy()\n",
    "X_train = train[columns].to_numpy()\n",
    "y_test  = {}\n",
    "y_train = {}\n",
    "\n",
    "for col in targets:\n",
    "    y_test[col]  = test[col].to_numpy()\n",
    "    y_train[col] = train[col].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (5pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>The implementation of logistic regression must be written only from the examples given to you by the instructor. No credit will be assigned to teams that copy implementations from another source, regardless of if the code is properly cited.</li>\n",
    "    <li>[<b>2 points</b>] Create a custom, one-versus-all logistic regression classifier using numpy and scipy to optimize. Use object oriented conventions identical to scikit-learn. You should start with the template developed by the instructor in the course. You should add the following functionality to the logistic regression classifier:\n",
    "    <ul>\n",
    "        <li>Ability to choose optimization technique when class is instantiated: either steepest descent, stochastic gradient descent, or Newton's method. </li>\n",
    "        <li>Update the gradient calculation to include a customizable regularization term (either using no regularization, L1 regularization, L2 regularization, or both L1 and L2 regularization). Associate a cost with the regularization term, \"C\", that can be adjusted when the class is instantiated.  </li>\n",
    "    </ul>\n",
    "    </li>\n",
    "    <li>[<b>1.5 points</b>] Train your classifier to achieve good generalization performance. That is, adjust the <b>optimization technique</b> and the value of the <b>regularization term \"C\"</b> to achieve the best performance on your test set. Visualize the performance of the classifier versus the parameters you investigated. Is your method of selecting parameters justified? That is, do you think there is any \"data snooping\" involved with this method of selecting parameters?</li>\n",
    "    <li>[<b>1.5 points</b>] Compare the performance of your \"best\" logistic regression optimization procedure to the procedure used in scikit-learn. Visualize the performance differences in terms of training time and classification performance. <b>Discuss the results</b>. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, optimization='bgd', eta = 0.01, iterations=20, regularization='none', c=0):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.opt = optimization\n",
    "        self.reg = regularization\n",
    "        self.c = c\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    #optimization methods\n",
    "    def _get_gradient(self, X, y):\n",
    "        \n",
    "        gradient = None\n",
    "        if self.opt == 'tgd': gradient = self.steepest_descent\n",
    "        elif self.opt == 'sgd': gradient = self.stochastic_gradient_descent\n",
    "        elif self.opt == 'newton': gradient = self.newton\n",
    "        return gradient(X,y)\n",
    "    \n",
    "    def steepest_descent(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += self.c * self._get_reg_gradient()\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def stochastic_gradient_descent(self,X,y):\n",
    "        idx = np.random.randint(len(y))\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += self.c * self._get_reg_gradient()\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def newton(self, X, y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.c  # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] +=  self.c * self._get_reg_gradient()\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    #regularization methods\n",
    "    def _get_reg_gradient(self):\n",
    "        if self.reg == 'none':\n",
    "            return self.w_[1:]\n",
    "        elif self.reg == 'ridge':\n",
    "            return -2 * self.w_[1:]\n",
    "        elif self.reg == 'lasso':\n",
    "            return np.sign(self.w_[1:])\n",
    "        elif self.reg == 'elastic_net':\n",
    "            return -2 * self.w_[1:] + np.sign(self.w_[1:])\n",
    "    \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logisitic Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, optimization, eta, iterations, regularization, c=0):\n",
    "    \n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.opt = optimization\n",
    "        self.reg = regularization\n",
    "        self.encodings = {}\n",
    "        self.c = c\n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "    \n",
    "    def fit(self,X,y): # y is a hash of target columns\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for name, target in y.items():\n",
    "            blr = BinaryLogisticRegression(self.opt, self.eta, self.iters, self.reg, self.c )\n",
    "            blr.fit(X,target)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "        #if np.count_nonzero(blr.predict_proba(X)) > 0:\n",
    "            #print(\"Not zero\")\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "            \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "    \n",
    "lr = LogisticRegression('tgd',0.01, 100, 'ridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.9393346379647749  -  stroke\n",
      "Accuracy of Testing Dataset:  0.9315068493150684  -  heart_disease\n",
      "Accuracy of Testing Dataset:  0.8845401174168297  -  hypertension\n"
     ]
    }
   ],
   "source": [
    "#evaluate on train dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression(optimization='tgd',eta=0.9, regularization='none', iterations=10, c=0.001)\n",
    "for col in targets:\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[col],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.4070450097847358  -  stroke\n",
      "Accuracy of Testing Dataset:  0.4090019569471624  -  heart_disease\n",
      "Accuracy of Testing Dataset:  0.410958904109589  -  hypertension\n"
     ]
    }
   ],
   "source": [
    "#trying larger iterations\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression(optimization='tgd',eta=0.9, regularization='none', iterations=500, c=0.001)\n",
    "for col in targets:\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[col],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, when performing no regularizations but using the default optimization of steepest descent, fewer iterations yielded a higher accuracies. This will be interesting to analyze as we increase the number of iterations and evaluate with different combinations of regularizations/optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Accuracy          Max Eta\n",
      "0.9393346379647749    0.57\n",
      "0.9315068493150684    0.57\n",
      "0.8845401174168297    0.57\n",
      "0.9354207436399217    0.31\n",
      "0.9275929549902152    0.31\n",
      "0.8806262230919765    0.31\n",
      "0.9305283757338552    0.3\n",
      "0.9227005870841487    0.3\n",
      "0.87573385518591    0.3\n",
      "0.9295499021526419    0.29\n",
      "\n",
      "Max Accuracy       Max c\n",
      "0.14090019569471623 0.0\n",
      "0.8845401174168297 0.001\n",
      "\n",
      "Final eta:  0.57\n",
      "Final c:  0.001\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "max_eta = -1\n",
    "accuracy_eta_list = dict()\n",
    "for val in range(58):\n",
    "    lr = LogisticRegression(optimization='tgd',eta=val/100, regularization='none', iterations=10, c=0.001)\n",
    "    for col in targets:\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test[col],yhat)\n",
    "        accuracy_eta_list[accuracy] = val/100\n",
    "new_list = dict(sorted(accuracy_eta_list.items(), key=operator.itemgetter(1), reverse=True)[:10])\n",
    "print('Max Accuracy          Max Eta')\n",
    "for key in new_list:\n",
    "    print(key, '  ', new_list[key])\n",
    "max_accuracy = list(new_list.keys())[0] \n",
    "max_eta = new_list[max_accuracy]\n",
    "    \n",
    "max_accuracy = -1\n",
    "print('\\nMax Accuracy       Max c')\n",
    "\n",
    "for val in range(100):\n",
    "    lr = LogisticRegression(optimization='tgd', eta=max_eta, regularization='none', iterations=500, c=val/1000.0)\n",
    "    for col in targets:\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test[col],yhat)\n",
    "    \n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        max_c = val/1000.0\n",
    "        print(max_accuracy, max_c)\n",
    "        \n",
    "print(\"\\nFinal eta: \", max_eta)\n",
    "print(\"Final c: \", max_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Steepest Gradient:  0.9393346379647749  - stroke\n",
      "Accuracy of Steepest Gradient:  0.9315068493150684  - heart_disease\n",
      "Accuracy of Steepest Gradient:  0.8845401174168297  - hypertension\n",
      "--- 0.16378935178120932 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Steepest Gradient L1 regularization:  0.0821917808219178  - stroke\n",
      "Accuracy of Steepest Gradient L1 regularization:  0.09001956947162426  - heart_disease\n",
      "Accuracy of Steepest Gradient L1 regularization:  0.13307240704500978  - hypertension\n",
      "--- 0.15881760915120444 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Steepest Gradient L2 regularization:  0.9393346379647749  - stroke\n",
      "Accuracy of Steepest Gradient L2 regularization:  0.9315068493150684  - heart_disease\n",
      "Accuracy of Steepest Gradient L2 regularization:  0.8845401174168297  - hypertension\n",
      "--- 0.161603053410848 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Steepest Gradient L12 regularization:  0.541095890410959  - stroke\n",
      "Accuracy of Steepest Gradient L12 regularization:  0.5303326810176126  - heart_disease\n",
      "Accuracy of Steepest Gradient L12 regularization:  0.49902152641878667  - hypertension\n",
      "--- 0.15591899553934732 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "lr_s0 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='none', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s0.fit(X_train,y_train)\n",
    "yhat_s0 = lr_s0.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient: \", accuracy_score(y_test['stroke'],yhat_s0), ' - stroke')\n",
    "\n",
    "lr_hd0 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='none', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd0.fit(X_train,y_train)\n",
    "yhat_hd0 = lr_hd0.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient: \", accuracy_score(y_test['heart_disease'],yhat_hd0), ' - heart_disease')\n",
    "\n",
    "lr_h0 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='none', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h0.fit(X_train,y_train)\n",
    "yhat_h0 = lr_h0.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient: \", accuracy_score(y_test['hypertension'],yhat_h0), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "lr_s1 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='ridge', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s1.fit(X_train,y_train)\n",
    "yhat_s1 = lr_s1.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L1 regularization: \", accuracy_score(y_test['stroke'],yhat_s1), ' - stroke')\n",
    "\n",
    "lr_hd1 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='ridge', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd1.fit(X_train,y_train)\n",
    "yhat_hd1 = lr_hd1.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L1 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd1), ' - heart_disease')\n",
    "\n",
    "lr_h1 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='ridge', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h1.fit(X_train,y_train)\n",
    "yhat_h1 = lr_h1.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L1 regularization: \", accuracy_score(y_test['hypertension'],yhat_h1), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "lr_s2 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='lasso', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s2.fit(X_train,y_train)\n",
    "yhat_s2 = lr_s2.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L2 regularization: \", accuracy_score(y_test['stroke'],yhat_s2), ' - stroke')\n",
    "\n",
    "lr_hd2 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='lasso', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd2.fit(X_train,y_train)\n",
    "yhat_hd2 = lr_hd2.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L2 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd2), ' - heart_disease')\n",
    "\n",
    "lr_h2 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='lasso', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h2.fit(X_train,y_train)\n",
    "yhat_h2 = lr_h2.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L2 regularization: \", accuracy_score(y_test['hypertension'],yhat_h2), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "lr_s3 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='elastic_net', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s3.fit(X_train,y_train)\n",
    "yhat_s3 = lr_s3.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L12 regularization: \", accuracy_score(y_test['stroke'],yhat_s3), ' - stroke')\n",
    "\n",
    "lr_hd3 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='elastic_net', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd3.fit(X_train,y_train)\n",
    "yhat_hd3 = lr_hd3.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L12 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd3), ' - heart_disease')\n",
    "\n",
    "lr_h3 = LogisticRegression(optimization=\"tgd\", eta=max_eta, regularization='elastic_net', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h3.fit(X_train,y_train)\n",
    "yhat_h3 = lr_h3.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Steepest Gradient L12 regularization: \", accuracy_score(y_test['hypertension'],yhat_h3), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Accuracy          Max Eta\n",
      "0.9315068493150684    0.57\n",
      "0.060665362035225046    0.57\n",
      "0.7729941291585127    0.57\n",
      "0.9393346379647749    0.56\n",
      "0.8845401174168297    0.56\n",
      "0.22015655577299412    0.56\n",
      "0.4246575342465753    0.55\n",
      "0.0    0.54\n",
      "0.8620352250489237    0.5\n",
      "0.9383561643835616    0.49\n",
      "\n",
      "Max Accuracy       Max c\n",
      "0.8845401174168297 0.0\n",
      "0.8855185909980431 0.016\n",
      "\n",
      "Final eta:  0.57\n",
      "Final c:  0.016\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "max_eta = -1\n",
    "accuracy_eta_list = dict()\n",
    "for val in range(58):\n",
    "    lr = LogisticRegression(optimization='sgd',eta=val/100, regularization='none', iterations=10, c=0.001)\n",
    "    for col in targets:\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test[col],yhat)\n",
    "        accuracy_eta_list[accuracy] = val/100\n",
    "new_list = dict(sorted(accuracy_eta_list.items(), key=operator.itemgetter(1), reverse=True)[:10])\n",
    "print('Max Accuracy          Max Eta')\n",
    "for key in new_list:\n",
    "    print(key, '  ', new_list[key])\n",
    "max_accuracy = list(new_list.keys())[0] \n",
    "max_eta = new_list[max_accuracy]\n",
    "\n",
    "max_accuracy = -1\n",
    "print('\\nMax Accuracy       Max c')\n",
    "\n",
    "for val in range(100):\n",
    "    lr = LogisticRegression(optimization='sgd', eta=max_eta, regularization='none', iterations=500, c=val/1000.0)\n",
    "    for col in targets:\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test[col],yhat)\n",
    "    \n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        max_c = val/1000.0\n",
    "        print(max_accuracy, max_c)\n",
    "        \n",
    "print(\"\\nFinal eta: \", max_eta)\n",
    "print(\"Final c: \", max_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stochastic Gradient:  0.9393346379647749  - stroke\n",
      "Accuracy of Stochastic Gradient:  0.0684931506849315  - heart_disease\n",
      "Accuracy of Stochastic Gradient:  0.8845401174168297  - hypertension\n",
      "--- 0.031053543090820312 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Stochastic Gradient L1 regularization:  0.7876712328767124  - stroke\n",
      "Accuracy of Stochastic Gradient L1 regularization:  0.48336594911937375  - heart_disease\n",
      "Accuracy of Stochastic Gradient L1 regularization:  0.687866927592955  - hypertension\n",
      "--- 0.030941009521484375 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Stochastic Gradient L2 regularization:  0.9393346379647749  - stroke\n",
      "Accuracy of Stochastic Gradient L2 regularization:  0.7622309197651663  - heart_disease\n",
      "Accuracy of Stochastic Gradient L2 regularization:  0.8845401174168297  - hypertension\n",
      "--- 0.028981765111287434 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Stochastic Gradient L12 regularization:  0.9393346379647749  - stroke\n",
      "Accuracy of Stochastic Gradient L12 regularization:  0.0684931506849315  - heart_disease\n",
      "Accuracy of Stochastic Gradient L12 regularization:  0.8845401174168297  - hypertension\n",
      "--- 0.03455559412638346 seconds ---\n"
     ]
    }
   ],
   "source": [
    "lr_s0 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='none', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s0.fit(X_train,y_train)\n",
    "yhat_s0 = lr_s0.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient: \", accuracy_score(y_test['stroke'],yhat_s0), ' - stroke')\n",
    "\n",
    "lr_hd0 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='none', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd0.fit(X_train,y_train)\n",
    "yhat_hd0 = lr_hd0.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient: \", accuracy_score(y_test['heart_disease'],yhat_hd0), ' - heart_disease')\n",
    "\n",
    "lr_h0 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='none', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h0.fit(X_train,y_train)\n",
    "yhat_h0 = lr_h0.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient: \", accuracy_score(y_test['hypertension'],yhat_h0), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "lr_s1 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='ridge', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s1.fit(X_train,y_train)\n",
    "yhat_s1 = lr_s1.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L1 regularization: \", accuracy_score(y_test['stroke'],yhat_s1), ' - stroke')\n",
    "\n",
    "lr_hd1 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='ridge', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd1.fit(X_train,y_train)\n",
    "yhat_hd1 = lr_hd1.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L1 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd1), ' - heart_disease')\n",
    "\n",
    "lr_h1 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='ridge', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h1.fit(X_train,y_train)\n",
    "yhat_h1 = lr_h1.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L1 regularization: \", accuracy_score(y_test['hypertension'],yhat_h1), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lr_s2 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='lasso', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s2.fit(X_train,y_train)\n",
    "yhat_s2 = lr_s2.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L2 regularization: \", accuracy_score(y_test['stroke'],yhat_s2), ' - stroke')\n",
    "\n",
    "lr_hd2 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='lasso', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd2.fit(X_train,y_train)\n",
    "yhat_hd2 = lr_hd2.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L2 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd2), ' - heart_disease')\n",
    "\n",
    "lr_h2 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='lasso', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h2.fit(X_train,y_train)\n",
    "yhat_h2 = lr_h2.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L2 regularization: \", accuracy_score(y_test['hypertension'],yhat_h2), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lr_s3 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='elastic_net', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s3.fit(X_train,y_train)\n",
    "yhat_s3 = lr_s3.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L12 regularization: \", accuracy_score(y_test['stroke'],yhat_s3), ' - stroke')\n",
    "\n",
    "lr_hd3 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='elastic_net', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd3.fit(X_train,y_train)\n",
    "yhat_hd3 = lr_hd3.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L12 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd3), ' - heart_disease')\n",
    "\n",
    "lr_h3 = LogisticRegression(optimization=\"sgd\", eta=max_eta, regularization='elastic_net', iterations=500, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h3.fit(X_train,y_train)\n",
    "yhat_h3 = lr_h3.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Stochastic Gradient L12 regularization: \", accuracy_score(y_test['hypertension'],yhat_h3), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Accuracy          Max Eta\n",
      "0.026418786692759294    0.57\n",
      "0.021526418786692758    0.57\n",
      "0.022504892367906065    0.57\n",
      "0.029354207436399216    0.55\n",
      "0.025440313111545987    0.55\n",
      "0.02446183953033268    0.55\n",
      "0.030332681017612523    0.54\n",
      "0.03131115459882583    0.5\n",
      "0.03424657534246575    0.47\n",
      "0.033268101761252444    0.45\n",
      "\n",
      "Max Accuracy       Max c\n",
      "0.022504892367906065 0.0\n",
      "\n",
      "Final eta:  0.57\n",
      "Final c:  0.0\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "max_eta = -1\n",
    "accuracy_eta_list = dict()\n",
    "for val in range(58):\n",
    "    lr = LogisticRegression(optimization='newton',eta=val/100, regularization='none', iterations=10, c=0.001)\n",
    "    for col in targets:\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test[col],yhat)\n",
    "        accuracy_eta_list[accuracy] = val/100\n",
    "new_list = dict(sorted(accuracy_eta_list.items(), key=operator.itemgetter(1), reverse=True)[:10])\n",
    "print('Max Accuracy          Max Eta')\n",
    "for key in new_list:\n",
    "    print(key, '  ', new_list[key])\n",
    "max_accuracy = list(new_list.keys())[0] \n",
    "max_eta = new_list[max_accuracy]\n",
    "    \n",
    "max_accuracy = -1\n",
    "print('\\nMax Accuracy       Max c')\n",
    "\n",
    "for val in range(100):\n",
    "    lr = LogisticRegression(optimization='newton', eta=max_eta, regularization='none', iterations=10, c=val/1000.0)\n",
    "    for col in targets:\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test[col],yhat)\n",
    "    \n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        max_c = val/1000.0\n",
    "        print(max_accuracy, max_c)\n",
    "        \n",
    "print(\"\\nFinal eta: \", max_eta)\n",
    "print(\"Final c: \", max_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Newton's Method:  0.021526418786692758  - stroke\n",
      "Accuracy of Newton's Method:  0.026418786692759294  - heart_disease\n",
      "Accuracy of Newton's Method:  0.022504892367906065  - hypertension\n",
      "--- 0.9348423480987549 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Newton's Method L1 regularization:  0.021526418786692758  - stroke\n",
      "Accuracy of Newton's Method L1 regularization:  0.026418786692759294  - heart_disease\n",
      "Accuracy of Newton's Method L1 regularization:  0.022504892367906065  - hypertension\n",
      "--- 0.921773354212443 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Newton's Method L2 regularization:  0.021526418786692758  - stroke\n",
      "Accuracy of Newton's Method L2 regularization:  0.026418786692759294  - heart_disease\n",
      "Accuracy of Newton's Method L2 regularization:  0.022504892367906065  - hypertension\n",
      "--- 0.9425066312154134 seconds ---\n",
      "\n",
      "\n",
      "Accuracy of Newton's Method L12 regularization:  0.021526418786692758  - stroke\n",
      "Accuracy of Newton's Method L12 regularization:  0.026418786692759294  - heart_disease\n",
      "Accuracy of Newton's Method L12 regularization:  0.022504892367906065  - hypertension\n",
      "--- 0.9215303262074789 seconds ---\n"
     ]
    }
   ],
   "source": [
    "lr_s0 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='none', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s0.fit(X_train,y_train)\n",
    "yhat_s0 = lr_s0.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method: \", accuracy_score(y_test['stroke'],yhat_s0), ' - stroke')\n",
    "\n",
    "lr_hd0 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='none', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd0.fit(X_train,y_train)\n",
    "yhat_hd0 = lr_hd0.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method: \", accuracy_score(y_test['heart_disease'],yhat_hd0), ' - heart_disease')\n",
    "\n",
    "lr_h0 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='none', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h0.fit(X_train,y_train)\n",
    "yhat_h0 = lr_h0.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method: \", accuracy_score(y_test['hypertension'],yhat_h0), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "lr_s1 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='ridge', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s1.fit(X_train,y_train)\n",
    "yhat_s1 = lr_s1.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L1 regularization: \", accuracy_score(y_test['stroke'],yhat_s1), ' - stroke')\n",
    "\n",
    "lr_hd1 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='ridge', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd1.fit(X_train,y_train)\n",
    "yhat_hd1 = lr_hd1.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L1 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd1), ' - heart_disease')\n",
    "\n",
    "lr_h1 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='ridge', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h1.fit(X_train,y_train)\n",
    "yhat_h1 = lr_h1.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L1 regularization: \", accuracy_score(y_test['hypertension'],yhat_h1), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "lr_s2 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='lasso', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s2.fit(X_train,y_train)\n",
    "yhat_s2 = lr_s2.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L2 regularization: \", accuracy_score(y_test['stroke'],yhat_s2), ' - stroke')\n",
    "\n",
    "lr_hd2 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='lasso', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd2.fit(X_train,y_train)\n",
    "yhat_hd2 = lr_hd2.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L2 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd2), ' - heart_disease')\n",
    "\n",
    "lr_h2 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='lasso', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h2.fit(X_train,y_train)\n",
    "yhat_h2 = lr_h2.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L2 regularization: \", accuracy_score(y_test['hypertension'],yhat_h2), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n",
    "print(\"\\n\")\n",
    "\n",
    "lr_s3 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='elastic_net', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_s3.fit(X_train,y_train)\n",
    "yhat_s3 = lr_s3.predict(X_test)\n",
    "time1 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L12 regularization: \", accuracy_score(y_test['stroke'],yhat_s3), ' - stroke')\n",
    "\n",
    "lr_hd3 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='elastic_net', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_hd3.fit(X_train,y_train)\n",
    "yhat_hd3 = lr_hd3.predict(X_test)\n",
    "time2 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L12 regularization: \", accuracy_score(y_test['heart_disease'],yhat_hd3), ' - heart_disease')\n",
    "\n",
    "lr_h3 = LogisticRegression(optimization=\"newton\", eta=max_eta, regularization='elastic_net', iterations=10, c=max_c)\n",
    "start_time = time.time()\n",
    "lr_h3.fit(X_train,y_train)\n",
    "yhat_h3 = lr_h3.predict(X_test)\n",
    "time3 = (time.time() - start_time)\n",
    "print(\"Accuracy of Newton's Method L12 regularization: \", accuracy_score(y_test['hypertension'],yhat_h3), ' - hypertension')\n",
    "\n",
    "print(\"--- {} seconds ---\".format(float(time1+time2+time3)/3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not think there was any data snooping because our method of deciding the parameters was optimized. We loop over a range of possibilities for the values of c and eta and keep the best functioning parameters that yield the highest accuracy. Because of this, we believe our method is justified and prevents data snooping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "cells": {
          "values": [
           [
            "Steepest Gradient Descent",
            "Steepest Gradient Descent with L1 Regularization",
            "Steepest Gradient Descent with L2 Regularization",
            "Steepest Gradient Descent with L1 and L2 Regularization",
            "Stochiastic Gradient Descent",
            "Stochiastic Gradient Descent with L1 Regularization",
            "Stochiastic Gradient Descent with L2 Regularization",
            "Stochiastic Gradient Descent with L1 and L2 RegularizationNewton's Method",
            "Newton's Method with L1 Regularization",
            "Newton's Method with L2 Regularization",
            "Newton's Method with L1 and L2 Regularization",
            "Sklearn",
            "Sklearn with L2 Regularization"
           ],
           [
            3430,
            3430,
            3490,
            3400,
            571,
            886,
            874,
            1120,
            148000,
            147000,
            149000,
            158000,
            354,
            364
           ],
           [
            0.7145,
            0.7145,
            0.7145,
            0.7415,
            0.7115,
            0.6905,
            0.7145,
            0.7145,
            0.728,
            0.728,
            0.728,
            0.728,
            0.7325,
            0.7345
           ],
           [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13
           ]
          ]
         },
         "header": {
          "values": [
           "Optimization/Regularization",
           "Training Time(s)",
           "Accuracy"
          ]
         },
         "type": "table"
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Stroke"
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"743735c8-1ce4-46b1-9c67-8807b1e7738c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"743735c8-1ce4-46b1-9c67-8807b1e7738c\")) {                    Plotly.newPlot(                        \"743735c8-1ce4-46b1-9c67-8807b1e7738c\",                        [{\"cells\": {\"values\": [[\"Steepest Gradient Descent\", \"Steepest Gradient Descent with L1 Regularization\", \"Steepest Gradient Descent with L2 Regularization\", \"Steepest Gradient Descent with L1 and L2 Regularization\", \"Stochiastic Gradient Descent\", \"Stochiastic Gradient Descent with L1 Regularization\", \"Stochiastic Gradient Descent with L2 Regularization\", \"Stochiastic Gradient Descent with L1 and L2 RegularizationNewton's Method\", \"Newton's Method with L1 Regularization\", \"Newton's Method with L2 Regularization\", \"Newton's Method with L1 and L2 Regularization\", \"Sklearn\", \"Sklearn with L2 Regularization\"], [3430, 3430, 3490, 3400, 571, 886, 874, 1120, 148000, 147000, 149000, 158000, 354, 364], [0.7145, 0.7145, 0.7145, 0.7415, 0.7115, 0.6905, 0.7145, 0.7145, 0.728, 0.728, 0.728, 0.728, 0.7325, 0.7345], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]]}, \"header\": {\"values\": [\"Optimization/Regularization\", \"Training Time(s)\", \"Accuracy\"]}, \"type\": \"table\"}],                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Stroke\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('743735c8-1ce4-46b1-9c67-8807b1e7738c');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "titles = ['Optimization/Regularization', 'Training Time(s)', 'Accuracy']\n",
    "columns = [\n",
    "    ['Steepest Gradient Descent', 'Steepest Gradient Descent with L1 Regularization', 'Steepest Gradient Descent with L2 Regularization', 'Steepest Gradient Descent with L1 and L2 Regularization',\n",
    "     'Stochiastic Gradient Descent', 'Stochiastic Gradient Descent with L1 Regularization', 'Stochiastic Gradient Descent with L2 Regularization', 'Stochiastic Gradient Descent with L1 and L2 Regularization'\n",
    "     'Newton\\'s Method', 'Newton\\'s Method with L1 Regularization', 'Newton\\'s Method with L2 Regularization', 'Newton\\'s Method with L1 and L2 Regularization',\n",
    "     'Sklearn', 'Sklearn with L2 Regularization'],\n",
    "    [0.16378935178120932, 0.0821917808219178, 0.9393346379647749, 0.9393346379647749, 571, 886, 874, 1120, 148000, 147000, 149000, 158000, 354, 364],\n",
    "    [.7145, .7145, .7145, .7415, .7115, .6905, .7145, .7145, .728, .728, .728, .728, .7325, .7345],\n",
    "    [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "]\n",
    "\n",
    "fig = go.Figure(data=[go.Table(header=dict(values=titles),\n",
    "                 cells=dict(values=columns))\n",
    "                     ])\n",
    "fig.update_layout(\n",
    "    title=\"Stroke\")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Accuracy of Steepest Gradient:  0.9393346379647749  - stroke\n",
    "# Accuracy of Steepest Gradient:  0.9315068493150684  - heart_disease\n",
    "# Accuracy of Steepest Gradient:  0.8845401174168297  - hypertension\n",
    "# --- 0.16378935178120932 seconds ---\n",
    "\n",
    "\n",
    "# Accuracy of Steepest Gradient L1 regularization:  0.0821917808219178  - stroke\n",
    "# Accuracy of Steepest Gradient L1 regularization:  0.09001956947162426  - heart_disease\n",
    "# Accuracy of Steepest Gradient L1 regularization:  0.13307240704500978  - hypertension\n",
    "# --- 0.15881760915120444 seconds ---\n",
    "\n",
    "\n",
    "# Accuracy of Steepest Gradient L2 regularization:  0.9393346379647749  - stroke\n",
    "# Accuracy of Steepest Gradient L2 regularization:  0.9315068493150684  - heart_disease\n",
    "# Accuracy of Steepest Gradient L2 regularization:  0.8845401174168297  - hypertension\n",
    "# --- 0.161603053410848 seconds ---\n",
    "\n",
    "\n",
    "# Accuracy of Steepest Gradient L12 regularization:  0.9393346379647749  - stroke\n",
    "# Accuracy of Steepest Gradient L12 regularization:  0.5303326810176126  - heart_disease\n",
    "# Accuracy of Steepest Gradient L12 regularization:  0.49902152641878667  - hypertension\n",
    "# --- 0.15591899553934732 seconds ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9393346379647749  -  stroke\n",
      "--- 0.04289388656616211 seconds --- \n",
      "\n",
      "0.9305283757338552  -  heart_disease\n",
      "--- 0.0422511100769043 seconds --- \n",
      "\n",
      "0.8835616438356164  -  hypertension\n",
      "--- 0.03864002227783203 seconds --- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fidelianawar/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/fidelianawar/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/fidelianawar/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "for col in targets:\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train, y_train[col])\n",
    "    clf.predict(X_test)\n",
    "    clf.predict_proba(X_test)\n",
    "    print(clf.score(X_test,y_test[col]), ' - ', col)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Which implementation of logistic regression would you advise be used in a deployed machine learning model, your implementation or scikit-learn (or other third party)? Why?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Scikit-learn's logistic regression implementation should be used over our implementation. The SKLearn's implementation is more optimized because it is written in C, which runs faster than Python. It is also more reliable because it is an open source library with more safety guards and frequent updates.\n",
    "\n",
    "Scikit-learn's implementation was also much faster than ours.\n",
    "\n",
    "Another reason to use SKlearns over ours is the results from comparing both implementations, SKlearns was 6.356 times faster than ours in training and 2.269 times faster in testing. Even though the differences in accuracy is large the differences in times are enough to go with SKlearns over ours. The reason why timing is so important is because the faster the implementation is, the quicker it is to update a model therefore it is cheaper to maintain and cheaper to scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>You have free reign to provide additional analyses. <b>One idea</b>: Update the code to use either \"one-versus-all\" or \"one-versus-one\" extensions of binary to multi-class classification. </li>\n",
    "    <li><b>Required for 7000 level students</b>: Choose ONE of the following:\n",
    "    <ul>\n",
    "        <li><b>Option One</b>: Implement an optimization technique for logistic regression using <b>mean square error</b> as your objective function (instead of binary cross entropy). Derive the gradient updates for the Hessian and use Newton's method to update the values of \"w\". Then answer, is this process better than using binary cross entropy? </li>\n",
    "        <li><b>Option Two</b>: Implement the BFGS algorithm from scratch to optimize logistic regression. That is, use BFGS without the use of an external package (for example, do not use SciPy). Compare your performance accuracy and runtime to the BFGS implementation in SciPy (that we used in lecture). </li>\n",
    "    </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-06\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'optimization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ffbc674d67f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meVals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bgd'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ridge'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'optimization'"
     ]
    }
   ],
   "source": [
    "eVals=[]\n",
    "start_e=0.000001\n",
    "for x in range(0,6):\n",
    "    eVals.append(start_e)\n",
    "    start_e*=10\n",
    "results=[]\n",
    "\n",
    "for e in eVals:\n",
    "    print(e)\n",
    "    lr = LogisticRegression(optimization='bgd',eta=e, regularization='ridge', iterations=300)\n",
    "    lr.fit(X_train, y_train)\n",
    "    pred = lr.predict(X_train)\n",
    "    encode = lambda x: lr.encodings[x]\n",
    "    y_train_encode = np.array(list(map(encode, y_train)))\n",
    "    train_mse = accuracy_score(y_train_encode, pred)\n",
    "    print(\"Training MSE: {}, Eta: {}, optimization: {}, regularization: {}\".format(train_mse,e,\"bgd\",\"ridge\"))\n",
    "    results.append([train_mse,e,\"bgd\",\"ridge\"])\n",
    "    \n",
    "    lr = LogisticRegression(optimization='bgd',eta=e, regularization='lasso', iterations=300)\n",
    "    lr.fit(X_train, y_train)\n",
    "    pred = lr.predict(X_train)\n",
    "    encode = lambda x: lr.encodings[x]\n",
    "    y_train_encode = np.array(list(map(encode, y_train)))\n",
    "    train_mse = accuracy_score(y_train_encode, pred)\n",
    "    print(\"Training MSE: {}, Eta: {}, optimization: {}, regularization: {}\".format(train_mse,e,\"bgd\",\"lasso\"))\n",
    "    results.append([train_mse,e,\"bgd\",\"lasso\"])\n",
    "    \n",
    "    lr = LogisticRegression(optimization='bgd',eta=e, regularization='elastic_net', iterations=300)\n",
    "    lr.fit(X_train, y_train)\n",
    "    pred = lr.predict(X_train)\n",
    "    encode = lambda x: lr.encodings[x]\n",
    "    y_train_encode = np.array(list(map(encode, y_train)))\n",
    "    train_mse = accuracy_score(y_train_encode, pred)\n",
    "    print(\"Training MSE: {}, Eta: {}, optimization: {}, regularization: {}\".format(train_mse,e,\"bgd\",\"elastic_net\"))\n",
    "    results.append([train_mse,e,\"bgd\",\"elastic_net\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
