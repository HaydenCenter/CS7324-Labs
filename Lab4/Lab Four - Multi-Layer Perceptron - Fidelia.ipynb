{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Four - The Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will compare the performance of multi-layer perceptrons programmed  via your own various implementations. <br>\n",
    "\n",
    "This report is worth 10% of the final grade. Please upload a report (<b>one per team</b>) with all code used, visualizations, and text in a rendered Jupyter notebook. Any visualizations that cannot be embedded in the notebook, please provide screenshots of the output. The results should be reproducible using your report. This lab project is slightly different from other reports in that you will be asked to complete more specific items.\n",
    "\n",
    "<b>Dataset Selection</b>\n",
    "\n",
    "For this assignment, you will be using a specific dataset chosen by the instructor.  This is US Census data available on Kaggle, and also downloadable from the following link: https://www.dropbox.com/s/bf7i7qjftk7cmzq/acs2017_census_tract_data.csv?dl=0 (Links to an external site.)\n",
    "\n",
    "The Kaggle description appears here: https://www.kaggle.com/muonneutrino/us-census-demographic-data/data (Links to an external site.) \n",
    "\n",
    "The classification task you will be performing is to predict, for each county, what the child poverty rate will be. You will need to convert this from regression to <b>four levels of classification</b> by quantizing the variable of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder of Assignments:\n",
    "<ul>\n",
    "    <li>Hayden</li>\n",
    "    <ul>\n",
    "        <li>Load, Split, and Balance</li>\n",
    "        <li>Pre-processing</li>\n",
    "    </ul>\n",
    "    <li>Amory</li>\n",
    "    <ul>\n",
    "        <li>Load, Split, and Balance</li>\n",
    "    </ul>\n",
    "    <li>Fidelia</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, Split, and Balance (1.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>[<b>.5 points</b>] Load the data into memory and save it to a pandas data frame. Do not normalize or one-hot encode any of the variables until asked to do so.  Remove any observations that having missing data. Encode any string data as integers for now. </li>\n",
    "    <li>[<b>.5 points</b>] Assume you are equally interested in the classification performance for each class in the dataset. Split the dataset into 80% for training and 20% for testing.</li>\n",
    "    <li>[<b>.5 points</b>] Balance the dataset so that about the same number of instances are within each class. Choose a method for balancing the dataset and explain your reasoning for selecting this method. One option is to choose quantization thresholds for the \"ChildPoverty\" variable that equally divide the data into four classes. <i>Should balancing of the dataset be done for both the training and testing set? Explain.</i></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 1.1.0\n",
      "Numpy: 1.20.1\n"
     ]
    }
   ],
   "source": [
    "# Importing packages and reading in dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print('Pandas:', pd.__version__)\n",
    "print('Numpy:',  np.__version__)\n",
    "\n",
    "df = pd.read_csv('acs2017_census_tract_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(include='object')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encoding all of the string columns as integers\n",
    "le = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        le[col] = LabelEncoder()\n",
    "        df[col] = le[col].fit_transform(df[col])\n",
    "        \n",
    "# Categorizing columns into categorical columns, continuous \n",
    "# columns, and columns to be removed from the training set\n",
    "remove_cols = [\"ChildPoverty\", \"TractId\", \"County\"]\n",
    "cat_cols = [\"State\"]\n",
    "cont_cols = list(df.columns)\n",
    "for col in set(remove_cols + cat_cols):\n",
    "    cont_cols.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(remove_cols, axis=1)\n",
    "y = df.ChildPoverty\n",
    "\n",
    "X_train, X_test, y_train_values, y_test_values = train_test_split(X.to_numpy(), y.to_numpy(),\n",
    "                                                                  test_size=.20, random_state=72)\n",
    "\n",
    "labels = [0, 1, 2, 3]\n",
    "y_train, bins = pd.qcut(y_train_values, q=4, retbins=True, labels=labels)\n",
    "y_test = pd.cut(y_test_values, bins, include_lowest=True, labels=labels)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train_values.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use Pandas' qcut function because it provides a simple way to balance the training data into 4 approximately equal classes and then apply those same class boundaries to the test data through the retbins option. Balancing should only ever be performed on the training data, not the testing data. Balancing the train and test data separately would innacurately classify the test data on the train data's class boundaries, which would reduce the accuracy. Balancing the entire dataset before splitting the train and test data will include the biases of the test data in training the model, which would then artificially increase the accuracy of the model on the test data specifically, without making the model any 'better' in terms of accuracy. Neither of these are good options, and so only the train data should be balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing (2.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>There are a number of version of the two layer perceptron covered in class. When using the example two layer network from class be sure that you use: (1) vectorized computation, (2) mini-batching, and (3) proper Glorot initialization, at a minimum.  </li>\n",
    "    <li>[<b>.5 points</b>] Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Do not normalize or one-hot encode the data (not yet). Be sure that training converges by graphing the loss function versus the number of epochs. </li>\n",
    "    <li>[<b>.5 points</b>] Now normalize the continuous numeric feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs.  </li>\n",
    "    <li>[<b>.5 points</b>] Now normalize the continuous numeric feature data AND one hot encode the categorical data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs. </li>\n",
    "    <li>[<b>1 points</b>] Compare the performance of the three models you just trained. Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have) different performances.</li>  \n",
    "        <ul>\n",
    "            <li><i>Use one-hot encoding and normalization on the dataset for the remainder of this lab assignment.</i></li>\n",
    "        </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "import sys\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0,size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        return W1, W2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X, how='column')\n",
    "        A1 = A1.T\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)  # last layer sensitivity\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2) # back prop the sensitivity \n",
    "        \n",
    "        grad2 = V2 @ A2.T # no bias on final layer\n",
    "        grad1 = V1[1:,:] @ A1.T # dont back prop sensitivity of bias\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# just start with the vectorized version and minibatch\n",
    "class TLPMiniBatch(TwoLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.0, shuffle=True, \n",
    "                 minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        rho_W1_prev = np.zeros(self.W1.shape)\n",
    "        rho_W2_prev = np.zeros(self.W2.shape)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # \\frac{\\eta}{1+\\epsilon\\cdot k}\n",
    "            eta = self.eta / (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2)\n",
    "                \n",
    "                cost = self._cost(A3,Y_enc[:, idx],self.W1,self.W2)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, \n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1,W2=self.W2)\n",
    "\n",
    "                # momentum calculations\n",
    "                rho_W1, rho_W2 = eta * grad1, eta * grad2\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev))\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev))\n",
    "                rho_W1_prev, rho_W2_prev = rho_W1, rho_W2\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to implement the new style of objective function, \n",
    "# we just need to update the final layer calculation of the gradient\n",
    "class TLPMiniBatchCrossEntropy(TLPMiniBatch):\n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = (A3-Y_enc) # <- this is only line that changed\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        grad2 = V2 @ A2.T\n",
    "        grad1 = V1[1:,:] @ A1.T\n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLPBetterInitial(TLPMiniBatchCrossEntropy):             \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_ + 1))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_ + 1))\n",
    "        W1[:,:1] = 0\n",
    "        \n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden + 1))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden + 1)) \n",
    "        W2[:,:1] = 0\n",
    "        \n",
    "        return W1, W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def print_result(nn,X_train,y_train,X_test,y_test,title=\"\",color=\"red\"):\n",
    "    \n",
    "    print(\"=================\")\n",
    "    print(title,\":\")\n",
    "    yhat = nn.predict(X_train)\n",
    "    print('Resubstitution acc:',accuracy_score(y_train,yhat))\n",
    "    \n",
    "    yhat = nn.predict(X_test)\n",
    "    print('Validation acc:',accuracy_score(y_test,yhat))\n",
    "    \n",
    "    if hasattr(nn,'val_score_'):\n",
    "        plt.plot(range(len(nn.val_score_)), nn.val_score_, color=color,label=title)\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "    else:\n",
    "        plt.plot(range(len(nn.score_)), nn.score_, color=color,label=title)\n",
    "        plt.ylabel('Resub Accuracy')\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "\n",
    "vals = { 'n_hidden':30, \n",
    "         'C':0.1, 'epochs':20, 'eta':0.001, \n",
    "         'alpha':0.001, 'decrease_const':1e-5, 'minibatches':50,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn_better = TLPBetterInitial(**vals)\n",
    "\n",
    "%time nn_better.fit(X_train, y_train, print_progress=1, XY_test=(X_test, y_test))\n",
    "\n",
    "print_result(nn_better,X_train,y_train,X_test,y_test,title=\"Glorot Initial\",color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X[cont_cols])\n",
    "\n",
    "X_scaled = X.copy()\n",
    "X_scaled[cont_cols] = scaler.transform(X[cont_cols])\n",
    "\n",
    "X_train, X_test = train_test_split(X_scaled.to_numpy(), test_size=.20, random_state=72)\n",
    "\n",
    "nn_better = TLPBetterInitial(**vals)\n",
    "\n",
    "%time nn_better.fit(X_train, y_train, print_progress=1, XY_test=(X_test, y_test))\n",
    "\n",
    "print_result(nn_better,X_train,y_train,X_test,y_test,title=\"Glorot Initial\",color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_final = X_scaled.copy()\n",
    "\n",
    "for col in cat_cols:\n",
    "    X_final = pd.concat([X_final,pd.get_dummies(X_final[col], prefix=col)],axis=1)\n",
    "    X_final.drop([col],axis=1, inplace=True)\n",
    "\n",
    "X_train, X_test = train_test_split(X_final.to_numpy(), test_size=.20, random_state=72)\n",
    "\n",
    "nn_better = TLPBetterInitial(**vals)\n",
    "\n",
    "%time nn_better.fit(X_train, y_train, print_progress=1, XY_test=(X_test, y_test))\n",
    "\n",
    "print_result(nn_better,X_train,y_train,X_test,y_test,title=\"Glorot Initial\",color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 3 models have very different performances. Before one-hot encoding the two categorical columns, the performance of the neural network was about as good as a random number generator. With an accuracy of around 25% for 4 classes, you could do about as well on the ACT by picking 'C' for every answer. The first classifier performs poorly from a combination of a high variation in feature scaling and the categorical data interfering with the prediction. Once the data is normalized, the issues with variation in feature scaling is essentially removed, because while it is technically an issue that the 'State' column will have a larger scale than all the others, the main issue with the state column is it being categorical. The model essentially starts using a state's position in the alphabet to predict the child poverty rate, since the state names were converted to integers from 0 to 49 in alphabetical order. This is obviously not a great way to predict the child poverty rate, and therefore lowers the accuracy quite a bit. Once those two issues were removed through normalization and one hot encoding the 'State' column, the network started to perform closer to how we would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>[<b>1 points</b>] Add support for a third layer in the multi-layer perceptron. Add support for saving (and plotting after training is completed) the average magnitude of the gradient for each layer, for each epoch. For magnitude calculation, you are free to use either the average absolute values or the L1/L2 norm. Quantify the performance of the model and graph the magnitudes for each layer versus the number of epochs.</li>\n",
    "    <li>[<b>1 points</b>] Repeat the previous step, adding support for a fourth layer.</li>\n",
    "    <li>[<b>1 points</b>] Repeat the previous step, adding support for a fifth layer. </li>\n",
    "    <li>[<b>2 points</b>] Implement an adaptive learning technique that was discussed in lecture and use it on the five layer network. Compare the performance of this model with and without the adaptive learning strategy. Do not use AdaM for the adaptive learning technique. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptronBase(object):   \n",
    "    def __init__(self, n_layers=3, n_hidden=[30], C=0.0, epochs=200, eta=0.001, random_state=None,\n",
    "                 alpha=0.0, decrease_const=0.0, shuffle=True, minibatches=1):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    #Glorot and He\n",
    "    def _initialize_weights(self):\n",
    "        W = []\n",
    "        \n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden[0] + self.n_features_ + 1))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden[0], self.n_features_ + 1))\n",
    "        W1[:,:1] = 0\n",
    "        W.append(W1)\n",
    "        \n",
    "        for idx in range(self.n_layers - 2):\n",
    "            init_bound = 4*np.sqrt(6 / (self.n_hidden[idx + 1] + self.n_hidden[idx] + 1))\n",
    "            WTemp = np.random.uniform(-init_bound, init_bound,(self.n_hidden[idx + 1], self.n_hidden[idx] + 1))\n",
    "            WTemp[:,:1] = 0\n",
    "            W.append(WTemp)\n",
    "        \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden[self.n_layers - 2] + 1))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden[self.n_layers - 2] + 1))\n",
    "        W2[:,:1] = 0\n",
    "        W.append(W2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        totWeight = 0\n",
    "        for w in W:\n",
    "            totWeight += np.mean(w[:, 1:] ** 2)\n",
    "        return (lambda_/2.0) * np.sqrt(totWeight)\n",
    "    \n",
    "    #Cross Entropy\n",
    "    def _cost(self,Afinal,Y_enc,W):\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(Afinal)+(1-Y_enc)*np.log(1-Afinal))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    #Mini Batching\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W = self._initialize_weights()\n",
    "        \n",
    "        delta_W_prev = []\n",
    "        for weight in self.W:\n",
    "            delta_W_prev.append(np.zeros(weight.shape))\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        \n",
    "        avgGrads = []\n",
    "        for idx in range(self.n_layers):\n",
    "                avgGrads.append([])\n",
    "        \n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        for idx in range(self.epochs):\n",
    "            #adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*idx)\n",
    "\n",
    "            if print_progress>0 and (idx+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (idx+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            \n",
    "            grads = []\n",
    "            for idx in range(self.n_layers):\n",
    "                grads.append(0)\n",
    "            \n",
    "            for idx in mini:\n",
    "                # feedforward\n",
    "                A, Z = self._feedforward(X_data[idx], self.W)\n",
    "                \n",
    "                cost = self._cost(A[len(A) - 1],Y_enc[:, idx],self.W)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad = self._get_gradient(A=A, Z=Z, \n",
    "                                          Y_enc=Y_enc[:, idx], \n",
    "                                          W=self.W)\n",
    "                \n",
    "                for idx in range(len(grad)):\n",
    "                    grads[idx] += np.mean(grad[idx])\n",
    "\n",
    "                # momentum calculations\n",
    "                delta_W = [g * self.eta for g in grad]\n",
    "                \n",
    "                for idx in range(len(delta_W)):\n",
    "                    self.W[idx] -= (delta_W[idx] + (self.alpha * delta_W_prev[idx]))\n",
    "                \n",
    "                delta_W_prev = delta_W\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            \n",
    "            for idx in range(len(avgGrads)):\n",
    "                avgGrads[idx].append(grads[idx])\n",
    "            \n",
    "        return avgGrads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(MultiLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W):\n",
    "        \"\"\"Compute feedforward step\"\"\"\n",
    "        A = []\n",
    "        Z = []\n",
    "        \n",
    "        A1 = self._add_bias_unit(X, how='column')\n",
    "        A1 = A1.T\n",
    "        \n",
    "        A.append(A1)\n",
    "        \n",
    "        for idx in range(self.n_layers - 1):\n",
    "            Z.append(W[idx] @ A[idx])\n",
    "            Atemp = self._sigmoid(Z[idx])\n",
    "            Atemp = self._add_bias_unit(Atemp, how='row')\n",
    "            A.append(Atemp)\n",
    "            \n",
    "        Z.append(W[len(W) - 1] @ A[len(A) - 1])\n",
    "        A.append(self._sigmoid(Z[len(Z) - 1]))\n",
    "            \n",
    "        return A, Z\n",
    "    \n",
    "    def _get_gradient(self, A, Z, Y_enc, W):\n",
    "        grad = []\n",
    "        Vfinal = (A[len(A) - 1] - Y_enc)\n",
    "        Vprev = Vfinal\n",
    "        tempGrad = (Vfinal @ A[len(A) - 2].T)\n",
    "        tempGrad[:, 1:] += W[len(W) - 1][:, 1:] * self.l2_C\n",
    "        grad.append(tempGrad)\n",
    "                \n",
    "        for i in range(self.n_layers - 2):\n",
    "            V = A[len(A) - i - 2]*(1 - A[len(A) - i - 2]) * (W[len(W) - i - 1].T @ Vprev)\n",
    "            Vprev = V[1:,:] \n",
    "            tempGrad = V[1:,:] @ A[len(A) - i - 3].T\n",
    "            tempGrad[:, 1:] += W[len(W) - i - 2][:, 1:] * self.l2_C\n",
    "            grad.insert(0, tempGrad)\n",
    "            \n",
    "        V = A[1]*(1 - A[1]) * (W[1].T @ Vprev)\n",
    "        tempGrad = V[1:,:] @ A[0].T\n",
    "        tempGrad[:, 1:] += W[0][:, 1:] * self.l2_C\n",
    "        grad.insert(0, tempGrad)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        A, Z = self._feedforward(X, self.W)\n",
    "        y_pred = np.argmax(A[len(A) - 1], axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def print_result(nn,X_train,y_train,X_test,y_test,grads, title=\"\",color=\"red\"):\n",
    "    \n",
    "    print(\"=================\")\n",
    "    print(title,\":\")\n",
    "    yhat = nn.predict(X_train)\n",
    "    print('Resubstitution acc:',accuracy_score(y_train,yhat))\n",
    "    \n",
    "    yhat = nn.predict(X_test)\n",
    "    print('Validation acc:',accuracy_score(y_test,yhat))\n",
    "    \n",
    "    if hasattr(nn,'val_score_'):\n",
    "        plt.plot(range(len(nn.val_score_)), nn.val_score_, color=color,label=title)\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "    else:\n",
    "        plt.plot(range(len(nn.score_)), nn.score_, color=color,label=title)\n",
    "        plt.ylabel('Resub Accuracy')\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    for idx in range(len(grads)):\n",
    "        plt.plot(range(len(grads[0])), grads[idx], label=\"grad_\"+str(idx + 1))\n",
    "    \n",
    "    plt.ylabel('Average Magnitude of the Gradient')\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = { 'n_layers':3, 'n_hidden':[30, 25], \n",
    "         'C':0.1, 'epochs':100, 'eta':0.001, \n",
    "         'alpha':0.001, 'decrease_const':0.0001, 'minibatches':50,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn3 = MultiLayerPerceptron(**vals)\n",
    "\n",
    "%time \n",
    "grads = nn3.fit(X_train, y_train, print_progress=1)\n",
    "\n",
    "print_result(nn3,X_train,y_train, X_test, y_test,grads,title=\"Cross Entropy Loss\",color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = { 'n_layers':4, 'n_hidden':[30, 25, 20], \n",
    "         'C':0.1, 'epochs':100, 'eta':0.001, \n",
    "         'alpha':0.001, 'decrease_const':0.0001, 'minibatches':50,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn4 = MultiLayerPerceptron(**vals)\n",
    "\n",
    "%time\n",
    "grads = nn4.fit(X_train, y_train, print_progress=1)\n",
    "\n",
    "print_result(nn4,X_train,y_train, X_test, y_test,grads,title=\"Cross Entropy Loss\",color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = { 'n_layers':5, 'n_hidden':[30, 25, 20, 15], \n",
    "         'C':0.1, 'epochs':100, 'eta':0.001, \n",
    "         'alpha':0.001, 'decrease_const':0.0001, 'minibatches':50,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn5 = MultiLayerPerceptron(**vals)\n",
    "\n",
    "%time\n",
    "grads = nn5.fit(X_train, y_train, print_progress=1)\n",
    "\n",
    "print_result(nn5,X_train,y_train, X_test, y_test,grads,title=\"Cross Entropy Loss\",color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no adaptive learning\n",
    "class MLP(MultiLayerPerceptron):\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        \n",
    "        avgGrads = []\n",
    "        for idx in range(self.n_layers):\n",
    "                avgGrads.append([])\n",
    "        \n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        \n",
    "        for idx in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (idx+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (idx+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            \n",
    "            grads = []\n",
    "            \n",
    "            for idx in range(self.n_layers):\n",
    "                grads.append(0)\n",
    "            \n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A, Z = self._feedforward(X_data[idx], self.W)\n",
    "                \n",
    "                cost = self._cost(A[len(A) - 1],Y_enc[:, idx],self.W)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad = self._get_gradient(A=A, Z=Z, Y_enc=Y_enc[:, idx], W=self.W)\n",
    "                \n",
    "                for idx in range(len(grad)):\n",
    "                    grads[idx] += np.mean(grad[idx])\n",
    "\n",
    "                delta_W = [g * self.eta for g in grad]\n",
    "                \n",
    "                for idx in range(len(delta_W)):\n",
    "                    self.W[idx] -= delta_W[idx]\n",
    "                \n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            \n",
    "            for idx in range(len(avgGrads)):\n",
    "                avgGrads[idx].append(grads[idx])\n",
    "            \n",
    "        return avgGrads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = { 'n_layers':5, 'n_hidden':[30, 25, 20, 15], \n",
    "         'C':0.1, 'epochs':100, 'eta':0.001, \n",
    "         'alpha':0.001, 'decrease_const':0.0001, 'minibatches':50,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn6 = MLP(**vals)\n",
    "\n",
    "%time\n",
    "grads = nn6.fit(X_train, y_train, print_progress=1)\n",
    "\n",
    "print_result(nn6,X_train,y_train, X_test, y_test,grads,title=\"Cross Entropy Loss\",color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (1 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>5000 level student: You have free reign to provide additional analyses.</li>\n",
    "    <li>One idea (<b>required for 7000 level students</b>):  Implement adaptive momentum (AdaM) in the five layer neural network and quantify the performance. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
