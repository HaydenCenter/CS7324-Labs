{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Six -  Convolutional Network Architectures\n",
    "Amory Weinzierl, Fidelia Nawar, and Hayden Center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will select a prediction task to perform on your dataset, evaluate a deep learning architecture and tune hyper-parameters. If any part of the assignment is not clear, ask the instructor to clarify. \n",
    "\n",
    "This report is worth 10% of the final grade. Please upload a report (<b>one per team</b>) with all code used, visualizations, and text in a rendered Jupyter notebook. Any visualizations that cannot be embedded in the notebook, please provide screenshots of the output. The results should be reproducible using your report. Please carefully describe every assumption and every step in your report.\n",
    "\n",
    "<b>Dataset Selection</b>\n",
    "\n",
    "Select a dataset identically to lab two (images). That is, the dataset must be image data. In terms of generalization performance, it is helpful to have a large dataset of identically sized images. It is fine to perform binary classification or multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [<b>1.5 points</b>] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a <b>detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate</b> for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "- [<b>1.5 points</b>] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). <b>Explain why your chosen method is appropriate or use more than one method as appropriate</b>. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages and reading in dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "print('Pandas:', pd.__version__)\n",
    "print('Numpy:',  np.__version__)\n",
    "print('Tensorflow:', tf.__version__)\n",
    "print('Keras:',  keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#source: https://www.geeksforgeeks.org/how-to-convert-images-to-numpy-array/\n",
    "from PIL import Image\n",
    "\n",
    "#source: https://stackoverflow.com/questions/10377998/how-can-i-iterate-over-files-in-a-given-directory\n",
    "from pathlib import Path\n",
    "\n",
    "#directory name\n",
    "paths = {\n",
    "    \"TRAIN\": './Coronahack-Chest-XRay-Dataset/train/',\n",
    "    \"TEST\":  './Coronahack-Chest-XRay-Dataset/test/'    \n",
    "}\n",
    "metadata = pd.read_csv('Chest_xray_Corona_Metadata.csv')\n",
    "\n",
    "h, w = 64, 64\n",
    "\n",
    "tf.random.set_seed(2)\n",
    "np.random.seed(0) # using this to help make results reproducible\n",
    "\n",
    "#shuffle data\n",
    "# data = data.sample(frac=1)\n",
    "\n",
    "# Define features and target\n",
    "images = metadata[[\"X_ray_image_name\", \"Dataset_type\"]]\n",
    "X_data = []\n",
    "y_data = metadata[\"Label\"]\n",
    "for idx, img in images.iterrows():\n",
    "    name = img[\"X_ray_image_name\"]\n",
    "    path = img[\"Dataset_type\"]\n",
    "    img_arr = np.asarray(Image.open(paths[path] + name).convert('L').resize((h,w)))\n",
    "    X_data.append(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "X = np.expand_dims(np.array(X_data), axis=-1)/255 - 0.5\n",
    "y = le.fit_transform(np.array(y_data))\n",
    "\n",
    "# X = X/255 - 0.5\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "display_imgs = np.concatenate((X[0:9], X[-9:]))\n",
    "labels = np.concatenate((y_data[0:9], y_data[-9:]))\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=3):\n",
    "    plt.figure(figsize=(n_col * n_col, 6 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    #normal scans tended towards front\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row * 2, n_col, i + 1)\n",
    "        plt.imshow(images[i], cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    #pnemonia scans toward back so we pulled some from the back \n",
    "    #for demonstration purposes\n",
    "    for j in range(n_row * n_col):\n",
    "        plt.subplot(n_row * 2, n_col, n_row * n_col + j + 1)\n",
    "        plt.imshow(images[-1*j], cmap=plt.cm.gray)\n",
    "        plt.title(titles[-1*j], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        \n",
    "plot_gallery(display_imgs, labels, 100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metric\n",
    "\n",
    "The primary evaluation metrics we are using for our model are recall and precision. Recall measures the percentage of positive cases that were identified correctly, and precision measures the percentage of positive predictions that were correct.\n",
    "\n",
    "These metrics emphasizes correct positive identifications, which is applicable to evaluate our solution because we want to minimize the amount of undetected pneumonia lungs, though recall is the more important metric, as it can be used to minimize the false negative rate. Having a low false negative rate is important in this situation because a diagnosis of a \"Normal\" lung condition when it is in fact penumonia is detrimental and possibly fatal to the patient. On the same token, it's necessary that healthy lungs are not misclassified as pneumonia because that would create unnecessary issues for a healthy patient. Because of this, we chose to use recall and precision, specifically the native Keras implementation of both, to evaluate our CNN solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing Data\n",
    "\n",
    "We are using stratified 10-fold cross validation in order to split up the data into training and test sets. We chose to use this method because almost 3/4 of our the lungs in our dataset are labeled as having pneumonia, whereas only 1/4 is labeled as healthy. Thus, if we did a random split/shuffle, there may be disproportionate amounts of pneumonia classification in the training variables, which would make the classification for the testing data less accurate. With \n",
    "stratified 10-fold cross validation, we can make a more effective model and also help with generalizing. It allows us to select training and testing sets while also decreasing overall variance because of the 10 folds, which will fit each CNN on each fold. This would be a realistic measuring of a real-world application of the algorithm because with smaller test sets, there is higher variance. Stratified cross validation reduces this variance by averaging over k different partitions, so the performance estimate is less sensitive to the partitioning of the data. We also chose 10 folds because this value has been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (6 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [<b>1.5 points</b>]  Setup the training to use data expansion in Keras. Explain why the chosen data expansion techniques are appropriate for your dataset. \n",
    "- [<b>2 points</b>] Create a convolutional neural network to use on your data using Keras. Investigate at least two different convolutional network architectures (and investigate changing some parameters of each architecture--at minimum have two variations of each network for a total of four models trained). Use the method of train/test splitting and evaluation metric that you argued for at the beginning of the lab. Visualize the performance of the training and validation sets per iteration (use the \"history\" parameter of Keras).\n",
    "- [<b>1.5 points</b>] Visualize the final results of the CNNs and interpret the performance. Use proper statistics as appropriate, especially for comparing models. \n",
    "- [<b>1 points</b>] Compare the performance of your convolutional network to a standard multi-layer perceptron (MLP) using the receiver operating characteristic and area under the curve. Use proper statistical comparison techniques.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using Keras's built in ImageDataGenerator for our data expansion. In reshaping all of our images to 128x128, many of the images were already stretched and squashed in different directions, and so expanding our dataset to stretch and squash them more randomly will hopefully remove any hidden biases that the different image sizes may have created. Additionally, since all of the xrays are more or less similarly oriented, we can add a slight rotational adjustment. However, since the images should all be uniquely oriented horizontally (because the heart is always located to one side of the body) and vertically (all of the images have the patients neck and shoulders on the top side of the image), it would not be useful to flip the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=5,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1)\n",
    "\n",
    "datagen.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers       import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers       import Dense, Dropout, Flatten, Activation, Average, BatchNormalization\n",
    "from tensorflow.keras.models       import Model, Sequential\n",
    "from tensorflow.keras.callbacks    import EarlyStopping\n",
    "from tensorflow.keras.utils        import plot_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "optimizer = 'rmsprop'\n",
    "metrics = [keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "batch_size = 128\n",
    "epochs = 3\n",
    "verbose = 1\n",
    "n_splits = 1\n",
    "kf = StratifiedKFold(n_splits=2, shuffle=True, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_histories(histories):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    for fold_no, history in enumerate(histories):\n",
    "        keys = list(history.history.keys())\n",
    "        \n",
    "        plt.subplot(n_splits,3,3*fold_no+1)\n",
    "        plt.plot(history.history[keys[0]])\n",
    "        plt.title('Binary Crossentropy')\n",
    "        plt.ylim(0.25, 1.25)\n",
    "        plt.ylabel('Fold #'+str(fold_no))\n",
    "\n",
    "        plt.subplot(n_splits,3,3*fold_no+2)\n",
    "        plt.plot(history.history[keys[1]])\n",
    "        plt.title('Precision')\n",
    "        plt.ylim(0.7, 1)\n",
    "\n",
    "        plt.subplot(n_splits,3,3*fold_no+3)\n",
    "        plt.plot(history.history[keys[2]])\n",
    "        plt.title('Recall')\n",
    "        plt.ylim(0.7, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Basic Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basic_model(l2_lambda):\n",
    "    reg = l2(l2_lambda)\n",
    "    print(\"Basic Architecture\")\n",
    "    print(\"L2 Lambda:\", l2_lambda,'\\n')\n",
    "\n",
    "    fold_no = 0\n",
    "    histories = []\n",
    "    eval_scores = []\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        cnn = Sequential()\n",
    "\n",
    "        cnn.add(Conv2D(filters=32,\n",
    "                    kernel_size=(3,3),\n",
    "                    kernel_regularizer=reg,\n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "        cnn.add(Conv2D(filters=32,\n",
    "                    kernel_size=(3,3),\n",
    "                    kernel_regularizer=reg,\n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "        cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        cnn.add(Conv2D(filters=64,\n",
    "                    kernel_size=(3,3),\n",
    "                    kernel_regularizer=reg,\n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "        cnn.add(Conv2D(filters=64,\n",
    "                    kernel_size=(3,3),\n",
    "                    kernel_regularizer=reg,\n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "        cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        cnn.add(Dropout(0.25))\n",
    "        cnn.add(Flatten())\n",
    "        cnn.add(Dense(128, activation='relu',\n",
    "                    kernel_regularizer=reg))\n",
    "        cnn.add(Dropout(0.5))\n",
    "        cnn.add(Dense(1, activation='sigmoid',\n",
    "                    kernel_regularizer=reg))\n",
    "\n",
    "        cnn.compile(loss=loss,\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=metrics)\n",
    "\n",
    "        print('Fold',fold_no)\n",
    "        print('')  \n",
    "        history = cnn.fit(datagen.flow(X_train, y_train, batch_size=batch_size), \n",
    "                    steps_per_epoch=int(len(X_train)/batch_size),\n",
    "                    epochs=epochs, verbose=verbose)\n",
    "\n",
    "        print('')\n",
    "        scores = cnn.evaluate(X_test, y_test, verbose=verbose)\n",
    "        print('-' * 110)\n",
    "\n",
    "        histories.append(history)\n",
    "        eval_scores.append(scores)\n",
    "\n",
    "        fold_no += 1\n",
    "\n",
    "    eval_scores = np.array(eval_scores)\n",
    "    print(\"Average Performance\")\n",
    "    print(f\"Precision:  {round(np.mean(eval_scores[:,1]), 5)}\")\n",
    "    print(f\"Recall:     {round(np.mean(eval_scores[:,2]), 5)}\")\n",
    "    \n",
    "    return histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "histories = basic_model(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_histories(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "histories = basic_model(0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_histories(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "histories = basic_model(0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_histories(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Network in Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Architecture based on https://www.kaggle.com/bingdiaoxiaomao/network-in-network-nin-with-keras\n",
    "\n",
    "def nin_model(l2_lambda):\n",
    "    reg = l2(l2_lambda)\n",
    "    print(\"NiN Architecture\")\n",
    "    print(\"L2 Lambda:\", l2_lambda,'\\n')\n",
    "\n",
    "    fold_no = 0\n",
    "    histories = []\n",
    "    eval_scores = []\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        cnn = Sequential()\n",
    "\n",
    "        cnn.add(Conv2D(filters=192,\n",
    "                    kernel_size=5,\n",
    "                    kernel_regularizer=reg,\n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "        cnn.add(Conv2D(filters=160,\n",
    "                    kernel_size=1,\n",
    "                    kernel_regularizer=reg,\n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "        cnn.add(Conv2D(filters=96,\n",
    "                    kernel_size=1,\n",
    "                    kernel_regularizer=reg,\n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "        cnn.add(MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same'))\n",
    "        \n",
    "        cnn.add(Dropout(0.2))\n",
    "\n",
    "        cnn.add(Conv2D(filters=192,\n",
    "                    kernel_size=5,\n",
    "                    kernel_regularizer=reg,\n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "        cnn.add(Conv2D(filters=192,\n",
    "                    kernel_size=1,\n",
    "                    kernel_regularizer=reg,\n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "        cnn.add(Conv2D(filters=192,\n",
    "                    kernel_size=1,\n",
    "                    kernel_regularizer=reg,\n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "        cnn.add(MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same'))\n",
    "        \n",
    "        cnn.add(Dropout(0.2))\n",
    "\n",
    "        cnn.add(Conv2D(filters=192,\n",
    "                    kernel_size=3,\n",
    "                    kernel_regularizer=reg,\n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "        cnn.add(Conv2D(filters=192,\n",
    "                    kernel_size=1,\n",
    "                    kernel_regularizer=reg,\n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "        cnn.add(Conv2D(filters=2,\n",
    "                    kernel_size=1,\n",
    "                    kernel_regularizer=reg,\n",
    "                    padding='same',\n",
    "                    activation='relu'))\n",
    "        \n",
    "        cnn.add(GlobalAveragePooling2D())\n",
    "        cnn.add(Activation('softmax'))   \n",
    "\n",
    "        cnn.compile(loss=loss,\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=metrics)\n",
    "\n",
    "        print('Fold',fold_no)\n",
    "        print('')  \n",
    "        # There seems to be an issue with this architecture for a 1D output for binary classification, so it is modified\n",
    "        # output a 2D output as if it were a multiclass classifier\n",
    "        history = cnn.fit(datagen.flow(X, np.array(pd.get_dummies(y)), batch_size=batch_size), \n",
    "                    steps_per_epoch=int(len(X)/batch_size),\n",
    "                    epochs=epochs, verbose=verbose)\n",
    "\n",
    "        print('')\n",
    "        scores = cnn.evaluate(X_test, y_test, verbose=verbose)\n",
    "        print('-' * 110)\n",
    "        \n",
    "        histories.append(history)\n",
    "        eval_scores.append(scores)\n",
    "\n",
    "        fold_no += 1\n",
    "\n",
    "    eval_scores = np.array(eval_scores)\n",
    "    print(\"Average Performance\")\n",
    "    print(f\"Precision:  {round(np.mean(eval_scores[:,1]), 5)}\")\n",
    "    print(f\"Recall:     {round(np.mean(eval_scores[:,2]), 5)}\")\n",
    "    \n",
    "    return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "histories = nin_model(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories(histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "histories = nin_model(0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories(histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "histories = nin_model(0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have free reign to provide additional analyses. \n",
    "- One idea (<b>required for 7000 level students</b>): Use transfer learning to pre-train the weights of your initial layers of your CNN. Compare the performance when using transfer learning to training without transfer learning (i.e., compare to your best model from above) in terms of classification performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
