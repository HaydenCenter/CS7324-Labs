{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Six -  Convolutional Network Architectures\n",
    "Amory Weinzierl, Fidelia Nawar, and Hayden Center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will select a prediction task to perform on your dataset, evaluate a deep learning architecture and tune hyper-parameters. If any part of the assignment is not clear, ask the instructor to clarify. \n",
    "\n",
    "This report is worth 10% of the final grade. Please upload a report (<b>one per team</b>) with all code used, visualizations, and text in a rendered Jupyter notebook. Any visualizations that cannot be embedded in the notebook, please provide screenshots of the output. The results should be reproducible using your report. Please carefully describe every assumption and every step in your report.\n",
    "\n",
    "<b>Dataset Selection</b>\n",
    "\n",
    "Select a dataset identically to lab two (images). That is, the dataset must be image data. In terms of generalization performance, it is helpful to have a large dataset of identically sized images. It is fine to perform binary classification or multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [<b>1.5 points</b>] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a <b>detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate</b> for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "- [<b>1.5 points</b>] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). <b>Explain why your chosen method is appropriate or use more than one method as appropriate</b>. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages and reading in dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "print('Pandas:', pd.__version__)\n",
    "print('Numpy:',  np.__version__)\n",
    "print('Tensorflow:', tf.__version__)\n",
    "print('Keras:',  keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#source: https://www.geeksforgeeks.org/how-to-convert-images-to-numpy-array/\n",
    "from PIL import Image\n",
    "\n",
    "#source: https://stackoverflow.com/questions/10377998/how-can-i-iterate-over-files-in-a-given-directory\n",
    "from pathlib import Path\n",
    "\n",
    "#directory name\n",
    "paths = {\n",
    "    \"TRAIN\": './Coronahack-Chest-XRay-Dataset/train/',\n",
    "    \"TEST\":  './Coronahack-Chest-XRay-Dataset/test/'    \n",
    "}\n",
    "metadata = pd.read_csv('Chest_xray_Corona_Metadata.csv')\n",
    "\n",
    "h, w = 64, 64\n",
    "\n",
    "tf.random.set_seed(2)\n",
    "np.random.seed(0) # using this to help make results reproducible\n",
    "\n",
    "images = metadata[[\"X_ray_image_name\", \"Dataset_type\"]]\n",
    "X_data = []\n",
    "y_data = metadata[\"Label\"]\n",
    "for idx, img in images.iterrows():\n",
    "    name = img[\"X_ray_image_name\"]\n",
    "    path = img[\"Dataset_type\"]\n",
    "    img_arr = np.asarray(Image.open(paths[path] + name).convert('L').resize((h,w)))\n",
    "    X_data.append(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "_X = np.expand_dims(np.array(X_data), axis=-1)/255 - 0.5\n",
    "_y = le.fit_transform(np.array(y_data))\n",
    "\n",
    "print(_X.shape, _y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "display_imgs = np.concatenate((_X[0:9], _X[-9:]))\n",
    "labels = np.concatenate((y_data[0:9], y_data[-9:]))\n",
    "def plot_gallery(images, titles, n_row=3, n_col=3):\n",
    "    plt.figure(figsize=(n_col * n_col, 6 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    #normal scans tended towards front\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row * 2, n_col, i + 1)\n",
    "        plt.imshow(images[i], cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    #pnemonia scans toward back so we pulled some from the back \n",
    "    #for demonstration purposes\n",
    "    for j in range(n_row * n_col):\n",
    "        plt.subplot(n_row * 2, n_col, n_row * n_col + j + 1)\n",
    "        plt.imshow(images[-1*j], cmap=plt.cm.gray)\n",
    "        plt.title(titles[-1*j], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        \n",
    "plot_gallery(display_imgs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metric\n",
    "\n",
    "The primary evaluation metrics we are using for our model are recall and precision. Recall measures the percentage of positive cases that were identified correctly, and precision measures the percentage of positive predictions that were correct.\n",
    "\n",
    "These metrics emphasizes correct positive identifications, which is applicable to evaluate our solution because we want to minimize the amount of undetected pneumonia lungs, though recall is the more important metric, as it can be used to minimize the false negative rate. Having a low false negative rate is important in this situation because a diagnosis of a \"Normal\" lung condition when it is in fact penumonia is detrimental and possibly fatal to the patient. On the same token, it's necessary that healthy lungs are not misclassified as pneumonia because that would create unnecessary issues for a healthy patient. Because of this, we chose to use recall and precision, specifically the native Keras implementation of both, to evaluate our CNN solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing Data\n",
    "\n",
    "We are using stratified 10-fold cross validation in order to split up the data into training and test sets. We chose to use this method because almost 3/4 of our the lungs in our dataset are labeled as having pneumonia, whereas only 1/4 is labeled as healthy. Thus, if we did a random split/shuffle, there may be disproportionate amounts of pneumonia classification in the training variables, which would make the classification for the testing data less accurate. With \n",
    "stratified 10-fold cross validation, we can make a more effective model and also help with generalizing. It allows us to select training and testing sets while also decreasing overall variance because of the 10 folds, which will fit each CNN on each fold. This would be a realistic measuring of a real-world application of the algorithm because with smaller test sets, there is higher variance. Stratified cross validation reduces this variance by averaging over k different partitions, so the performance estimate is less sensitive to the partitioning of the data. We also chose 10 folds because this value has been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.\n",
    "\n",
    "Additionally, we will be using an 80/20 split, where the 80% test set will be used for cross validation, and then used to train our final models for statistical comparisons of performance on the 20% split, as cross validation does not render a final trained model, and is only useful for comparing our evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, X_final, y, y_final = train_test_split(_X, _y, test_size=0.2, stratify=_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (6 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [<b>1.5 points</b>]  Setup the training to use data expansion in Keras. Explain why the chosen data expansion techniques are appropriate for your dataset. \n",
    "- [<b>2 points</b>] Create a convolutional neural network to use on your data using Keras. Investigate at least two different convolutional network architectures (and investigate changing some parameters of each architecture--at minimum have two variations of each network for a total of four models trained). Use the method of train/test splitting and evaluation metric that you argued for at the beginning of the lab. Visualize the performance of the training and validation sets per iteration (use the \"history\" parameter of Keras).\n",
    "- [<b>1.5 points</b>] Visualize the final results of the CNNs and interpret the performance. Use proper statistics as appropriate, especially for comparing models. \n",
    "- [<b>1 points</b>] Compare the performance of your convolutional network to a standard multi-layer perceptron (MLP) using the receiver operating characteristic and area under the curve. Use proper statistical comparison techniques.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using Keras's built in ImageDataGenerator for our data expansion. In reshaping all of our images to 128x128, many of the images were already stretched and squashed in different directions, and so expanding our dataset to stretch and squash them more randomly will hopefully remove any hidden biases that the different image sizes may have created. Additionally, since all of the xrays are more or less similarly oriented, we can add a slight rotational adjustment. However, since the images should all be uniquely oriented horizontally (because the heart is always located to one side of the body) and vertically (all of the images have the patients neck and shoulders on the top side of the image), it would not be useful to flip the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=1,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05)\n",
    "\n",
    "datagen.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers       import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers       import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.models       import Model, Sequential\n",
    "from tensorflow.keras.callbacks    import EarlyStopping\n",
    "from tensorflow.keras.utils        import plot_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "optimizer = 'rmsprop'\n",
    "metrics = [keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "verbose = 1\n",
    "n_splits = 10\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1234)\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histories(histories, title):\n",
    "    fig, subplots = plt.subplots(2,3,figsize=(15,n_splits*4))\n",
    "    fold_names = [\"Fold \" + str(fold) for fold in range(n_splits)]\n",
    "    for fold_no, history in enumerate(histories):\n",
    "        keys = list(history.history.keys())\n",
    "        \n",
    "        subplots[0,0].plot(history.history[keys[0]], label=fold_no)\n",
    "        subplots[0,0].set_title('Binary Crossentropy')\n",
    "\n",
    "        subplots[0,1].plot(history.history[keys[1]], label=fold_no)\n",
    "        subplots[0,1].set_title('Precision')\n",
    "        subplots[0,1].set_ylim(0.4, 1.1)\n",
    "\n",
    "        subplots[0,2].plot(history.history[keys[2]], label=fold_no)\n",
    "        subplots[0,2].set_title('Recall')\n",
    "        subplots[0,2].set_ylim(0.4, 1.1)\n",
    "        \n",
    "        subplots[1,0].plot(history.history[keys[3]], label=fold_no)\n",
    "        subplots[1,0].set_title('Validation Binary Crossentropy')\n",
    "\n",
    "        subplots[1,1].plot(history.history[keys[4]], label=fold_no)\n",
    "        subplots[1,1].set_title('Validation Precision')\n",
    "        subplots[1,1].set_ylim(0.4, 1.1)\n",
    "\n",
    "        subplots[1,2].plot(history.history[keys[5]], label=fold_no)\n",
    "        subplots[1,2].set_title('Validation Recall')\n",
    "        subplots[1,2].set_ylim(0.4, 1.1)\n",
    "    handles, labels = subplots[1,2].get_legend_handles_labels()\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    fig.legend(handles, labels, title=\"Fold #\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Basic Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_basic_model(kernel_size, metrics):\n",
    "    reg = l2(0.00001)\n",
    "    cnn = Sequential()\n",
    "\n",
    "    cnn.add(Conv2D(filters=32,\n",
    "                kernel_size=kernel_size,\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same',\n",
    "                activation='relu'))\n",
    "    cnn.add(Conv2D(filters=32,\n",
    "                kernel_size=kernel_size,\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same',\n",
    "                activation='relu'))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    cnn.add(Conv2D(filters=64,\n",
    "                kernel_size=kernel_size,\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same',\n",
    "                activation='relu'))\n",
    "    cnn.add(Conv2D(filters=64,\n",
    "                kernel_size=kernel_size,\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same',\n",
    "                activation='relu'))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    cnn.add(Dropout(0.25))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(128, activation='relu',\n",
    "                kernel_regularizer=reg))\n",
    "    cnn.add(Dropout(0.5))\n",
    "    cnn.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=reg))\n",
    "\n",
    "    cnn.compile(loss=loss,\n",
    "                optimizer=optimizer,\n",
    "                metrics=metrics)\n",
    "    \n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_model(kernel_size, metrics):\n",
    "    print(\"Basic Architecture\")\n",
    "    print(\"Kernel Size:\", kernel_size,'\\n')\n",
    "\n",
    "    fold_no = 0\n",
    "    histories = []\n",
    "    eval_scores = []\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        cnn = build_basic_model(kernel_size, metrics)\n",
    "\n",
    "        print('Fold',fold_no)\n",
    "        print('')\n",
    "        \n",
    "        history = cnn.fit(datagen.flow(X_train, y_train, batch_size=batch_size), \n",
    "                          steps_per_epoch=int(len(X_train)/batch_size),\n",
    "                          epochs=epochs, verbose=verbose,\n",
    "                          validation_data=(X_test, y_test))\n",
    "\n",
    "        print('')\n",
    "        scores = cnn.evaluate(X_test, y_test, verbose=verbose)\n",
    "        print('-' * 110)\n",
    "\n",
    "        histories.append(history)\n",
    "        eval_scores.append(scores)\n",
    "\n",
    "        fold_no += 1\n",
    "\n",
    "    eval_scores = np.array(eval_scores)\n",
    "    print(\"Average Performance\")\n",
    "    print(f\"Precision:  {round(np.mean(eval_scores[:,1]), 5)}\")\n",
    "    print(f\"Recall:     {round(np.mean(eval_scores[:,2]), 5)}\")\n",
    "    \n",
    "    return histories, eval_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "histories1, eval_scores1 = basic_model(3, metrics)\n",
    "scores.append(('basic', 3, eval_scores1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title1 = \"CNN - Kernel Size: 3\"\n",
    "plot_histories(histories1, title1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "histories2, eval_scores2 = basic_model(5, metrics)\n",
    "scores.append(('basic', 5, eval_scores2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title2 = \"CNN - Kernel Size: 5\"\n",
    "plot_histories(histories2, title2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Network in Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nin_model(kernel_size, metrics):\n",
    "    reg = l2(0.00001)\n",
    "    cnn = Sequential()\n",
    "\n",
    "    cnn.add(Conv2D(filters=32,\n",
    "                kernel_size=kernel_size,\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same'))\n",
    "    cnn.add(Activation('relu'))\n",
    "    cnn.add(Conv2D(filters=32,\n",
    "                kernel_size=(1,1),\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same'))\n",
    "    cnn.add(Activation('relu'))\n",
    "    cnn.add(Conv2D(filters=32,\n",
    "                kernel_size=(1,1),\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same'))\n",
    "    cnn.add(Activation('relu'))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    cnn.add(Dropout(0.2))\n",
    "\n",
    "    cnn.add(Conv2D(filters=64,\n",
    "                kernel_size=kernel_size,\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same'))\n",
    "    cnn.add(Activation('relu'))\n",
    "    cnn.add(Conv2D(filters=64,\n",
    "                kernel_size=(1,1),\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same'))\n",
    "    cnn.add(Activation('relu'))\n",
    "    cnn.add(Conv2D(filters=64,\n",
    "                kernel_size=(1,1),\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same'))\n",
    "    cnn.add(Activation('relu'))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    cnn.add(Dropout(0.2))\n",
    "\n",
    "    cnn.add(Conv2D(filters=64,\n",
    "                kernel_size=kernel_size,\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same'))\n",
    "    cnn.add(Activation('relu'))\n",
    "    cnn.add(Conv2D(filters=64,\n",
    "                kernel_size=(1,1),\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same'))\n",
    "    cnn.add(Activation('relu'))\n",
    "    cnn.add(Conv2D(filters=1,\n",
    "                kernel_size=(1,1),\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same'))\n",
    "    cnn.add(Activation('relu'))\n",
    "\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=reg))\n",
    "\n",
    "    cnn.compile(loss=loss,\n",
    "                optimizer=optimizer,\n",
    "                metrics=metrics)\n",
    "    \n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture based on https://www.kaggle.com/bingdiaoxiaomao/network-in-network-nin-with-keras\n",
    "\n",
    "def nin_model(kernel_size, metrics):\n",
    "    print(\"NiN Architecture\")\n",
    "    print(\"Kernel Size:\", kernel_size,'\\n')\n",
    "\n",
    "    fold_no = 0\n",
    "    histories = []\n",
    "    eval_scores = []\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        cnn = build_nin_model(kernel_size, metrics)\n",
    "\n",
    "        print('Fold',fold_no)\n",
    "        print('')\n",
    "        \n",
    "        history = cnn.fit(datagen.flow(X_train, y_train, batch_size=batch_size), \n",
    "                          steps_per_epoch=int(len(X_train)/batch_size),\n",
    "                          epochs=epochs, verbose=verbose,\n",
    "                          validation_data=(X_test, y_test))\n",
    "\n",
    "        print('')\n",
    "        scores = cnn.evaluate(X_test,y_test, verbose=verbose)\n",
    "        print('-' * 110)\n",
    "        \n",
    "        histories.append(history)\n",
    "        eval_scores.append(scores)\n",
    "\n",
    "        fold_no += 1\n",
    "\n",
    "    eval_scores = np.array(eval_scores)\n",
    "    print(\"Average Performance\")\n",
    "    print(f\"Precision:  {round(np.mean(eval_scores[:,1]), 5)}\")\n",
    "    print(f\"Recall:     {round(np.mean(eval_scores[:,2]), 5)}\")\n",
    "    \n",
    "    return histories, eval_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "histories3, eval_scores3 = nin_model(3, metrics)\n",
    "scores.append(('nin', 3, eval_scores3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title3 = \"NiN - Kernel Size: 3\"\n",
    "plot_histories(histories3, title3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "histories4, eval_scores4 = nin_model(5, metrics)\n",
    "scores.append(('nin', 5, eval_scores4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title4 = \"NiN - Kernel Size: 5\"\n",
    "plot_histories(histories4, title4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histories(histories, title):\n",
    "    fig, subplots = plt.subplots(2,3,figsize=(15,n_splits*4))\n",
    "    for fold_no, history in enumerate(histories):\n",
    "        keys = list(history.history.keys())\n",
    "        \n",
    "        subplots[0,0].plot(history.history[keys[0]], label=fold_no)\n",
    "        subplots[0,0].set_title('Binary Crossentropy')\n",
    "\n",
    "        subplots[0,1].plot(history.history[keys[1]], label=fold_no)\n",
    "        subplots[0,1].set_title('Precision')\n",
    "        subplots[0,1].set_ylim(0.4, 1.1)\n",
    "\n",
    "        subplots[0,2].plot(history.history[keys[2]], label=fold_no)\n",
    "        subplots[0,2].set_title('Recall')\n",
    "        subplots[0,2].set_ylim(0.4, 1.1)\n",
    "        \n",
    "        subplots[1,0].plot(history.history[keys[3]], label=fold_no)\n",
    "        subplots[1,0].set_title('Validation Binary Crossentropy')\n",
    "\n",
    "        subplots[1,1].plot(history.history[keys[4]], label=fold_no)\n",
    "        subplots[1,1].set_title('Validation Precision')\n",
    "        subplots[1,1].set_ylim(0.4, 1.1)\n",
    "\n",
    "        subplots[1,2].plot(history.history[keys[5]], label=fold_no)\n",
    "        subplots[1,2].set_title('Validation Recall')\n",
    "        subplots[1,2].set_ylim(0.4, 1.1)\n",
    "    handles, labels = subplots[1,2].get_legend_handles_labels()\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    fig.legend(handles, labels, title=\"Fold #\")\n",
    "    fig.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histories2(histories, titles):\n",
    "    for hists, title in zip(histories, titles):\n",
    "        plot_histories(hists, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_histories2([histories1, histories2, histories3, histories4], [title1, title2, title3, title4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Comparison of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## McNemar Test for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of McNemar Test Results for Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(metrics):\n",
    "    reg = l2(0.00001)\n",
    "    \n",
    "    mlp = Sequential()\n",
    "    mlp.add( Dropout(0.25))\n",
    "    mlp.add( Flatten() )\n",
    "    mlp.add( Dense(input_dim=images.shape[1], units=100, activation='relu', kernel_regularizer= reg) )\n",
    "    mlp.add( Dropout(0.5))\n",
    "    mlp.add( Dense(units=50, activation='relu', kernel_regularizer= reg) )\n",
    "    mlp.add( Dense(units=50, activation='relu', kernel_regularizer= reg) )\n",
    "    mlp.add( Dense(1) )\n",
    "    mlp.add( Activation('sigmoid') )\n",
    "\n",
    "    mlp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['Precision', 'Recall'])\n",
    "    \n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mlp(metrics):\n",
    "    print(\"MLP\")\n",
    "    \n",
    "    fold_no = 0\n",
    "    histories = []\n",
    "    eval_scores = []\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        mlp = build_mlp(metrics)\n",
    "\n",
    "        print('Fold',fold_no)\n",
    "        print('')\n",
    "        \n",
    "        history = mlp.fit(X_train, y_train, batch_size=batch_size, epochs=epochs*10, shuffle=True, verbose=1,\n",
    "                          validation_data=(X_test,y_test))\n",
    "\n",
    "        print('')\n",
    "        scores = mlp.evaluate(X_test,y_test, verbose=verbose)\n",
    "        print('-' * 110)\n",
    "        \n",
    "        histories.append(history)\n",
    "        eval_scores.append(scores)\n",
    "\n",
    "        fold_no += 1\n",
    "\n",
    "    eval_scores = np.array(eval_scores)\n",
    "    print(\"Average Performance\")\n",
    "    print(f\"Precision:  {round(np.mean(eval_scores[:,1]), 5)}\")\n",
    "    print(f\"Recall:     {round(np.mean(eval_scores[:,2]), 5)}\")\n",
    "    \n",
    "    return histories, eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "histories5, eval_scores5 = mlp(metrics)\n",
    "scores.append(('mlp', 0, eval_scores5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title5 = \"MLP\"\n",
    "plot_histories(histories5, title5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Models vs MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_histories2(list(range(n_splits)), [histories1, histories2, histories3, histories4, histories5], [title1,title2,title3,title4,title5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Comparison of Models vs MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## McNemar Test for Models vs MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sklearn import metrics as mt\n",
    "\n",
    "# mlp = build_mlp(.0001, metrics)\n",
    "# y_hat = mlp.predict(X_final)\n",
    "# #print(y_hat, y_final)\n",
    "# #print(np.count_nonzero(np.round(y_hat)))\n",
    "# #print(len(y_hat))\n",
    "\n",
    "# yhat = np.round(y_hat)\n",
    "\n",
    "# #print(mt.confusion_matrix(y_final,yhat))\n",
    "# print(mt.classification_report(y_final,yhat))\n",
    "\n",
    "# mlp.evaluate(X_final, y_final, verbose=verbose)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "bm1 = build_basic_model(3, metrics)\n",
    "bm1.fit(datagen.flow(X, y, batch_size=batch_size), \n",
    "                     steps_per_epoch=int(len(X)/batch_size),\n",
    "                     epochs=epochs, verbose=verbose)\n",
    "bm1_results = bm1.predict(X_final)\n",
    "bm1_results[bm1_results != 1] = 0\n",
    "bm1_confusion = confusion_matrix(y_final, bm1_results.flatten()).ravel()\n",
    "\n",
    "bm2 = build_basic_model(5, metrics)\n",
    "bm2.fit(datagen.flow(X, y, batch_size=batch_size), \n",
    "                     steps_per_epoch=int(len(X)/batch_size),\n",
    "                     epochs=epochs, verbose=verbose)\n",
    "bm2_results = bm2.predict(X_final)\n",
    "bm2_results[bm2_results != 1] = 0\n",
    "bm2_confusion = confusion_matrix(y_final, bm2_results.flatten()).ravel()\n",
    "\n",
    "nin1 = build_nin_model(3, metrics)\n",
    "nin1.fit(datagen.flow(X, y, batch_size=batch_size), \n",
    "                     steps_per_epoch=int(len(X)/batch_size),\n",
    "                     epochs=epochs, verbose=verbose)\n",
    "nin1_results = nin1.predict(X_final)\n",
    "nin1_results[nin1_results != 1] = 0\n",
    "nin1_confusion = confusion_matrix(y_final, nin1_results.flatten()).ravel()\n",
    "\n",
    "nin2 = build_nin_model(5, metrics)\n",
    "nin2.fit(datagen.flow(X, y, batch_size=batch_size), \n",
    "                     steps_per_epoch=int(len(X)/batch_size),\n",
    "                     epochs=epochs, verbose=verbose)\n",
    "nin2_results = nin2.predict(X_final)\n",
    "nin2_results[nin2_results != 1] = 0\n",
    "nin2_confusion = confusion_matrix(y_final, nin2_results.flatten()).ravel()\n",
    "\n",
    "mlp = build_mlp(metrics)\n",
    "mlp.fit(datagen.flow(X, y, batch_size=batch_size), \n",
    "                     steps_per_epoch=int(len(X)/batch_size),\n",
    "                     epochs=epochs, verbose=verbose)\n",
    "mlp_results = mlp.predict(X_final)\n",
    "mlp_results[mlp_results != 1] = 0\n",
    "mlp_confusion = confusion_matrix(y_final, mlp_results.flatten()).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = [\n",
    "    ['', 'bm1', 'bm2', 'nin1', 'nin2', 'mlp'],\n",
    "    ['True Negative',  bm1_confusion[0], bm2_confusion[0], nin1_confusion[0], nin2_confusion[0], mlp_confusion[0]],\n",
    "    ['False Positive', bm1_confusion[1], bm2_confusion[1], nin1_confusion[1], nin2_confusion[1], mlp_confusion[1]],\n",
    "    ['False Negative', bm1_confusion[2], bm2_confusion[2], nin1_confusion[2], nin2_confusion[2], mlp_confusion[2]],\n",
    "    ['True Positive',  bm1_confusion[3], bm2_confusion[3], nin1_confusion[3], nin2_confusion[3], mlp_confusion[3]],\n",
    "                   ]\n",
    "#https://stackoverflow.com/questions/13214809/pretty-print-2d-python-list\n",
    "s = [[str(e) for e in row] for row in confusion_matrix]\n",
    "lens = [max(map(len, col)) for col in zip(*s)]\n",
    "fmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\n",
    "table = [fmt.format(*row) for row in s]\n",
    "print ('\\n'.join(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reate contingency tables\n",
    "\n",
    "both_correct = [\n",
    "    ['', 'bm1', 'bm2', 'nin1', 'nin2', 'mlp'],\n",
    "    ['bm1', 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ['bm2', 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ['nin1', 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ['nin2', 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ['mlp', 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "first_correct = [\n",
    "    ['', 'bm1', 'bm2', 'nin1', 'nin2', 'mlp'],\n",
    "    ['bm1', 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ['bm2', 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ['nin1', 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ['nin2', 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ['mlp', 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "both_incorrect = [\n",
    "    ['', 'bm1', 'bm2', 'nin1', 'nin2', 'mlp'],\n",
    "    ['bm1', 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ['bm2', 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ['nin1', 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ['nin2', 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ['mlp', 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "for basic1, basic2, net1, net2, multi, y_vals in zip('bm1_results', 'bm2_results', 'nin1_results', 'nin2_results', 'mlp_results', 'y_test'):\n",
    "    if(basic1 == targetVal):\n",
    "        both_correct[1][1] += 1;\n",
    "        \n",
    "        if(basic2 == targetVal):\n",
    "            both_correct[1][2] += 1;\n",
    "        else:\n",
    "            first_correct[1][2] += 1;\n",
    "            \n",
    "        if(resnetVal == targetVal):\n",
    "            both_correct[1][3] += 1;\n",
    "        else:\n",
    "            first_correct[1][3] += 1;\n",
    "            \n",
    "        if(resnet2Val == targetVal):\n",
    "            both_correct[1][4] += 1;\n",
    "        else:\n",
    "            first_correct[1][4] += 1;\n",
    "            \n",
    "        if(mlpVal == targetVal):\n",
    "            both_correct[1][5] += 1;\n",
    "        else:\n",
    "            first_correct[1][5] += 1;\n",
    "    else:\n",
    "        both_incorrect[1][1] += 1;\n",
    "        \n",
    "        if(cnn2Val != targetVal):\n",
    "            both_incorrect[1][2] += 1;\n",
    "            \n",
    "        if(resnetVal != targetVal):\n",
    "            both_incorrect[1][3] += 1;\n",
    "            \n",
    "        if(resnet2Val != targetVal):\n",
    "            both_incorrect[1][4] += 1;\n",
    "            \n",
    "        if(mlpVal != targetVal):\n",
    "            both_incorrect[1][5] += 1;\n",
    "            \n",
    "    if(cnn2Val == targetVal):\n",
    "        both_correct[2][2] += 1;\n",
    "        \n",
    "        if(cnnVal == targetVal):\n",
    "            both_correct[2][1] += 1;\n",
    "        else:\n",
    "            first_correct[2][1] += 1;\n",
    "            \n",
    "        if(resnetVal == targetVal):\n",
    "            both_correct[2][3] += 1;\n",
    "        else:\n",
    "            first_correct[2][3] += 1;\n",
    "            \n",
    "        if(resnet2Val == targetVal):\n",
    "            both_correct[2][4] += 1;\n",
    "        else:\n",
    "            first_correct[2][4] += 1;\n",
    "            \n",
    "        if(mlpVal == targetVal):\n",
    "            both_correct[2][5] += 1;\n",
    "        else:\n",
    "            first_correct[2][5] += 1;\n",
    "    else:\n",
    "        both_incorrect[2][2] += 1;\n",
    "        \n",
    "        if(cnnVal != targetVal):\n",
    "            both_incorrect[2][1] += 1;\n",
    "            \n",
    "        if(resnetVal != targetVal):\n",
    "            both_incorrect[2][3] += 1;\n",
    "            \n",
    "        if(resnet2Val != targetVal):\n",
    "            both_incorrect[2][4] += 1;\n",
    "            \n",
    "        if(mlpVal != targetVal):\n",
    "            both_incorrect[2][5] += 1;\n",
    "            \n",
    "    if(resnetVal == targetVal):\n",
    "        both_correct[3][3] += 1;\n",
    "        \n",
    "        if(cnn2Val == targetVal):\n",
    "            both_correct[3][2] += 1;\n",
    "        else:\n",
    "            first_correct[3][2] += 1;\n",
    "            \n",
    "        if(cnnVal == targetVal):\n",
    "            both_correct[3][1] += 1;\n",
    "        else:\n",
    "            first_correct[3][1] += 1;\n",
    "            \n",
    "        if(resnet2Val == targetVal):\n",
    "            both_correct[3][4] += 1;\n",
    "        else:\n",
    "            first_correct[3][4] += 1;\n",
    "            \n",
    "        if(mlpVal == targetVal):\n",
    "            both_correct[3][5] += 1;\n",
    "        else:\n",
    "            first_correct[3][5] += 1;\n",
    "    else:\n",
    "        both_incorrect[3][3] += 1;\n",
    "        \n",
    "        if(cnn2Val != targetVal):\n",
    "            both_incorrect[3][2] += 1;\n",
    "            \n",
    "        if(cnnVal != targetVal):\n",
    "            both_incorrect[3][1] += 1;\n",
    "            \n",
    "        if(resnet2Val != targetVal):\n",
    "            both_incorrect[3][4] += 1;\n",
    "            \n",
    "        if(mlpVal != targetVal):\n",
    "            both_incorrect[3][5] += 1;\n",
    "            \n",
    "    if(resnet2Val == targetVal):\n",
    "        both_correct[4][4] += 1;\n",
    "        \n",
    "        if(cnn2Val == targetVal):\n",
    "            both_correct[4][2] += 1;\n",
    "        else:\n",
    "            first_correct[4][2] += 1;\n",
    "            \n",
    "        if(resnetVal == targetVal):\n",
    "            both_correct[4][3] += 1;\n",
    "        else:\n",
    "            first_correct[4][3] += 1;\n",
    "            \n",
    "        if(cnnVal == targetVal):\n",
    "            both_correct[4][1] += 1;\n",
    "        else:\n",
    "            first_correct[4][1] += 1;\n",
    "            \n",
    "        if(mlpVal == targetVal):\n",
    "            both_correct[4][5] += 1;\n",
    "        else:\n",
    "            first_correct[4][5] += 1;\n",
    "    else:\n",
    "        both_incorrect[4][4] += 1;\n",
    "        \n",
    "        if(cnn2Val != targetVal):\n",
    "            both_incorrect[4][4] += 1;\n",
    "            \n",
    "        if(resnetVal != targetVal):\n",
    "            both_incorrect[4][4] += 1;\n",
    "            \n",
    "        if(cnnVal != targetVal):\n",
    "            both_incorrect[4][1] += 1;\n",
    "            \n",
    "        if(mlpVal != targetVal):\n",
    "            both_incorrect[4][5] += 1;\n",
    "            \n",
    "    if(mlpVal == targetVal):\n",
    "        both_correct[5][5] += 1;\n",
    "        \n",
    "        if(cnn2Val == targetVal):\n",
    "            both_correct[5][2] += 1;\n",
    "        else:\n",
    "            first_correct[5][2] += 1;\n",
    "            \n",
    "        if(resnetVal == targetVal):\n",
    "            both_correct[5][3] += 1;\n",
    "        else:\n",
    "            first_correct[5][3] += 1;\n",
    "            \n",
    "        if(resnet2Val == targetVal):\n",
    "            both_correct[5][4] += 1;\n",
    "        else:\n",
    "            first_correct[5][4] += 1;\n",
    "            \n",
    "        if(cnnVal == targetVal):\n",
    "            both_correct[5][1] += 1;\n",
    "        else:\n",
    "            first_correct[5][1] += 1;\n",
    "    else:\n",
    "        both_correct[5][5] += 1;\n",
    "        \n",
    "        if(cnn2Val != targetVal):\n",
    "            both_incorrect[5][2] += 1;\n",
    "            \n",
    "        if(resnetVal != targetVal):\n",
    "            both_incorrect[5][3] += 1;\n",
    "            \n",
    "        if(resnet2Val != targetVal):\n",
    "            both_incorrect[5][4] += 1;\n",
    "            \n",
    "        if(cnnVal != targetVal):\n",
    "            both_incorrect[5][1] += 1;\n",
    "\n",
    "print('both correct')\n",
    "s = [[str(e) for e in row] for row in both_correct]\n",
    "lens = [max(map(len, col)) for col in zip(*s)]\n",
    "fmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\n",
    "table = [fmt.format(*row) for row in s]\n",
    "print ('\\n'.join(table))\n",
    "\n",
    "print('\\nboth incorrect')\n",
    "s = [[str(e) for e in row] for row in both_incorrect]\n",
    "lens = [max(map(len, col)) for col in zip(*s)]\n",
    "fmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\n",
    "table = [fmt.format(*row) for row in s]\n",
    "print ('\\n'.join(table))\n",
    "\n",
    "print('\\nfirst correct')\n",
    "s = [[str(e) for e in row] for row in first_correct]\n",
    "lens = [max(map(len, col)) for col in zip(*s)]\n",
    "fmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\n",
    "table = [fmt.format(*row) for row in s]\n",
    "print ('\\n'.join(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('cnn and cnn2')\n",
    "    b = contingency_table_first_correct[1][2]\n",
    "    c = contingency_table_first_correct[2][1]\n",
    "    print(((b-c)**2) / (b+c))\n",
    "    print()\n",
    "except:\n",
    "    print('cnn and cnn2 are too similar \\n')\n",
    "\n",
    "try:\n",
    "    print('cnn and resnet')\n",
    "    b = contingency_table_first_correct[1][3]\n",
    "    c = contingency_table_first_correct[3][1]\n",
    "    print(((b-c)**2) / (b+c))\n",
    "    print()\n",
    "except:\n",
    "    print('cnn and resnet are too similar \\n')\n",
    "\n",
    "try:\n",
    "    print('cnn and resnet2')\n",
    "    b = contingency_table_first_correct[1][4]\n",
    "    c = contingency_table_first_correct[4][1]\n",
    "    print(((b-c)**2) / (b+c))\n",
    "    print()\n",
    "except:\n",
    "    print('cnn and resnet2 are too similar \\n')\n",
    "\n",
    "try:\n",
    "    print('cnn and mlp')\n",
    "    b = contingency_table_first_correct[1][5]\n",
    "    c = contingency_table_first_correct[5][1]\n",
    "    print(((b-c)**2) / (b+c))\n",
    "    print()\n",
    "except:\n",
    "    print('cnn and mlp are too similar \\n')\n",
    "    \n",
    "\n",
    "try:\n",
    "    print('cnn2 and resnet')\n",
    "    b = contingency_table_first_correct[2][3]\n",
    "    c = contingency_table_first_correct[3][2]\n",
    "    print(((b-c)**2) / (b+c))\n",
    "    print()\n",
    "except:\n",
    "    print('cnn2 and resnet are too similar \\n')\n",
    "\n",
    "try:\n",
    "    print('cnn2 and resnet2')\n",
    "    b = contingency_table_first_correct[2][4]\n",
    "    c = contingency_table_first_correct[4][2]\n",
    "    print(((b-c)**2) / (b+c))\n",
    "    print()\n",
    "except:\n",
    "    print('cnn2 and resnet2 are too similar \\n')\n",
    "\n",
    "try:\n",
    "    print('cnn2 and mlp')\n",
    "    b = contingency_table_first_correct[2][5]\n",
    "    c = contingency_table_first_correct[5][2]\n",
    "    print(((b-c)**2) / (b+c))\n",
    "    print()\n",
    "except:\n",
    "    print('cnn2 and mlp are too similar \\n')\n",
    "\n",
    "try:\n",
    "    print('resnet and resnet2')\n",
    "    b = contingency_table_first_correct[3][4]\n",
    "    c = contingency_table_first_correct[4][3]\n",
    "    print(((b-c)**2) / (b+c))\n",
    "    print()\n",
    "except:\n",
    "    print('resnet and resnet2 are too similar \\n')\n",
    "\n",
    "try:\n",
    "    print('resnet and mlp')\n",
    "    b = contingency_table_first_correct[3][5]\n",
    "    c = contingency_table_first_correct[5][3]\n",
    "    print(((b-c)**2) / (b+c))\n",
    "    print()\n",
    "except:\n",
    "    print('resnet and mlp are too similar \\n')\n",
    "\n",
    "try:\n",
    "    print('resnet2 and mlp')\n",
    "    b = contingency_table_first_correct[4][5]\n",
    "    c = contingency_table_first_correct[5][4]\n",
    "    print(((b-c)**2) / (b+c))\n",
    "    print()\n",
    "except:\n",
    "    print('resnet2 and mlp are too similar \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc(model):\n",
    "    kfold = StratifiedKFold(n_splits=4).split(train_images, train_targets)\n",
    "\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "\n",
    "    for i, (train, test) in enumerate(kfold):\n",
    "        probas = model.predict(train_images[test])\n",
    "\n",
    "        perclass_mean_tpr = 0.0\n",
    "        roc_auc = 0\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(train_targets[test],\n",
    "                                         probas)\n",
    "        perclass_mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
    "        perclass_mean_tpr[0] = 0.0\n",
    "        roc_auc += auc(fpr, tpr)\n",
    "\n",
    "        mean_tpr += perclass_mean_tpr\n",
    "        plt.plot(mean_fpr,perclass_mean_tpr,'--',lw=1,label='Mean Class ROC fold %d (area = %0.2f)' % (i+1, roc_auc))\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc(mlp)\n",
    "roc(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of McNemar Test Results for Models vs MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have free reign to provide additional analyses. \n",
    "- One idea (<b>required for 7000 level students</b>): Use transfer learning to pre-train the weights of your initial layers of your CNN. Compare the performance when using transfer learning to training without transfer learning (i.e., compare to your best model from above) in terms of classification performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
