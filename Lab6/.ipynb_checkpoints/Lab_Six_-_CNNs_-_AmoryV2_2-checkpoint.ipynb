{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Six -  Convolutional Network Architectures\n",
    "Amory Weinzierl, Fidelia Nawar, and Hayden Center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will select a prediction task to perform on your dataset, evaluate a deep learning architecture and tune hyper-parameters. If any part of the assignment is not clear, ask the instructor to clarify. \n",
    "\n",
    "This report is worth 10% of the final grade. Please upload a report (<b>one per team</b>) with all code used, visualizations, and text in a rendered Jupyter notebook. Any visualizations that cannot be embedded in the notebook, please provide screenshots of the output. The results should be reproducible using your report. Please carefully describe every assumption and every step in your report.\n",
    "\n",
    "<b>Dataset Selection</b>\n",
    "\n",
    "Select a dataset identically to lab two (images). That is, the dataset must be image data. In terms of generalization performance, it is helpful to have a large dataset of identically sized images. It is fine to perform binary classification or multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [<b>1.5 points</b>] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a <b>detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate</b> for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "- [<b>1.5 points</b>] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). <b>Explain why your chosen method is appropriate or use more than one method as appropriate</b>. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages and reading in dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "print('Pandas:', pd.__version__)\n",
    "print('Numpy:',  np.__version__)\n",
    "print('Tensorflow:', tf.__version__)\n",
    "print('Keras:',  keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#source: https://www.geeksforgeeks.org/how-to-convert-images-to-numpy-array/\n",
    "from PIL import Image\n",
    "\n",
    "#source: https://stackoverflow.com/questions/10377998/how-can-i-iterate-over-files-in-a-given-directory\n",
    "from pathlib import Path\n",
    "\n",
    "#directory name\n",
    "paths = {\n",
    "    \"TRAIN\": './Coronahack-Chest-XRay-Dataset/train/',\n",
    "    \"TEST\":  './Coronahack-Chest-XRay-Dataset/test/'    \n",
    "}\n",
    "metadata = pd.read_csv('Chest_xray_Corona_Metadata.csv')\n",
    "\n",
    "h, w = 64, 64\n",
    "\n",
    "tf.random.set_seed(2)\n",
    "np.random.seed(0) # using this to help make results reproducible\n",
    "\n",
    "images = metadata[[\"X_ray_image_name\", \"Dataset_type\"]]\n",
    "X_data = []\n",
    "y_data = metadata[\"Label\"]\n",
    "for idx, img in images.iterrows():\n",
    "    name = img[\"X_ray_image_name\"]\n",
    "    path = img[\"Dataset_type\"]\n",
    "    img_arr = np.asarray(Image.open(paths[path] + name).convert('L').resize((h,w)))\n",
    "    X_data.append(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "_X = np.expand_dims(np.array(X_data), axis=-1)/255 - 0.5\n",
    "_y = le.fit_transform(np.array(y_data))\n",
    "\n",
    "print(_X.shape, _y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "display_imgs = np.concatenate((_X[0:9], _X[-9:]))\n",
    "labels = np.concatenate((y_data[0:9], y_data[-9:]))\n",
    "def plot_gallery(images, titles, n_row=3, n_col=3):\n",
    "    plt.figure(figsize=(n_col * n_col, 6 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    #normal scans tended towards front\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row * 2, n_col, i + 1)\n",
    "        plt.imshow(images[i], cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    #pnemonia scans toward back so we pulled some from the back \n",
    "    #for demonstration purposes\n",
    "    for j in range(n_row * n_col):\n",
    "        plt.subplot(n_row * 2, n_col, n_row * n_col + j + 1)\n",
    "        plt.imshow(images[-1*j], cmap=plt.cm.gray)\n",
    "        plt.title(titles[-1*j], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        \n",
    "plot_gallery(display_imgs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metric\n",
    "\n",
    "The primary evaluation metric we are using for our model is the f-score, and therefore we are also measuring recall and precision. Recall measures the percentage of positive cases that were identified correctly, precision measures the percentage of positive predictions that were correct, and f-score is a formula which combines those two metrics into one score.\n",
    "\n",
    "These metrics emphasize correct positive identifications, which is applicable to evaluate our solution because we want to minimize the amount of undetected pneumonia lungs, though recall is the more important metric, as it can be used to minimize the false negative rate. Having a low false negative rate is important in this situation because a diagnosis of a \"Normal\" lung condition when it is in fact penumonia is detrimental and possibly fatal to the patient. On the same token, it's necessary that healthy lungs are not misclassified as pneumonia because that would create unnecessary issues for a healthy patient. Because of this, we chose to use f-score, as well as recall and precision for auxiliary metrics, to evaluate our CNN solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing Data\n",
    "\n",
    "We are using stratified 10-fold cross validation in order to split up the data into training and test sets. We chose to use this method because almost 3/4 of our the lungs in our dataset are labeled as having pneumonia, whereas only 1/4 is labeled as healthy. Thus, if we did a random split/shuffle, there may be disproportionate amounts of pneumonia classification in the training variables, which would make the classification for the testing data less accurate. With \n",
    "stratified 10-fold cross validation, we can make a more effective model and also help with generalizing. It allows us to select training and testing sets while also decreasing overall variance because of the 10 folds, which will fit each CNN on each fold. This would be a realistic measuring of a real-world application of the algorithm because with smaller test sets, there is higher variance. Stratified cross validation reduces this variance by averaging over k different partitions, so the performance estimate is less sensitive to the partitioning of the data. We also chose 10 folds because this value has been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.\n",
    "\n",
    "Additionally, we will be using an 80/20 split, where the 80% test set will be used for cross validation, and then used to train our final models for statistical comparisons of performance on the 20% split, as cross validation does not render a final trained model, and is only useful for comparing our evaluation metrics when exposed to data it hasn't seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, X_final, y, y_final = train_test_split(_X, _y, test_size=0.2, stratify=_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (6 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [<b>1.5 points</b>]  Setup the training to use data expansion in Keras. Explain why the chosen data expansion techniques are appropriate for your dataset. \n",
    "- [<b>2 points</b>] Create a convolutional neural network to use on your data using Keras. Investigate at least two different convolutional network architectures (and investigate changing some parameters of each architecture--at minimum have two variations of each network for a total of four models trained). Use the method of train/test splitting and evaluation metric that you argued for at the beginning of the lab. Visualize the performance of the training and validation sets per iteration (use the \"history\" parameter of Keras).\n",
    "- [<b>1.5 points</b>] Visualize the final results of the CNNs and interpret the performance. Use proper statistics as appropriate, especially for comparing models. \n",
    "- [<b>1 points</b>] Compare the performance of your convolutional network to a standard multi-layer perceptron (MLP) using the receiver operating characteristic and area under the curve. Use proper statistical comparison techniques.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using Keras's built in ImageDataGenerator for our data expansion. In reshaping all of our images to 128x128, many of the images were already stretched and squashed in different directions, and so expanding our dataset to stretch and squash them more randomly will hopefully remove any hidden biases that the different image sizes may have created. Additionally, since all of the xrays are more or less similarly oriented, we can add a slight rotational adjustment. However, since the images should all be uniquely oriented horizontally (because the heart is always located to one side of the body) and vertically (all of the images have the patients neck and shoulders on the top side of the image), it would not be useful to flip the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=1,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05)\n",
    "\n",
    "datagen.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers       import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers       import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.models       import Model, Sequential\n",
    "from tensorflow.keras.callbacks    import EarlyStopping\n",
    "from tensorflow.keras.utils        import plot_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "optimizer = 'rmsprop'\n",
    "metrics = [keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "verbose = 1\n",
    "n_splits = 10\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1234)\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total    = len(_y)\n",
    "num_positive = np.count_nonzero(_y)\n",
    "num_negative = num_total - num_positive\n",
    "weight_negative = num_positive / num_negative\n",
    "class_weights = {0: weight_negative, 1: 1}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histories(model):\n",
    "    histories = model['histories']\n",
    "    title     = model['title']\n",
    "    \n",
    "    fig, subplots = plt.subplots(2,3,figsize=(15,8))\n",
    "    fold_names = [\"Fold \" + str(fold) for fold in range(n_splits)]\n",
    "    for fold_no, history in enumerate(histories):\n",
    "        keys = list(history.history.keys())\n",
    "        \n",
    "        subplots[0,0].plot(history.history[keys[0]], label=fold_no)\n",
    "        subplots[0,0].set_title('Binary Crossentropy')\n",
    "\n",
    "        subplots[0,1].plot(history.history[keys[1]], label=fold_no)\n",
    "        subplots[0,1].set_title('Precision')\n",
    "        subplots[0,1].set_ylim(0.4, 1.1)\n",
    "\n",
    "        subplots[0,2].plot(history.history[keys[2]], label=fold_no)\n",
    "        subplots[0,2].set_title('Recall')\n",
    "        subplots[0,2].set_ylim(0.4, 1.1)\n",
    "        \n",
    "        subplots[1,0].plot(history.history[keys[3]], label=fold_no)\n",
    "        subplots[1,0].set_title('Validation Binary Crossentropy')\n",
    "\n",
    "        subplots[1,1].plot(history.history[keys[4]], label=fold_no)\n",
    "        subplots[1,1].set_title('Validation Precision')\n",
    "        subplots[1,1].set_ylim(0.4, 1.1)\n",
    "\n",
    "        subplots[1,2].plot(history.history[keys[5]], label=fold_no)\n",
    "        subplots[1,2].set_title('Validation Recall')\n",
    "        subplots[1,2].set_ylim(0.4, 1.1)\n",
    "    handles, labels = subplots[1,2].get_legend_handles_labels()\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    fig.legend(handles, labels, title=\"Fold #\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Basic Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_basic_model(kernel_size, metrics):\n",
    "    reg = l2(0.00001)\n",
    "    cnn = Sequential()\n",
    "\n",
    "    cnn.add(Conv2D(filters=32,\n",
    "                kernel_size=kernel_size,\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same',\n",
    "                activation='relu'))\n",
    "    cnn.add(Conv2D(filters=32,\n",
    "                kernel_size=kernel_size,\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same',\n",
    "                activation='relu'))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    cnn.add(Conv2D(filters=64,\n",
    "                kernel_size=kernel_size,\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same',\n",
    "                activation='relu'))\n",
    "    cnn.add(Conv2D(filters=64,\n",
    "                kernel_size=kernel_size,\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same',\n",
    "                activation='relu'))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    cnn.add(Dropout(0.25))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(128, activation='relu',\n",
    "                kernel_regularizer=reg))\n",
    "    cnn.add(Dropout(0.5))\n",
    "    cnn.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=reg))\n",
    "\n",
    "    cnn.compile(loss=loss,\n",
    "                optimizer=optimizer,\n",
    "                metrics=metrics)\n",
    "    \n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Network in Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nin_model(kernel_size, metrics):\n",
    "    reg = l2(0.00001)\n",
    "    cnn = Sequential()\n",
    "\n",
    "    cnn.add(Conv2D(filters=32,\n",
    "                kernel_size=kernel_size,\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same',\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "    cnn.add(Activation('relu'))\n",
    "    cnn.add(Conv2D(filters=32,\n",
    "                kernel_size=(1,1),\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same',\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "    cnn.add(Activation('relu'))\n",
    "    cnn.add(Conv2D(filters=32,\n",
    "                kernel_size=(1,1),\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same',\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "    cnn.add(Activation('relu'))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    cnn.add(Dropout(0.25))\n",
    "\n",
    "    cnn.add(Conv2D(filters=64,\n",
    "                kernel_size=kernel_size,\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same',\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "    cnn.add(Activation('relu'))\n",
    "    cnn.add(Conv2D(filters=64,\n",
    "                kernel_size=(1,1),\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same',\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "    cnn.add(Activation('relu'))\n",
    "    cnn.add(Conv2D(filters=1,\n",
    "                kernel_size=(1,1),\n",
    "                kernel_regularizer=reg,\n",
    "                padding='same',\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "    cnn.add(Activation('relu'))\n",
    "    cnn.add(Dropout(0.5))\n",
    "\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=reg))\n",
    "\n",
    "    cnn.compile(loss=loss,\n",
    "                optimizer=optimizer,\n",
    "                metrics=metrics)\n",
    "    \n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Model - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_model(kernel_size, metrics):\n",
    "    reg = l2(0.00001)\n",
    "    \n",
    "    mlp = Sequential()\n",
    "    mlp.add( Dropout(0.25))\n",
    "    mlp.add( Flatten() )\n",
    "    mlp.add( Dense(input_dim=images.shape[1], units=100, activation='relu', kernel_regularizer= reg) )\n",
    "    mlp.add( Dropout(0.5))\n",
    "    mlp.add( Dense(units=50, activation='relu', kernel_regularizer= reg) )\n",
    "    mlp.add( Dense(units=50, activation='relu', kernel_regularizer= reg) )\n",
    "    mlp.add( Dense(1) )\n",
    "    mlp.add( Activation('sigmoid') )\n",
    "\n",
    "    mlp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['Precision', 'Recall'])\n",
    "    \n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build = {\n",
    "    'Basic': build_basic_model,\n",
    "    'NiN':   build_nin_model,\n",
    "    'MLP':   build_mlp_model\n",
    "}\n",
    "\n",
    "def create_model(arch, kernel_size):\n",
    "    if kernel_size > 0:\n",
    "        title = arch + ' (Kernel Size: ' + str(kernel_size) + ')'\n",
    "    else:\n",
    "        title = arch\n",
    "    \n",
    "    print(title)\n",
    "    print('')\n",
    "    print(\"Cross Validation\")\n",
    "    print('')\n",
    "    \n",
    "    fold_no = 0\n",
    "    histories = []\n",
    "    scores = []\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model = build[arch](kernel_size, metrics)\n",
    "\n",
    "        print('Fold',fold_no)\n",
    "        print('')\n",
    "        \n",
    "        history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                            steps_per_epoch=int(len(X_train)/batch_size),\n",
    "                            epochs=epochs, verbose=verbose,\n",
    "                            class_weight=class_weights,\n",
    "                            validation_data=(X_test, y_test))\n",
    "\n",
    "        print('-' * 110)\n",
    "        \n",
    "        scores.append(np.array(list(history.history.values()))[:,-1])\n",
    "        histories.append(history)\n",
    "        \n",
    "        fold_no += 1\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    print(\"Cross Validation Average Performance\")\n",
    "    print(f\"Precision:  {round(np.mean(scores[:,4]), 5)}\")\n",
    "    print(f\"Recall:     {round(np.mean(scores[:,5]), 5)}\")\n",
    "    print('-' * 110)\n",
    "    print('Evaluation Model')\n",
    "    print('')\n",
    "    \n",
    "    model = build[arch](kernel_size, metrics)\n",
    "    model.fit(datagen.flow(X, y, batch_size=batch_size), \n",
    "                         steps_per_epoch=int(len(X)/batch_size),\n",
    "                         epochs=epochs, verbose=verbose,\n",
    "                         validation_data=(X_final, y_final))\n",
    "    prob = model.predict(X_final)\n",
    "    results = np.round(prob)\n",
    "\n",
    "    return {'title':       title,\n",
    "            'arch':        arch,\n",
    "            'kernel_size': kernel_size,\n",
    "            'histories':   histories,\n",
    "            'scores':      scores,\n",
    "            'model':       model,\n",
    "            'probs':       prob,\n",
    "            'results':     results,\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "models['b1'] = create_model('Basic', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories(models['b1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "models['b2'] = create_model('Basic', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories(models['b2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NiN Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "build = {\n",
    "    'Basic': build_basic_model,\n",
    "    'NiN':   build_nin_model,\n",
    "    'MLP':   build_mlp_model\n",
    "}\n",
    "\n",
    "models['n1'] = create_model('NiN', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_histories(models['n1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NiN Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "models['n2'] = create_model('NiN', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories(models['n2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mlp = create_model('MLP', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_histories(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we compare the models based on the f-score of their average validation precision and recall over the 10 folds of cross validation on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def f_score(model):\n",
    "    avg_val_precision = np.mean(model['scores'][:,4])\n",
    "    avg_val_recall    = np.mean(model['scores'][:,5])\n",
    "    f = 2 * avg_val_precision * avg_val_recall / (avg_val_precision + avg_val_recall)\n",
    "    return f\n",
    "\n",
    "f_scores = [[k, f_score(v)] for k, v in models.items()]\n",
    "f_scores.sort(key=lambda x: -x[1])\n",
    "\n",
    "print(\"Model Cross Validation Performance Rankings\")\n",
    "for i, (k, v) in enumerate(f_scores):\n",
    "    print(f\"{i+1}:  F-score: {np.format_float_positional(v, precision=4, trim='k')}  Name: {models[k]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the McNemar test to see if the models are statistically different from one another. Since cross validation does not yield a final trained model, we will be using a single model trained on the entirety of the training set and evaluated on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import copy\n",
    "\n",
    "def contingency_value(row):\n",
    "        if row['1'] and row['2']:\n",
    "            return 'A'\n",
    "        elif row['1']:\n",
    "            return 'B'\n",
    "        elif row['2']:\n",
    "            return 'C'\n",
    "        else:\n",
    "            return 'D'\n",
    "\n",
    "# arguments are 1D numpy arrays of 0 or 1\n",
    "def create_contingency_table(truth, model1, model2):\n",
    "    T  = truth.flatten()\n",
    "    m1 = model1['results'].flatten()\n",
    "    m2 = model2['results'].flatten()\n",
    "    \n",
    "    # create truth table\n",
    "    truth_table = pd.DataFrame({'T': T,\n",
    "                                '1': m1,\n",
    "                                '2': m2},\n",
    "                                dtype=bool)\n",
    "    \n",
    "    # convert all target values to be 'true' and adjust predictions accordingly\n",
    "    truth_table.loc[truth_table['T'] == False] = ~truth_table\n",
    "    \n",
    "    # get value for each square of the contingency table\n",
    "    contingency_values = truth_table.apply(contingency_value, axis=1)\n",
    "    A = len(contingency_values.loc[contingency_values == 'A'])\n",
    "    B = len(contingency_values.loc[contingency_values == 'B'])\n",
    "    C = len(contingency_values.loc[contingency_values == 'C'])\n",
    "    D = len(contingency_values.loc[contingency_values == 'D'])\n",
    "\n",
    "    return np.array([[A, B],\n",
    "                     [C, D]])\n",
    "\n",
    "def get_p_value(truth, model1, model2):\n",
    "    c_table = create_contingency_table(truth, model1, model2)\n",
    "    return mcnemar(c_table, exact=False, correction=False).pvalue\n",
    "\n",
    "def get_better_model(alpha, truth, model1, model2):\n",
    "    c_table = create_contingency_table(truth, model1, model2)\n",
    "    \n",
    "    correct1 = c_table[0,1]\n",
    "    correct2 = c_table[1,0]\n",
    "    if correct1 + correct2 == 0:\n",
    "        p = 1\n",
    "    else:\n",
    "        p = get_p_value(truth, model1, model2)\n",
    "    \n",
    "    print(\"Comparing\", model1['title'],\"to\",model2['title'])\n",
    "    if p >= alpha:\n",
    "        print(model1['title'], \"and\", model2['title'], \"perform similarly\",\"\\n\")\n",
    "        return [model1, model2]\n",
    "    elif correct1 > correct2:\n",
    "        print(model1['title'], \"performs better than\", model2['title'],\"\\n\")\n",
    "        return [model1]\n",
    "    else:\n",
    "        print(model2['title'], \"performs better than\", model1['title'],\"\\n\")\n",
    "        return [model2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "similar_to = {}\n",
    "best_model = list(models.values())[0]\n",
    "for model in list(models.values())[1:]:\n",
    "    if best_model['title'] not in similar_to:\n",
    "        similar_to[best_model['title']] = []\n",
    "    result = get_better_model(alpha, y_final, best_model, model)\n",
    "    if len(result) > 1:\n",
    "        similar_to[best_model['title']].append(model)\n",
    "    best_model = result[0]\n",
    "    \n",
    "    \n",
    "best_models = [best_model, *similar_to[best_model['title']]]\n",
    "print(\"Best Models:\")\n",
    "for model in best_models:\n",
    "    print(model['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def roc(model):\n",
    "#     kfold = StratifiedKFold(n_splits=4).split(train_images, train_targets)\n",
    "\n",
    "#     mean_tpr = 0.0\n",
    "#     mean_fpr = np.linspace(0, 1, 100)\n",
    "#     all_tpr = []\n",
    "\n",
    "#     for i, (train, test) in enumerate(kfold):\n",
    "#         probas = model.predict(train_images[test])\n",
    "\n",
    "#         perclass_mean_tpr = 0.0\n",
    "#         roc_auc = 0\n",
    "        \n",
    "#         fpr, tpr, thresholds = roc_curve(train_targets[test],\n",
    "#                                          probas)\n",
    "#         perclass_mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
    "#         perclass_mean_tpr[0] = 0.0\n",
    "#         roc_auc += auc(fpr, tpr)\n",
    "\n",
    "#         mean_tpr += perclass_mean_tpr\n",
    "#         plt.plot(mean_fpr,perclass_mean_tpr,'--',lw=1,label='Mean Class ROC fold %d (area = %0.2f)' % (i+1, roc_auc))\n",
    "\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.grid()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc(mlp)\n",
    "# roc(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of McNemar Test Results for Models vs MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have free reign to provide additional analyses. \n",
    "- One idea (<b>required for 7000 level students</b>): Use transfer learning to pre-train the weights of your initial layers of your CNN. Compare the performance when using transfer learning to training without transfer learning (i.e., compare to your best model from above) in terms of classification performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
