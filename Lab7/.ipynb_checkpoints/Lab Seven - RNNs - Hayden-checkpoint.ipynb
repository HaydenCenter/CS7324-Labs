{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Seven: Recurrent Network Architectures\n",
    "Amory Weinzierl and Hayden Center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will select a prediction task to perform on your dataset, evaluate a recurrent architecture and tune hyper-parameters. If any part of the assignment is not clear, ask the instructor to clarify. \n",
    "\n",
    "This report is worth 10% of the final grade. Please upload a report (<b>one per team</b>) with all code used, visualizations, and text in a rendered Jupyter notebook. Any visualizations that cannot be embedded in the notebook, please provide screenshots of the output. The results should be reproducible using your report. Please carefully describe every assumption and every step in your report.\n",
    "\n",
    "<b>Dataset Selection</b>\n",
    "\n",
    "Select a dataset that is text. That is, the dataset should be text data (or a time series sequence). In terms of generalization performance, it is helpful to have a large dataset of similar sized text documents. It is fine to perform binary classification or multi-class classification. The classification can be \"many-to-one\" or \"many-to-many\" sequence classification, whichever you feel more comfortable with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [<b>1 points</b>] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed). Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). <b>Discuss methods of tokenization in your dataset as well as any decisions to force a specific length of sequence.</b>.  \n",
    "- [<b>1 points</b>] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a <b>detailed argument for why this (these) metric(s) are appropriate</b> on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "- [<b>1 points</b>] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). <b>Explain why your chosen method is appropriate or use more than one method as appropriate.</b> Convince me that your train/test splitting method is a realistic mirroring of how an algorithm would be used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 1.2.1\n",
      "Numpy: 1.19.2\n",
      "Tensorflow: 2.3.0\n",
      "Keras: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# Importing packages and reading in dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "print('Pandas:', pd.__version__)\n",
    "print('Numpy:',  np.__version__)\n",
    "print('Tensorflow:', tf.__version__)\n",
    "print('Keras:',  keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "top_words = 50\n",
    "maxlen = 200\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words, maxlen=maxlen)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prediction task will be the ever popular IMDB review sentiment analysis. This dataset (built into keras) is made up of lists of integers representing words in a review. Each word is encoded as its frequency ranking in the dataset, starting at index 3 (as 1 and 2 are special characters reserved for marking the start of a sequence and a word that isn't found in the training set vocabulary): the most popular word would be 3, followed by the second most popular word as 4, and so on. \n",
    "\n",
    "We set two constraints on our dataset. First, we are ignoring the 50 most frequent words in the dataset, which (if the distribution is similar to the typical English distribution) is made up mostly of articles, prepositions, and the like, which are not very useful for classifying sentiment. The decision to only exclude the top 50 words instead of more or less than that was influenced by the existence of several important words right outside of the top 50 most frequent; words such as 'like', 'good', and 'bad' appear not far outside of the top 50 in regular English, and these words seem at least at first glance to be very significant for sentiment analysis, and therefore are left in to use as features. \n",
    "\n",
    "Second, we are limiting each sequence to 200 words maximum, and padding shorter sequences with zeros to match. This decision was influenced by reading a sample of real IMDB reviews. We felt that it was usually fairly easy from clear context within the first 200 words of a review how the reviewer felt about the film. Obviously, humans are going to be naturally good at classifying things like this, however we still felt that, for the sake of dimensionality reduction, 200 words should be an adequate amount of data for this classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using ROC area under the curve as our binary classifier's evaluation metric. Since this classification task does not demand a certain threshold in terms of false negatives or false positives being more important, using ROC AUC allows us to evaluate the performance of the classifier over several thresholds and determine the overall performance of the model, instead of the performance at a single threshold decided by the importance of false positives or false negatives.\n",
    "\n",
    "A more specific use case of this classifier could be creating a score for a film based on the reviews users write as opposed to the score users give, since a particular score, such as a 3/5, can mean very different things to very different people; for some, 3/5 may mean about average, and for others, might mean pretty bad or pretty good. The classifier would then analyze the sentiment of the reviews to create a less biased scoring system. As such, this classifier would not prioritize classifying one type of sentiment over another, and so a balanced metric of general performance is needed to evaluate the model.\n",
    "\n",
    "Comparing ROC AUC to other evaluation metrics for our use case demonstrates the superior effectiveness of that metric compared to other possible metrics. Accuracy is a good metric for simplicity, but simply doesn't capture some of the nuances of a model's performance; accuracy tells us nothing about what the model is over or under predicting. Precision and recall are good metrics, however can suffer from the same issues that accuracy does when used alone. A perfect precision or a perfect recall score may seem good, but based on context, can actually also be awful, and therefore are not effective as sole metrics. F-score is a more robust way to look at precision and recall, but its ease of implementation using Keras is much worse than ROC AUC, and though it is about as useful for our classification task, ROC AUC being built into the Keras API edges out F-score for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [keras.metrics.AUC()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this dataset is so large, the predefined 50/50 split is adequate. 25,000 data points to train on should be more than enough data points to allow the loss function to converge. This method is realistic as the sample size is massive enough to likely allow the model to encounter the vast majority of relevant features and train on those features to create a robust classifier that is effective when used to predict data it hasn't seen before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (6 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [<b>3 points</b>] Investigate at least two different recurrent network architectures (perhaps LSTM and GRU). Be sure to use an embedding layer (pre-trained, from scratch, OR both). Adjust hyper-parameters of the networks as needed to improve generalization performance. Discuss the performance of each network nd compare them.\n",
    "- [<b>1 points</b>] Using the best RNN parameters and architecture, add a second recurrent chain to your RNN. The input to the second chain should be the output sequence of the first chain. Visualize the performance of training and validation sets versus the training iterations. \n",
    "- [<b>2 points</b>] Use the method of cross validation and evaluation criteria that you argued for at the beginning of the lab. Visualize the results of all the RNNs you trained.  Use proper statistical comparison techniques to determine which method(s) is (are) superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have free reign to provide additional analyses.\n",
    "- One idea (<b>required for 7000 level students to do one of these options</b>):\n",
    "    - <b>Option 1</b>: Use t-SNE (or SVD or PCA or UMAP) to visualize the word embeddings of a subset of words in your vocabulary. Try to interpret what each dimension reflects (in your own words). That is, try to explain what aspect of the language is encoded in the reduced dimensionality embedding. \n",
    "    - <b>Options 2</b>: Use the ConceptNet Numberbatch embedding and compare to GloVe\n",
    "- Another Idea (NOT required): Try to create a RNN for generating novel text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
